{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04_02_automated_machine_learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengwangk/dl-projects/blob/master/04_09_auto_ml_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4hyoPGdjpqa_"
      },
      "source": [
        "# Automated ML - Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SLxr2k_ue8yq",
        "colab": {}
      },
      "source": [
        "COLAB = True\n",
        "\n",
        "DATASET_NAME = '4D.zip'\n",
        "\n",
        "FEATURE_DATASET_PREFIX = 'feature_matrix_d2_v3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwYshXtLt7b7",
        "colab": {}
      },
      "source": [
        "#!pip install -U imblearn\n",
        "#!pip install -U xgboost\n",
        "# !pip install -U featuretools\n",
        "\n",
        "# https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28\n",
        "# https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
        "# https://machinelearningmastery.com/imbalanced-classification-model-to-detect-oil-spills/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oy5ww2zRfFGG",
        "outputId": "a081ea78-75b8-4c2b-c024-667afc9aaeb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "if COLAB:\n",
        "  !rm -rf dl-projects\n",
        "  !git clone https://github.com/mengwangk/dl-projects"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dl-projects'...\n",
            "remote: Enumerating objects: 76, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/76)\u001b[K\rremote: Counting objects:   2% (2/76)\u001b[K\rremote: Counting objects:   3% (3/76)\u001b[K\rremote: Counting objects:   5% (4/76)\u001b[K\rremote: Counting objects:   6% (5/76)\u001b[K\rremote: Counting objects:   7% (6/76)\u001b[K\rremote: Counting objects:   9% (7/76)\u001b[K\rremote: Counting objects:  10% (8/76)\u001b[K\rremote: Counting objects:  11% (9/76)\u001b[K\rremote: Counting objects:  13% (10/76)\u001b[K\rremote: Counting objects:  14% (11/76)\u001b[K\rremote: Counting objects:  15% (12/76)\u001b[K\rremote: Counting objects:  17% (13/76)\u001b[K\rremote: Counting objects:  18% (14/76)\u001b[K\rremote: Counting objects:  19% (15/76)\u001b[K\rremote: Counting objects:  21% (16/76)\u001b[K\rremote: Counting objects:  22% (17/76)\u001b[K\rremote: Counting objects:  23% (18/76)\u001b[K\rremote: Counting objects:  25% (19/76)\u001b[K\rremote: Counting objects:  26% (20/76)\u001b[K\rremote: Counting objects:  27% (21/76)\u001b[K\rremote: Counting objects:  28% (22/76)\u001b[K\rremote: Counting objects:  30% (23/76)\u001b[K\rremote: Counting objects:  31% (24/76)\u001b[K\rremote: Counting objects:  32% (25/76)\u001b[K\rremote: Counting objects:  34% (26/76)\u001b[K\rremote: Counting objects:  35% (27/76)\u001b[K\rremote: Counting objects:  36% (28/76)\u001b[K\rremote: Counting objects:  38% (29/76)\u001b[K\rremote: Counting objects:  39% (30/76)\u001b[K\rremote: Counting objects:  40% (31/76)\u001b[K\rremote: Counting objects:  42% (32/76)\u001b[K\rremote: Counting objects:  43% (33/76)\u001b[K\rremote: Counting objects:  44% (34/76)\u001b[K\rremote: Counting objects:  46% (35/76)\u001b[K\rremote: Counting objects:  47% (36/76)\u001b[K\rremote: Counting objects:  48% (37/76)\u001b[K\rremote: Counting objects:  50% (38/76)\u001b[K\rremote: Counting objects:  51% (39/76)\u001b[K\rremote: Counting objects:  52% (40/76)\u001b[K\rremote: Counting objects:  53% (41/76)\u001b[K\rremote: Counting objects:  55% (42/76)\u001b[K\rremote: Counting objects:  56% (43/76)\u001b[K\rremote: Counting objects:  57% (44/76)\u001b[K\rremote: Counting objects:  59% (45/76)\u001b[K\rremote: Counting objects:  60% (46/76)\u001b[K\rremote: Counting objects:  61% (47/76)\u001b[K\rremote: Counting objects:  63% (48/76)\u001b[K\rremote: Counting objects:  64% (49/76)\u001b[K\rremote: Counting objects:  65% (50/76)\u001b[K\rremote: Counting objects:  67% (51/76)\u001b[K\rremote: Counting objects:  68% (52/76)\u001b[K\rremote: Counting objects:  69% (53/76)\u001b[K\rremote: Counting objects:  71% (54/76)\u001b[K\rremote: Counting objects:  72% (55/76)\u001b[K\rremote: Counting objects:  73% (56/76)\u001b[K\rremote: Counting objects:  75% (57/76)\u001b[K\rremote: Counting objects:  76% (58/76)\u001b[K\rremote: Counting objects:  77% (59/76)\u001b[K\rremote: Counting objects:  78% (60/76)\u001b[K\rremote: Counting objects:  80% (61/76)\u001b[K\rremote: Counting objects:  81% (62/76)\u001b[K\rremote: Counting objects:  82% (63/76)\u001b[K\rremote: Counting objects:  84% (64/76)\u001b[K\rremote: Counting objects:  85% (65/76)\u001b[K\rremote: Counting objects:  86% (66/76)\u001b[K\rremote: Counting objects:  88% (67/76)\u001b[K\rremote: Counting objects:  89% (68/76)\u001b[K\rremote: Counting objects:  90% (69/76)\u001b[K\rremote: Counting objects:  92% (70/76)\u001b[K\rremote: Counting objects:  93% (71/76)\u001b[K\rremote: Counting objects:  94% (72/76)\u001b[K\rremote: Counting objects:  96% (73/76)\u001b[K\rremote: Counting objects:  97% (74/76)\u001b[K\rremote: Counting objects:  98% (75/76)\u001b[K\rremote: Counting objects: 100% (76/76)\u001b[K\rremote: Counting objects: 100% (76/76), done.\u001b[K\n",
            "remote: Compressing objects: 100% (73/73), done.\u001b[K\n",
            "remote: Total 1878 (delta 47), reused 7 (delta 3), pack-reused 1802\u001b[K\n",
            "Receiving objects: 100% (1878/1878), 77.12 MiB | 34.55 MiB/s, done.\n",
            "Resolving deltas: 100% (1158/1158), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G2xin10SfozR",
        "colab": {}
      },
      "source": [
        "if COLAB:\n",
        "  !cp dl-projects/utils* .\n",
        "  !cp dl-projects/preprocess* .\n",
        "  !cp dl-projects/plot* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fC2-l3JBpqbE",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "# %reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TP7V_IzepqbK",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "import matplotlib\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import featuretools as ft\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, make_scorer, recall_score, roc_curve, mean_squared_error, accuracy_score, average_precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                     ClusterCentroids,\n",
        "                                     TomekLinks,\n",
        "                                     NeighbourhoodCleaningRule,\n",
        "                                     AllKNN,\n",
        "                                     NearMiss,\n",
        "                                     OneSidedSelection,\n",
        "                                     EditedNearestNeighbours)\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
        "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC, ADASYN \n",
        "import pylab as pl\n",
        "import xgboost as xgb\n",
        "from collections import Counter\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "# from skopt import BayesSearchCV\n",
        "# from skopt.space import Real, Categorical, Integer\n",
        "# from scikitplot.plotters import plot_precision_recall_curve\n",
        "\n",
        "from utils import feature_selection, plot_feature_importances\n",
        "from preprocess import *\n",
        "from plot import plot_correlation_matrix, plot_labeled_scatter\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "sns.set(style=\"ticks\")\n",
        "\n",
        "# The Answer to the Ultimate Question of Life, the Universe, and Everything.\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3bFT5CoxpqbP",
        "outputId": "65ba334a-766a-457d-d526-ee071e86ab6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%aimport"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modules to reload:\n",
            "all-except-skipped\n",
            "\n",
            "Modules to skip:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3E16jPVPpqbV"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U421BuhtfYS7",
        "outputId": "a38c1ac1-faa7-4580-caac-0fac62b5b232",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  GDRIVE_DATASET_FOLDER = Path('gdrive/My Drive/datasets/')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9IgnETKkpqbX",
        "outputId": "7045d677-78c5-49b1-af35-8734fa189dbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "if COLAB:\n",
        "  DATASET_PATH = GDRIVE_DATASET_FOLDER\n",
        "  ORIGIN_DATASET_PATH = Path('dl-projects/datasets')\n",
        "else:\n",
        "  DATASET_PATH = Path(\"../datasets\")\n",
        "  ORIGIN_DATASET_PATH = Path('datasets')\n",
        "\n",
        "DATASET = DATASET_PATH/f\"{FEATURE_DATASET_PREFIX}.ft\"\n",
        "ORIGIN_DATASET = ORIGIN_DATASET_PATH/DATASET_NAME\n",
        "\n",
        "if COLAB:\n",
        "  !ls -l gdrive/\"My Drive\"/datasets/ --block-size=M\n",
        "  !ls -l dl-projects/datasets --block-size=M"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4720M\n",
            "-rw------- 1 root root 2454M Jan 12 01:24 feature_matrix_d2_v2.ft\n",
            "-rw------- 1 root root 1585M Jan 12 23:39 feature_matrix_d2_v3.ft\n",
            "-rw------- 1 root root   17M Feb 21 13:07 feature_matrix_snapshot.ft\n",
            "-rw------- 1 root root   17M Feb 21 13:07 feature_matrix_snapshot_origin.pkl\n",
            "-rw------- 1 root root    5M Jan 30 04:33 orig_X_test.ft\n",
            "-rw------- 1 root root  415M Jan 30 04:33 orig_X_train.ft\n",
            "-rw------- 1 root root    1M Jan 30 04:33 orig_y_test.ft\n",
            "-rw------- 1 root root    7M Jan 30 04:33 orig_y_train.ft\n",
            "-rw------- 1 root root    3M Feb 29 08:34 test_X_test.ft\n",
            "-rw------- 1 root root  213M Feb 29 08:34 test_X_train.ft\n",
            "-rw------- 1 root root    1M Feb 29 08:34 test_y_test.ft\n",
            "-rw------- 1 root root    7M Feb 29 08:34 test_y_train.ft\n",
            "total 25M\n",
            "-rw-r--r-- 1 root root  1M Feb 29 12:49 4D.zip\n",
            "-rw-r--r-- 1 root root 25M Feb 29 12:49 labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "urQTD6DQNutw",
        "colab": {}
      },
      "source": [
        "# Read the data\n",
        "data = pd.read_feather(DATASET)\n",
        "origin_data = format_tabular(ORIGIN_DATASET)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vOYlp-8Br61r"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kHiN1VVlG9Kh"
      },
      "source": [
        "### View data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnQXyVqng5Cm",
        "colab": {}
      },
      "source": [
        "# Feature matrix\n",
        "feature_matrix = data.drop(columns=['NumberId', 'month', 'year'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "apMYVNz9HK9e",
        "outputId": "de823a50-b611-4618-d2ee-e985ed4366f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Sort data\n",
        "feature_matrix.sort_values(by=['time', 'MAX(Results.LuckyNo)'], inplace=True)\n",
        "feature_matrix.info()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 959893 entries, 7020 to 956511\n",
            "Columns: 214 entries, time to LAST(Results.PrizeType)_Prize\n",
            "dtypes: datetime64[ns](1), float64(155), int64(56), uint8(2)\n",
            "memory usage: 1.5 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZKTbWRFJNUq",
        "outputId": "c64aee4e-582e-4d81-8687-80b905f714a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print('Distribution')\n",
        "print(feature_matrix['Label'].value_counts())\n",
        "print()\n",
        "print('Positive: ' + str(feature_matrix['Label'].value_counts()[0]) + ' which is ', round(feature_matrix['Label'].value_counts()[0]/len(feature_matrix) * 100,2), '% of the dataset')\n",
        "print('Negative: ' + str(feature_matrix['Label'].value_counts()[1]) + ' which is ', round(feature_matrix['Label'].value_counts()[1]/len(feature_matrix) * 100,2), '% of the dataset')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Positive: 927839 which is  96.66 % of the dataset\n",
            "Negative: 32054 which is  3.34 % of the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "plplpAQ6JrKb",
        "outputId": "de538067-3545-4d35-82a3-4008d68c4d3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "feature_matrix.isna().sum().sort_values(ascending=False)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))               7636\n",
              "CUM_MEAN(TREND(Results.LuckyNo, DrawDate))                7636\n",
              "TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)    7636\n",
              "TREND(Results.CUM_MEAN(TotalStrike), DrawDate)            7636\n",
              "TREND(Results.CUM_SUM(LuckyNo), DrawDate)                 7636\n",
              "                                                          ... \n",
              "CUM_SUM(MIN(Results.DrawNo))                                 0\n",
              "NUM_UNIQUE(Results.DAY(DrawDate))                            0\n",
              "NUM_UNIQUE(Results.MONTH(DrawDate))                          0\n",
              "SUM(Results.PERCENTILE(LuckyNo))                             0\n",
              "time                                                         0\n",
              "Length: 214, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF_zCRksL1Ls"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1aLGsXSOa9K",
        "colab": {}
      },
      "source": [
        "# Fill all NaN with 0\n",
        "feature_matrix = feature_matrix.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5A8LZ805MqjP",
        "outputId": "73036cb5-3f01-46e3-99c7-e16df5de473c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "feature_matrix.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(959893, 214)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rPFOkiGjhuKj",
        "outputId": "839d9904-e58f-4669-ac9c-a82a0874fd87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "feature_matrix_selection = feature_selection(feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label']))\n",
        "# feature_matrix_selection = feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label'])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape:  (959893, 211)\n",
            "0 missing columns with threshold: 90.\n",
            "41 zero variance columns.\n",
            "109 collinear columns removed with threshold: 0.95.\n",
            "Total columns removed:  150\n",
            "Shape after feature selection: (959893, 61).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vT2K0WeJhugH",
        "outputId": "f3cffc28-f79c-48bd-a6da-addb56aa323b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        }
      },
      "source": [
        "feature_matrix_selection.shape, feature_matrix_selection.columns"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((959893, 61),\n",
              " Index(['STD(Results.DrawNo)', 'MAX(Results.DrawNo)', 'MAX(Results.LuckyNo)',\n",
              "        'MIN(Results.DrawNo)', 'MEAN(Results.DrawNo)', 'SKEW(Results.DrawNo)',\n",
              "        'AVG_TIME_BETWEEN(Results.DrawDate)', 'COUNT(Results)',\n",
              "        'SUM(Results.DrawNo)', 'SUM(Results.LuckyNo)',\n",
              "        'TREND(Results.DrawNo, DrawDate)', 'MONTH(first_Results_time)',\n",
              "        'DAY(first_Results_time)', 'TIME_SINCE(first_Results_time)',\n",
              "        'TIME_SINCE_PREVIOUS(first_Results_time)',\n",
              "        'STD(Results.PERCENTILE(DrawNo))',\n",
              "        'STD(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'STD(Results.CUM_SUM(DrawNo))', 'STD(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MAX(Results.PERCENTILE(DrawNo))',\n",
              "        'MAX(Results.PERCENTILE(TotalStrike))',\n",
              "        'MAX(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MIN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MIN(Results.CUM_MEAN(LuckyNo))', 'MODE(Results.MONTH(DrawDate))',\n",
              "        'MODE(Results.DAY(DrawDate))', 'MEAN(Results.TIME_SINCE(DrawDate))',\n",
              "        'MEAN(Results.PERCENTILE(DrawNo))',\n",
              "        'MEAN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MEAN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'SKEW(Results.CUM_SUM(DrawNo))', 'SKEW(Results.CUM_MEAN(LuckyNo))',\n",
              "        'LAST(Results.DAY(DrawDate))',\n",
              "        'LAST(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'LAST(Results.MONTH(DrawDate))', 'LAST(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SUM(Results.TIME_SINCE(DrawDate))',\n",
              "        'SUM(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'TREND(Results.CUM_MEAN(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(DrawNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(TotalStrike), DrawDate)',\n",
              "        'TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)',\n",
              "        'TREND(Results.CUM_SUM(DrawNo), DrawDate)',\n",
              "        'NUM_UNIQUE(Results.MONTH(DrawDate))',\n",
              "        'NUM_UNIQUE(Results.DAY(DrawDate))', 'CUM_SUM(MIN(Results.DrawNo))',\n",
              "        'CUM_SUM(SKEW(Results.DrawNo))',\n",
              "        'CUM_MEAN(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'CUM_MEAN(SUM(Results.LuckyNo))', 'CUM_MEAN(SKEW(Results.DrawNo))',\n",
              "        'PERCENTILE(STD(Results.LuckyNo))', 'PERCENTILE(LAST(Results.DrawNo))',\n",
              "        'PERCENTILE(MAX(Results.TotalStrike))',\n",
              "        'PERCENTILE(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'PERCENTILE(COUNT(Results))', 'PERCENTILE(STD(Results.DrawNo))',\n",
              "        'PERCENTILE(SKEW(Results.DrawNo))', 'PERCENTILE(SUM(Results.DrawNo))',\n",
              "        'PERCENTILE(TREND(Results.DrawNo, DrawDate))'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yZUhYrWFiRod",
        "colab": {}
      },
      "source": [
        "feature_matrix_selection['time'] = feature_matrix['time']\n",
        "feature_matrix_selection['TotalStrike'] = feature_matrix['TotalStrike']\n",
        "feature_matrix_selection['Label'] = feature_matrix['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hugygOqSiR6K"
      },
      "source": [
        "### Feature Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "loagcqTEKOkO",
        "outputId": "92c625f5-6771-423f-ba00-37becad84405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "feature_matrix.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LAST(Results.PrizeType)_Prize                  0\n",
              "SKEW(Results.TIME_SINCE(DrawDate))             0\n",
              "MEAN(Results.CUM_MEAN(TotalStrike))            0\n",
              "MEAN(Results.CUM_MEAN(LuckyNo))                0\n",
              "MEAN(Results.CUM_SUM(DrawNo))                  0\n",
              "                                              ..\n",
              "CUM_SUM(COUNT(Results))                        0\n",
              "CUM_SUM(MAX(Results.DrawNo))                   0\n",
              "CUM_SUM(MEAN(Results.LuckyNo))                 0\n",
              "CUM_SUM(AVG_TIME_BETWEEN(Results.DrawDate))    0\n",
              "time                                           0\n",
              "Length: 214, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7Ha8Zlkhuoe",
        "colab": {}
      },
      "source": [
        "# Check with feature selection\n",
        "corrs = feature_matrix_selection.corr().sort_values('Label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EWRODfAdPk6j",
        "outputId": "85a2bb2e-9f5e-4d73-d7c3-a73ef79c1f46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "corrs['Label'].head(60)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CUM_MEAN(SUM(Results.LuckyNo))                           -0.003288\n",
              "TIME_SINCE(first_Results_time)                           -0.002944\n",
              "STD(Results.DrawNo)                                      -0.002877\n",
              "STD(Results.CUM_SUM(DrawNo))                             -0.002778\n",
              "MAX(Results.LuckyNo)                                     -0.002680\n",
              "SUM(Results.LuckyNo)                                     -0.002426\n",
              "MAX(Results.DrawNo)                                      -0.002377\n",
              "MIN(Results.CUM_MEAN(LuckyNo))                           -0.002333\n",
              "CUM_MEAN(AVG_TIME_BETWEEN(Results.DrawDate))             -0.002238\n",
              "MEAN(Results.TIME_SINCE(DrawDate))                       -0.002056\n",
              "STD(Results.PERCENTILE(DrawNo))                          -0.001937\n",
              "PERCENTILE(STD(Results.LuckyNo))                         -0.001931\n",
              "PERCENTILE(STD(Results.DrawNo))                          -0.001814\n",
              "CUM_SUM(SKEW(Results.DrawNo))                            -0.001741\n",
              "SUM(Results.TIME_SINCE(DrawDate))                        -0.001492\n",
              "MEAN(Results.CUM_MEAN(LuckyNo))                          -0.001477\n",
              "AVG_TIME_BETWEEN(Results.DrawDate)                       -0.001429\n",
              "PERCENTILE(SKEW(Results.DrawNo))                         -0.001354\n",
              "SKEW(Results.CUM_MEAN(LuckyNo))                          -0.001339\n",
              "SKEW(Results.CUM_SUM(DrawNo))                            -0.001294\n",
              "SKEW(Results.DrawNo)                                     -0.001030\n",
              "MAX(Results.PERCENTILE(DrawNo))                          -0.001006\n",
              "LAST(Results.DAY(DrawDate))                              -0.001000\n",
              "TREND(Results.CUM_SUM(DrawNo), DrawDate)                 -0.000958\n",
              "PERCENTILE(AVG_TIME_BETWEEN(Results.DrawDate))           -0.000921\n",
              "NUM_UNIQUE(Results.DAY(DrawDate))                        -0.000833\n",
              "MODE(Results.MONTH(DrawDate))                            -0.000827\n",
              "DAY(first_Results_time)                                  -0.000677\n",
              "TREND(Results.DrawNo, DrawDate)                          -0.000630\n",
              "SUM(Results.DrawNo)                                      -0.000541\n",
              "COUNT(Results)                                           -0.000535\n",
              "CUM_MEAN(SKEW(Results.DrawNo))                           -0.000398\n",
              "LAST(Results.MONTH(DrawDate))                            -0.000351\n",
              "NUM_UNIQUE(Results.MONTH(DrawDate))                      -0.000248\n",
              "SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))              -0.000226\n",
              "MEAN(Results.DrawNo)                                     -0.000160\n",
              "TREND(Results.PERCENTILE(TotalStrike), DrawDate)         -0.000156\n",
              "PERCENTILE(COUNT(Results))                               -0.000139\n",
              "LAST(Results.TIME_SINCE_PREVIOUS(DrawDate))              -0.000131\n",
              "PERCENTILE(LAST(Results.DrawNo))                         -0.000094\n",
              "TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)    0.000169\n",
              "STD(Results.TIME_SINCE_PREVIOUS(DrawDate))                0.000213\n",
              "MEAN(Results.TIME_SINCE_PREVIOUS(DrawDate))               0.000345\n",
              "SUM(Results.TIME_SINCE_PREVIOUS(DrawDate))                0.000442\n",
              "MODE(Results.DAY(DrawDate))                               0.000471\n",
              "PERCENTILE(SUM(Results.DrawNo))                           0.000610\n",
              "TREND(Results.PERCENTILE(LuckyNo), DrawDate)              0.000687\n",
              "MONTH(first_Results_time)                                 0.000858\n",
              "PERCENTILE(TREND(Results.DrawNo, DrawDate))               0.000912\n",
              "MIN(Results.TIME_SINCE_PREVIOUS(DrawDate))                0.000984\n",
              "TIME_SINCE_PREVIOUS(first_Results_time)                   0.000987\n",
              "LAST(Results.CUM_MEAN(LuckyNo))                           0.000988\n",
              "MAX(Results.CUM_MEAN(LuckyNo))                            0.001007\n",
              "MEAN(Results.PERCENTILE(DrawNo))                          0.001149\n",
              "TREND(Results.CUM_MEAN(LuckyNo), DrawDate)                0.001493\n",
              "TREND(Results.PERCENTILE(DrawNo), DrawDate)               0.001528\n",
              "PERCENTILE(MAX(Results.TotalStrike))                      0.001564\n",
              "MIN(Results.DrawNo)                                       0.001718\n",
              "CUM_SUM(MIN(Results.DrawNo))                              0.001852\n",
              "MAX(Results.PERCENTILE(TotalStrike))                      0.002320\n",
              "Name: Label, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "waeD1ED_kqDB"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9yrJyIVLh5So",
        "colab": {}
      },
      "source": [
        "def recall_optim(y_true, y_pred):\n",
        "    \"\"\"Make a scoring function that improves specificity while identifying all strikes\n",
        "    \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Recall will be worth a greater value than specificity\n",
        "    rec = recall_score(y_true, y_pred) * 0.8 \n",
        "    spe = conf_matrix[0,0]/conf_matrix[0,:].sum() * 0.2 \n",
        "    \n",
        "    # Imperfect recalls will lose a penalty\n",
        "    # This means the best results will have perfect recalls and compete for specificity\n",
        "    if rec < 0.8:\n",
        "        rec -= 0.2\n",
        "    return rec + spe \n",
        "\n",
        "\n",
        "# Make a scoring callable from recall_score\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Create a scoring callable based on the scoring function\n",
        "optimize = make_scorer(recall_optim)\n",
        "\n",
        "# Geometric mean scorer\n",
        "geo_mean_scorer = make_scorer(geometric_mean_score)\n",
        "\n",
        "# DataFrame to store classifier performance\n",
        "performance = pd.DataFrame(columns=['Train_Recall','Test_Recall','Test_Specificity', 'Optimize'])\n",
        "\n",
        "def to_labels(pos_probs, threshold):\n",
        "    \"\"\"Apply threshold to positive probabilities to create labels.\n",
        "    \"\"\"\n",
        "    return (pos_probs >= threshold).astype('int')\n",
        " \n",
        "\n",
        "def score_optimization(dt, feature_matrix, clf, params, X_train, y_train, X_test, y_test, skip_grid_search_cv=False, optimized_scorer=False):\n",
        "    \"\"\"Find the optimized classifier.\n",
        "    \"\"\"\n",
        "    if not skip_grid_search_cv:\n",
        "      print(\"\\nFinding the optimized classifier...\")\n",
        "\n",
        "      # Load GridSearchCV\n",
        "      # search = GridSearchCV(\n",
        "      search = RandomizedSearchCV(\n",
        "            estimator=clf,\n",
        "            #param_grid=params,\n",
        "            param_distributions=params,\n",
        "            n_jobs=4,\n",
        "            scoring=optimize  # Use custom scorer\n",
        "      )\n",
        "\n",
        "      # Train search object\n",
        "      search.fit(X_train, y_train)\n",
        "\n",
        "      # Heading\n",
        "      print('\\n','-'*40,'\\n',clf.__class__.__name__,'\\n','-'*40)\n",
        "\n",
        "      # Extract best estimator\n",
        "      best = search.best_estimator_\n",
        "      print('Best parameters: \\n\\n',search.best_params_,'\\n')\n",
        "    \n",
        "    else:\n",
        "      print(\"\\nUse the passed in classifier...\\n\")\n",
        "      best = clf\n",
        "\n",
        "    # Cross-validate on the train data\n",
        "    if not skip_grid_search_cv: \n",
        "      print(\"TRAIN GROUP\")\n",
        "      #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
        "      cv = 3\n",
        "      if not optimized_scorer:\n",
        "        print('\\nUse default scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  scoring=recall,\n",
        "                                  cv=cv)\n",
        "      else:\n",
        "        print('\\nUse optimized scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  #scoring=optimize,\n",
        "                                  scoring='roc_auc',\n",
        "                                  #scoring=geo_mean_scorer,\n",
        "                                  cv=cv)\n",
        "\n",
        "      print(\"\\nCross-validation recall scores:\",train_cv)\n",
        "      print(\"Mean recall score:\",train_cv.mean())\n",
        "      print('Mean G-Mean: %.3f (%.3f)' % (np.mean(train_cv), np.std(train_cv)))\n",
        "    else:\n",
        "      train_cv = np.zeros(3)\n",
        "\n",
        "    # Now predict on the test group\n",
        "    print(\"\\nTEST GROUP\")\n",
        "    y_pred = best.fit(X_train, y_train).predict(X_test)\n",
        "    # y_pred = best.fit(X_train, y_train,\n",
        "    #                   eval_set=[(X_test, y_test)],\n",
        "    #                   eval_metric='auc',\n",
        "    #                   early_stopping_rounds=10,\n",
        "    #                   verbose=True\n",
        "    #                   ).predict(X_test)\n",
        "\n",
        "    # keep probabilities for the positive outcome only\n",
        "    probas = best.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # define thresholds\n",
        "    thresholds = np.arange(0, 1, 0.001)\n",
        "\n",
        "    # evaluate each threshold\n",
        "    scores = [f1_score(y_test, to_labels(probas, t)) for t in thresholds]\n",
        "\n",
        "    # get best threshold\n",
        "    ix = np.argmax(scores)\n",
        "    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))\n",
        "\n",
        "    # print recall\n",
        "    print(\"\\nRecall:\",recall_score(y_test,y_pred))\n",
        "\n",
        "    # Get imbalanced classification report\n",
        "    print(classification_report_imbalanced(y_test, y_pred))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test,y_pred)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=plt.cm.copper)\n",
        "    plt.show()\n",
        "\n",
        "    # Store results\n",
        "    performance.loc[clf.__class__.__name__+'_optimize',:] = [\n",
        "        train_cv.mean(),\n",
        "        recall_score(y_test,y_pred),\n",
        "        conf_matrix[0,0]/conf_matrix[0,:].sum(),\n",
        "        recall_optim(y_test,y_pred)\n",
        "    ]\n",
        "    # Look at the parameters for the top best scores\n",
        "    if not skip_grid_search_cv:\n",
        "      display(pd.DataFrame(search.cv_results_).iloc[:,4:].sort_values(by='rank_test_score').head())\n",
        "    display(performance)\n",
        "\n",
        "    # Additionl info\n",
        "    print('\\n\\nAdditional Info')\n",
        "    print('='*40)\n",
        "    positive = np.where((y_pred==1))\n",
        "    print(f'Total predicted to be positive: {len(positive[0])} \\n')\n",
        "\n",
        "    pred = np.where((y_pred==1))\n",
        "    all_preds = pred[0]\n",
        "\n",
        "    # Total predicted matches\n",
        "    print('First 23 matches')\n",
        "    print(23, all_preds[0:23])\n",
        "    print(f'\\n{probas[all_preds[0:23]]}\\n') \n",
        "\n",
        "    print(\"\\nTop 23 Probable Matches\")\n",
        "    #print('probas', probas)\n",
        "    topN = np.argpartition(probas, -23)[-23:]\n",
        "    print(f'\\n{topN}\\n')          # Top N most high probability numbers\n",
        "    print(f'\\n{probas[topN]}\\n')  # Top N probability\n",
        "\n",
        "    # Check for 2 to 3 digits range \n",
        "    print('\\n2 To 3 Digits\\n')\n",
        "    idx_range = np.where((all_preds < 1000) & (all_preds >= 10))\n",
        "    #print(idx_range)\n",
        "    range_numbers = all_preds[idx_range]\n",
        "    print(len(range_numbers), range_numbers)\n",
        "    print(f'\\n{probas[range_numbers]}\\n') \n",
        "\n",
        "\n",
        "    # 2 to 3 Digits > Average Probas\n",
        "    print('\\n2 To 3 Digits Average Proba\\n')\n",
        "    avg_proba = np.average(probas[range_numbers])\n",
        "    print(f'Average proba {avg_proba}\\n')\n",
        "    idx_avg_proba = np.where(probas > avg_proba) \n",
        "    print(len(idx_avg_proba[0]), idx_avg_proba[0])\n",
        "\n",
        "    # 2 to 3 Digits > All Average Probas\n",
        "    print('\\n\\nAll Average Proba\\n')\n",
        "    all_avg_proba = np.average(probas[all_preds])\n",
        "    print(f'All average probas {all_avg_proba}\\n')\n",
        "    idx_all_avg_proba = np.where(probas > all_avg_proba) \n",
        "    print(len(idx_all_avg_proba[0]), idx_all_avg_proba[0])\n",
        "\n",
        "\n",
        "    # All predicted matches\n",
        "    print('\\n\\nAll Predictions\\n')\n",
        "    print(len(all_preds), all_preds)\n",
        "    print()\n",
        "    print(len(probas[all_preds]), probas[all_preds])\n",
        "    \n",
        "    #print('Debug')\n",
        "    #print(pred)\n",
        "    \n",
        "    if len(positive[0]) > 0:\n",
        "    \n",
        "      # Matching draws\n",
        "      print('\\nMatched draws')\n",
        "      md = np.where((y_pred==1) & (y_test==1))\n",
        "      print(f\"Count: {len(md[0])}, Index: {md}\")\n",
        "      month_data = feature_matrix.loc[feature_matrix['time'] == dt]\n",
        "      numbers = month_data.iloc[md[0]][['MAX(Results.LuckyNo)']]\n",
        "\n",
        "      print('\\n\\nTop 23 Possibility')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(topN))].head(23))  \n",
        "      \n",
        "      print('\\n\\nFirst 23 Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(pred[0][0:23]))].head(23))    \n",
        "             \n",
        "\n",
        "      print('\\n\\n2 To 3 Digits Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(range_numbers))].head(23))    \n",
        "     \n",
        "\n",
        "      print('\\n\\nAll matched')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(numbers['MAX(Results.LuckyNo)']))].head(100))    \n",
        "                                                  \n",
        "    else:\n",
        "      print('No luck this month')  \n",
        "\n",
        "    if len(range_numbers) >= 50:\n",
        "      return False\n",
        "\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrL8gYwjc-hd",
        "colab": {}
      },
      "source": [
        "def remove_outliers(balanced, threshold=0.001, remove=True):\n",
        "    \"\"\"Removing Outliers from high-correlation features.\n",
        "    \"\"\"\n",
        "\n",
        "    if not remove:\n",
        "      return balanced\n",
        "\n",
        "    bal_corr = balanced.corr()\n",
        "    no_outliers=pd.DataFrame(balanced.copy())\n",
        "\n",
        "    cols = bal_corr.Label.index[:-1]\n",
        "\n",
        "    # For each feature correlated with Class...\n",
        "    for col in cols:\n",
        "        # If absolute correlation value is more than X percent...\n",
        "        correlation = bal_corr.loc['Label',col]\n",
        "\n",
        "        if np.absolute(correlation) > threshold:\n",
        "          # Separate the classes of the high-correlation column\n",
        "          nonstrikes = no_outliers.loc[no_outliers.Label==0,col]\n",
        "          strikes = no_outliers.loc[no_outliers.Label==1,col]\n",
        "\n",
        "          # Identify the 25th and 75th quartiles\n",
        "          all_values = no_outliers.loc[:,col]\n",
        "          q25, q75 = np.percentile(all_values, 25), np.percentile(all_values, 75)\n",
        "          # Get the inter quartile range\n",
        "          iqr = q75 - q25\n",
        "          # Smaller cutoffs will remove more outliers\n",
        "          cutoff = iqr * 7\n",
        "          # Set the bounds of the desired portion to keep\n",
        "          lower, upper = q25 - cutoff, q75 + cutoff\n",
        "          \n",
        "          # If positively correlated...\n",
        "          # Drop nonstrikes above upper bound, and strikes below lower bound\n",
        "          if correlation > 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes>upper].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes<lower].index,inplace=True)\n",
        "          \n",
        "          # If negatively correlated...\n",
        "          # Drop non strikes below lower bound, and strikes above upper bound\n",
        "          elif correlation < 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes<lower].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes>upper].index,inplace=True)\n",
        "        \n",
        "    print('\\nData shape before removing outliers:', balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in previous data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "    print('-'*40)\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after removing outliers:', no_outliers.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(no_outliers.Label.value_counts())\n",
        "\n",
        "    # no_outliers.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distributions with Less Outliers', fontsize=17)\n",
        "    # plt.show()\n",
        "    \n",
        "    no_outliers.reset_index(drop=True, inplace=True)\n",
        "    return no_outliers\n",
        "\n",
        "\n",
        "def filter_features(no_outliers, threshold=0.001):\n",
        "    \"\"\"Feature selection.\n",
        "    \"\"\"\n",
        "    feat_sel = pd.DataFrame(no_outliers.copy())\n",
        "\n",
        "    # Make a dataframe with the label-correlations before removing outliers\n",
        "    # corr_change = pd.DataFrame()\n",
        "    # corr_change['correlation']= bal_corr.Label\n",
        "    # corr_change['origin']= 'w/outliers'\n",
        "\n",
        "    # Make a dataframe with label-correlations after removing outliers \n",
        "    # corr_other = pd.DataFrame()\n",
        "    # corr_other['correlation']= feat_sel.corr().Label\n",
        "    # corr_other['origin']= 'no_outliers'\n",
        "\n",
        "    # Join them\n",
        "    # corr_change = corr_change.append(corr_other)\n",
        "\n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.xticks(rotation=90)\n",
        "\n",
        "    # Plot them\n",
        "    # sns.set_style('darkgrid')\n",
        "    # plt.title('Label correlation per feature. With vs without outliers', fontsize=17)\n",
        "    # sns.barplot(data=corr_change,x=corr_change.index,y='correlation',hue='origin')\n",
        "    # plt.show()\n",
        "\n",
        "    # Feature Selection based on correlation with label\n",
        "\n",
        "    print('\\nData shape before feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes before feature selection:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # Correlation matrix after removing outliers\n",
        "    new_corr = feat_sel.corr()\n",
        "\n",
        "    for col in new_corr.Label.index[:-1]:\n",
        "        # Pick desired cutoff for dropping features. In absolute-value terms.\n",
        "        if np.absolute(new_corr.loc['Label',col]) < threshold:\n",
        "            # Drop the feature if correlation is below cutoff\n",
        "            feat_sel.drop(columns=col,inplace=True)\n",
        "\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in new data:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "\n",
        "    return feat_sel\n",
        "\n",
        "    # feat_sel.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distribution of Features Selected', fontsize=17)\n",
        "    # plt.show()\n",
        "\n",
        "def under_sampler(data, sample_size=20000, sampling=False):\n",
        "    # Undersample model for efficiency and balance classes.\n",
        "\n",
        "    X_train = data.drop('Label',1)\n",
        "    y_train = data.Label\n",
        "\n",
        "    if not sampling:\n",
        "      return X_train, y_train\n",
        "\n",
        "    # After feature-selection, X_test needs to include only the same features as X_train\n",
        "    # cols = X_train.columns\n",
        "    # X_test = X_test[cols]\n",
        "\n",
        "    # Undersample and balance classes\n",
        "    X_train, y_train = RandomUnderSampler(sampling_strategy={1:sample_size,0:sample_size}).fit_resample(X_train,y_train)\n",
        "\n",
        "    print('\\nX_train shape after reduction:', X_train.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in y_train:')\n",
        "    print(np.unique(y_train, return_counts=True))\n",
        "\n",
        "    return X_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pl5ZoepSNPf4",
        "colab": {}
      },
      "source": [
        "def gen_train_test_set(dt, feature_matrix, file_prefix='data'): \n",
        "    \n",
        "    # Subset labels\n",
        "    test_labels = feature_matrix.loc[feature_matrix['time'] == dt, 'Label']\n",
        "    train_labels = feature_matrix.loc[feature_matrix['time'] < dt, 'Label']\n",
        "\n",
        "    # Features\n",
        "    X_train = feature_matrix[feature_matrix['time'] < dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    X_test = feature_matrix[feature_matrix['time'] == dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    feature_names = list(X_train.columns)\n",
        "    \n",
        "    # Labels\n",
        "    y_train = np.array(train_labels).reshape((-1, ))\n",
        "    y_test = np.array(test_labels).reshape((-1, ))\n",
        "    \n",
        "    print('Training on {} observations.'.format(len(X_train)))\n",
        "    print('Testing on {} observations.\\n'.format(len(X_test)))\n",
        "\n",
        "    # Join the train data\n",
        "    train = X_train.join(train_labels)\n",
        "\n",
        "    print('Data shape before balancing:', train.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in previous data:')\n",
        "    print(train.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # sklearn pipeline\n",
        "    pipeline = make_pipeline(\n",
        "        SimpleImputer(strategy = 'constant', fill_value=0),\n",
        "        StandardScaler())\n",
        "    \n",
        "    X_train = pipeline.fit_transform(X_train)\n",
        "    X_test = pipeline.transform(X_test)\n",
        "\n",
        "    # imblearn pipeline\n",
        "    imb_pipeline = make_pipeline_imb(\n",
        "          # NearMiss(version=3, n_neighbors_ver3=3, n_jobs=4)\n",
        "          # SMOTE(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # ADASYN(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # OneSidedSelection(n_neighbors=1, n_seeds_S=200, random_state=42, n_jobs=4)\n",
        "          SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "    )\n",
        "     \n",
        "    # Balance the data\n",
        "    to_balanced = False\n",
        "    if to_balanced:\n",
        "      print('\\nBalancing data')\n",
        "      X_bal, y_bal = imb_pipeline.fit_resample(X_train, y_train)\n",
        "      X_bal = pd.DataFrame(X_bal,columns=feature_names)\n",
        "      y_bal = pd.DataFrame(y_bal,columns=['Label'])\n",
        "    else:\n",
        "      print('\\nNO balancing')\n",
        "      X_bal = pd.DataFrame(X_train,columns=feature_names)\n",
        "      y_bal = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    balanced = X_bal.join(y_bal)\n",
        "\n",
        "    # print('-'*40)\n",
        "    print('Data shape after balancing:',balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "\n",
        "    # Remove high correlation outliers\n",
        "    no_outliers = remove_outliers(balanced, remove=False)\n",
        "   \n",
        "    # Remove features with low correlation\n",
        "    remove_features = True\n",
        "    if remove_features:\n",
        "      print('\\nFiltering features')\n",
        "      features_selected = filter_features(no_outliers)\n",
        "    else:\n",
        "      print('\\nNO filtering')\n",
        "      features_selected = no_outliers \n",
        "\n",
        "    columns_selected = features_selected.drop('Label',1).columns\n",
        "\n",
        "    # Under sampling\n",
        "    X_train, y_train = under_sampler(features_selected, sampling=False) \n",
        "    X_train = pd.DataFrame(X_train,columns=columns_selected)\n",
        "    y_train = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    # For X_test, now only use the selected features\n",
        "    X_test = pd.DataFrame(X_test,columns=feature_names)\n",
        "    X_test = X_test[columns_selected]\n",
        "    y_test = pd.DataFrame(y_test,columns=['Label'])\n",
        "\n",
        "    #print(X_train.describe())\n",
        "    #return\n",
        "\n",
        "    # Save data\n",
        "    # print(X_train.head(10))\n",
        "    # print(y_train.head(10)) \n",
        "\n",
        "    # print(X_test.head(10))\n",
        "    # print(y_test.head(10)) \n",
        "    X_train.to_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "    y_train.to_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "   \n",
        "    X_test.to_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "    y_test.to_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "\n",
        "    gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PcKlL67TP9UM",
        "colab": {}
      },
      "source": [
        "def model(dt, feature_matrix, file_prefix='data', csv=False, class_weight=1.0):\n",
        "    \"\"\"Predict for a particular month.\n",
        "\n",
        "    - https://www.kaggle.com/miguelniblock/optimizing-imbalanced-classification-100-recall\n",
        "    - https://www.kaggle.com/saxinou/imbalanced-data-xgboost-tunning\n",
        "    - https://www.kaggle.com/andreanuzzo/balance-the-imbalanced-rf-and-xgboost-with-smote\n",
        "    - https://github.com/mengwangk/FraudDetection/blob/master/05_Sampling_techniques_for_extremely_imbalanced_data.ipynb\n",
        "    - https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
        "    - https://github.com/coding-maniacs/over-under-sampling/blob/master/src/main.py\n",
        "    - https://github.com/scikit-learn-contrib/imbalanced-learn/issues/552#issuecomment-466348310\n",
        "    - https://stackoverflow.com/questions/52499788/smotetomek-how-to-set-ratio-as-dictionary-for-fixed-balance\n",
        "    - https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.OneSidedSelection.html#imblearn.under_sampling.OneSidedSelection\n",
        "    - https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
        "    - https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/\n",
        "    - https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "    - https://stats.stackexchange.com/questions/243207/what-is-the-proper-usage-of-scale-pos-weight-in-xgboost-for-imbalanced-datasets\n",
        "    - https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py\n",
        "    - https://machinelearningmastery.com/cost-sensitive-logistic-regression/\n",
        "    \n",
        "    - https://datascience.stackexchange.com/questions/28285/what-is-the-best-way-to-deal-with-imbalanced-data-for-xgboost/28292\n",
        "    - https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
        "    - https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Read data\n",
        "    if not csv:\n",
        "      X_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "      y_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "    \n",
        "      X_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "      y_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "    else:\n",
        "      X_train = pd.read_csv(DATASET_PATH/f\"{file_prefix}_X_train.csv\", header=0, sep=',', quotechar='\"')\n",
        "      y_train = pd.read_csv(DATASET_PATH/f\"{file_prefix}_y_train.csv\", header=0, sep=',', quotechar='\"')\n",
        "    \n",
        "      X_test = pd.read_csv(DATASET_PATH/f\"{file_prefix}_X_test.csv\", header=0, sep=',', quotechar='\"')\n",
        "      y_test = pd.read_csv(DATASET_PATH/f\"{file_prefix}_y_test.csv\", header=0, sep=',', quotechar='\"')\n",
        "\n",
        "\n",
        "    print(f'\\n-----------{dt}-----------------\\n')\n",
        "\n",
        "    # Reshape\n",
        "    y_train = np.array(y_train).reshape((-1, ))\n",
        "    y_test = np.array(y_test).reshape((-1, ))\n",
        "    \n",
        "    print('Data shape')\n",
        "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "    # print(X_train.describe())\n",
        "    # return\n",
        "\n",
        "    # Calculate hit ratio for xgboost classifier\n",
        "    print(\"\\nCalculating scale pos weight\")\n",
        "    counter = Counter(y_train)\n",
        "    print(Counter(y_train))\n",
        "    #scale_pos_weight = float(counter[0] / counter[1])\n",
        "    scale_pos_weight = (float(counter[0] / counter[1])) * class_weight\n",
        "    print(f\"\\nscale_pos_weight - {scale_pos_weight}\\n\")\n",
        "    \n",
        "     # Modeling\n",
        "#     clf = xgb.XGBClassifier(\n",
        "#               n_jobs=4, \n",
        "#               random_state=42,\n",
        "#               #learning_rate=0.1,\n",
        "#               #n_estimators=500,\n",
        "#               #max_depth=6, \n",
        "#               #min_child_weight=3, \n",
        "#               #gamma=0,\n",
        "#               #subsample=0.8,\n",
        "#               #colsample_bytree=0.8,\n",
        "#               objective='binary:logistic', \n",
        "#               scale_pos_weight=scale_pos_weight,\n",
        "#               ##eval_metric=\"auc\",\n",
        "#               ##max_delta_step=1,\n",
        "#               seed=27)\n",
        "#     clf = xgb.XGBClassifier(n_jobs=4, \n",
        "#                             random_state=42,\n",
        "#                             objective='binary:logistic', \n",
        "#                             #scale_pos_weight=28)\n",
        "#                             scale_pos_weight=scale_pos_weight)\n",
        "    \n",
        "    clf = xgb.XGBClassifier(\n",
        "                    n_jobs=4, \n",
        "                    random_state=42,\n",
        "                    objective='binary:logistic',\n",
        "                    subsample=0.55, \n",
        "                    n_estimators=300,\n",
        "                    #n_estimators=500,\n",
        "                    min_child_weight=1,\n",
        "                    max_depth=3, \n",
        "                    learning_rate=0.007,\n",
        "                    gamma=0.1, \n",
        "                    colsample_bytree=0.95,\n",
        "                    tree_method='hist',\n",
        "                    booster='dart',\n",
        "                    scale_pos_weight=scale_pos_weight\n",
        "                    )\n",
        "\n",
        "    clf_params = clf.get_params()\n",
        "    print(clf_params)\n",
        "\n",
        "    # Set parameters\n",
        "    #clf_params['max_depth'] = 10\n",
        "    #clf.set_params(clf_params)\n",
        "\n",
        "    # Parameters to compare\n",
        "    weights = [i for i in range(1,36,1)]\n",
        "    weights.append(scale_pos_weight)\n",
        "    learn_params = {\n",
        "        'n_estimators': [100, 300, 500, 800, 1000], \n",
        "        'max_depth': range(3,10,2),\n",
        "        'min_child_weight': range(1,6,2),\n",
        "        #'gamma':[i/10.0 for i in range(0,5)],\n",
        "        'subsample':[i/100.0 for i in range(55,70,5)],\n",
        "        'colsample_bytree':[i/100.0 for i in range(85,100,5)],\n",
        "        #'learning_rate':[i/1000.0 for i in range(5,20,2)],\n",
        "        'scale_pos_weight': weights\n",
        "    }\n",
        "    print(f'Parameter distribution: {learn_params}')\n",
        "    \n",
        "    # Test and validate\n",
        "    ret_val = score_optimization(dt,\n",
        "                       feature_matrix,\n",
        "                       clf, \n",
        "                       learn_params,  \n",
        "                       X_train, \n",
        "                       y_train, \n",
        "                       X_test, \n",
        "                       y_test, \n",
        "                       skip_grid_search_cv=True,\n",
        "                       optimized_scorer=True)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return ret_val\n",
        "    \n",
        "    # clf.fit(X_train, y_train)\n",
        "    # y_pred = clf.predict(X_test)\n",
        "\n",
        "    # # ROC score\n",
        "    # auc = roc_auc_score(y_test, y_pred)\n",
        "    # print(\"ROC score: \", auc)\n",
        "\n",
        "    # # Print confusion matrix\n",
        "    # conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    # sns.heatmap(conf_matrix, annot=True,fmt='d', cmap=plt.cm.copper)\n",
        "    # plt.show()\n",
        "\n",
        "    # Parameters to compare\n",
        "    # params = {\n",
        "    #     'criterion':['entropy','gini'],\n",
        "    #     'class_weight':[{1:1,0:0.3},{1:1,0:0.4},{1:1,0:0.5},{1:1,0:0.6},{1:1,0:7}]\n",
        "    # }\n",
        "\n",
        "    # Implement the classifier\n",
        "    # clf = RandomForestClassifier(\n",
        "    #     n_estimators=100,\n",
        "    #     max_features=None,\n",
        "    #     n_jobs=4,\n",
        "    # )\n",
        "\n",
        "    # # Test and validate\n",
        "    # score_optimization(clf, params, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m9UobqUWMI9b",
        "jupyter": {
          "source_hidden": true
        },
        "colab": {}
      },
      "source": [
        "# Predict for a particular month\n",
        "\n",
        "# %time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection)\n",
        "\n",
        "#%time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='test')\n",
        "#%time model(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='orig')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ns3Puh7Gnxl5",
        "colab": {}
      },
      "source": [
        "#%time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='test')\n",
        "#%time model(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE6Xbz-IyvLj",
        "colab_type": "code",
        "outputId": "32dfdac2-42f0-4b5d-dda5-7f869ca33a56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "weight=1.0\n",
        "decrement = 0.000\n",
        "to_stop=False\n",
        "\n",
        "dt = pd.datetime(2019,6,1)\n",
        "%time gen_train_test_set(dt, feature_matrix_selection, file_prefix='test')\n",
        "while not to_stop:\n",
        "  to_stop = model(dt, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "  decrement = decrement + 0.001"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 889893 observations.\n",
            "Testing on 10000 observations.\n",
            "\n",
            "Data shape before balancing: (889893, 62)\n",
            "\n",
            "Counts of strikes vs non-strikes in previous data:\n",
            "0    860060\n",
            "1     29833\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "NO balancing\n",
            "Data shape after balancing: (889893, 62)\n",
            "\n",
            "Counts of strikes VS non-strikes in new data:\n",
            "0    860060\n",
            "1     29833\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Filtering features\n",
            "\n",
            "Data shape before feature selection: (889893, 62)\n",
            "\n",
            "Counts of strikes vs non-strikes before feature selection:\n",
            "0    860060\n",
            "1     29833\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "\n",
            "Data shape after feature selection: (889893, 31)\n",
            "\n",
            "Counts of strikes vs non-strikes in new data:\n",
            "0    860060\n",
            "1     29833\n",
            "Name: Label, dtype: int64\n",
            "CPU times: user 12.5 s, sys: 160 ms, total: 12.7 s\n",
            "Wall time: 19.3 s\n",
            "\n",
            "-----------2019-06-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(889893, 30) (889893,) (10000, 30) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 860060, 1: 29833})\n",
            "\n",
            "scale_pos_weight - 28.829148929038315\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 300, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.829148929038315, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.829148929038315]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.489, F-Score=0.06839\n",
            "\n",
            "Recall: 0.017857142857142856\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.97      0.99      0.02      0.98      0.13      0.02      9664\n",
            "          1       0.05      0.02      0.99      0.03      0.13      0.02       336\n",
            "\n",
            "avg / total       0.94      0.96      0.05      0.95      0.13      0.02     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAYwklEQVR4nO3dfXRU9b3v8U9mQsJDmRMmkBgSK8Se\nQjiID+SW3qNIBUu8pyFIT+8KTbH0Wh85iWArD7VAJGiXAY4FCYralqZHqVLXUUpEgu3xHhUrChYE\nEgpGHoKZEMwDAxISmNn3D9edloZkMbPzyzA77xdr/zH7t3fmO6wwH777tx/iLMuyBABAhFzRLgAA\nENsIEgCALQQJAMAWggQAYAtBAgCwhSABANgS36Pv9t4jPfp26B3cN5ZEuwQ4UCAQ7L4fFu5339fD\n3D7KejZIAKA3cvjlehzaAgDYQkcCAKY5vCMhSADANGfnCEECAMY5vCNhjgQAYAsdCQCY5vCOhCAB\nANOcnSMECQAYR0cCALDF2TlCkACAcXQkAABbCBIAgC3OzhGCBACMc3hHwgWJAABb6EgAwDSHdyQE\nCQCY5uwcIUgAwDiHdyTMkQAAbKEjAQDTHN6RECQAYFqQIAEA2OHsHCFIAMA8ZycJQQIApjk7RwgS\nADCOyXYAgC3OzhGCBADMc3aSECQAYJqzc4QgAQDjmCMBANji7BwhSADAODoSAIAtBAkAwBZn5wi3\nkQcA4ywrvCUMb775pm6//XZNnTpVeXl52rp1qyTp0KFDys/PV05OjvLz83X48OHQPpGOdYYgAYAY\nZVmW5s2bp2XLlmnjxo1atmyZ5s+fr2AwqOLiYhUUFKiyslIFBQVavHhxaL9IxzpDkACAaQY7EpfL\npVOnTkmSTp06pZSUFDU3N6uqqkq5ubmSpNzcXFVVVampqUmNjY0RjXWFORIAMC3MORK/3y+/399h\nvcfjkcfjCb2Oi4vTypUrNWvWLPXv31+ff/65nn32Wfl8PqWmpsrtdkuS3G63UlJS5PP5ZFlWRGNe\nr7fTegkSADAtzC6jvLxcZWVlHdYXFhaqqKgo9Pr8+fN65pln9NRTT2ns2LHauXOn5syZo2XLltku\nORwECQCYFmZHMnPmTE2bNq3D+r/tRiSpurpaDQ0NGjt2rCRp7Nix6tevnxITE3X8+HEFAgG53W4F\nAgE1NDQoLS1NlmVFNNYV5kgAwDgrrMXj8SgjI6PD8vdBcsUVV6i+vl6ffPKJJKmmpkaNjY266qqr\nlJWVpYqKCklSRUWFsrKy5PV6lZycHNFYV+IsqwevlHnvkR57K/Qe7htLol0CHCgQCHbfD1s/K7zt\nC5665E1///vf67nnnlNcXJwk6YEHHtCtt96qmpoaLViwQH6/Xx6PR6WlpcrMzJSkiMc6Q5Ag5hEk\nMKFbg+T5+8PbfsbT3ffePYA5EgAwzeG3SGGOBABgCx0JABgW7gxCnKE6TCFIAMCwcI9sESQAgAv0\n5DlN0UCQAIBhzo4RggQAjKMjAQDYEnR2jhAkAGCawxsSggQATAs6PEm4INGwmrqT+v7jf9TY+17W\nN+du0hs7aiVJx06c1oiZv9X19/wutKzZuLfD/i2n2/T1wv/Udx9944L1m7cf1f9a8Jquv/d3+pef\nvKY/7DzWI58Hl6dZs/5N27e/rzNnWvWrX/0qtL5Pnz7asGGDamo+USAQ1IQJEzrse/311+vNN/+v\nTp70q67Op6KiB3qy9F4haFlhLbGGjsSg84GgZq18W9MnfkXr5t2i9/c36P6fv6VXMv5BfdxfZPgH\nT/+r4t2d5/mKDbt1dZrngl+u401nNO+ZP2nN7PG6eUya/nt3nWav2ab/+vc8JXv6Gv9cuPz4fHX6\n2c8e0+TJOerX78LfgXfe2aZVq1bppZc2dNgvOTlZmze/rh//+Ed6+eWXlZCQoIyMjJ4qu9eIwWwI\nCx2JQZ/4/GpoadUPckbI7XLpf466Qjf84xBt3Hb4kvb/8OAJHTzWom+Pv/DOm/XNZzSwfx9NuHao\n4uLi9I3r0tUvMV5HG04b+BSIBa+88oo2btyoxsbGC9afO3dOTz65Stu2bVMgEOiw34MP/khbt1Zq\n/fr1am9v1+nTp7V///6eKrvXsCwrrCXWXFKQNDc3q7q6WtXV1WpubjZdk6NZsnTw2MnQ61t+9Hvd\nPOdV/eS599R0qi20PhAMaul/7NSiO7IV93eXuY4e7tXVQz3644fHFAgG9Yedx5QQ79KIK5N66mPA\nIcaNG6empma9/fY78vnqtXHjRl155ZXRLstxwnsaSezp8tDW0aNHtWjRIlVVVSklJUWS1NDQoFGj\nRmnJkiUaNmzYRffr7HnDva1hHn6FR15Pon6xuVo/yBmp7dXH9cH+ExqXlaJBAxP18iOTlfXlQWo5\n3aaS3+zU3LXv6pdzb5Ek/cfWAxqTmazRw706cKzlgp/rdrk09cbhemjtn9R2LqA+8S6t+rcb1T+R\nI5UIT0ZGhm644Qbl5EzWnj17VFq6TC+8sF433zw+2qU5SizOe4Sjy2+eefPmqaCgQOvWrZPL9UXz\nEgwGtWnTJs2fP18vvfTSRffr7HnDfyn/bjeUHDv6xLu05oHxevT5nfrFa9UaPdyr2752pRL6uDWg\nbx9dMzxZkjT4H/pp0R1jddPsV3W69Zw+P3tOv3njgP5zSc5Ff+67++q14qVd+s1PJuqfrvJq7+Em\nzVr5lp778TeUddWgnvyIiHGtra169dVXtGPHDklSSckSnTjxmTwez0X/M4jIODxHug6SlpYW5eXl\nXbDO5XJp6tSpevrpzh+80tnzhnXsF5FVGcNGfnmQnn/41tDr6Uvf0O03De+w3f8/fGVZlvZ80qQT\nJ1v1rYc3S5LOtgfU1h7QjQ+8ordWTlX1kWZlj0gJBdGYzGSNuTpZ7+6rJ0gQlj17PrrgSy4Wj8/H\nAqf/vXYZJElJSaqoqNC3vvWt0GMcLcvSpk2bOjw7+G95PJ6Lj/fCM1T3H23W8Cu+OOtq/R8PqqGl\nVd++abh213ymgf0TNCx1oE6eadejz3+or41M0cD+Cbp5TJr+a8VfA3zz+0dV8acjemr2eLldLl2T\nmaxnX6tS9ZFmZV01SFVHmrTzLydUMPEfo/hJEU1ut1vx8fFyu91yu91KTEzU+fPnFQgElJCQEPr3\nm5CQoMTERLW1fTEf9+tf/1q/+93LWr36Se3bt08LFy7S22+/TTfSzRyeI10HyeOPP67i4mKVlJQo\nNTVVknT8+HGNHDlSjz/+eI8UGOs2vntYL/93jc4HLI396hCtm3eLEvq4VdtwWk+8/JGa/Gf1pX59\n9M//dIWeuP+fJUkJfdwaktQv9DMG9uujeHdcaN3XRqao6PZr9EDZO/rMf1begYm6d8oo3XRNWlQ+\nI6Lvpz9dqOLi4tDrGTPu0JIlS1RSskTV1ftD85lbtlRKkjIzh+vIkSN68803tXDhT7VpU4X69++v\nbdve0YwZ34vGR3C0YExOoV+6S3pme1NTk3w+nyQpLS1NXq83snfjme0wgGe2w4TufGZ7/ervh7X9\nFUW/6bb37gmXdJqP1+uNPDwAoJfr1Ye2AAD2WQ4/tEWQAIBh3EYeAGBLrz79FwBgn8NzhCABANPo\nSAAAtnTficSXJ4IEAAyjIwEA2OLwHCFIAMA0OhIAgC0BggQAYIfDc4QgAQDTOLQFALCFW6QAAGzh\npo0AAFscfmSLIAEA05gjAQDY4vAcIUgAwLSgw5OEIAEAw5wdI5Ir2gUAgNMFg1ZYSzja2tpUXFys\nyZMna8qUKVq0aJEk6dChQ8rPz1dOTo7y8/N1+PDh0D6RjnWGIAEAw4KWFdYSjuXLlysxMVGVlZXa\ntGmTZs+eLUkqLi5WQUGBKisrVVBQoMWLF4f2iXSsMwQJABhmhblcqs8//1yvvvqqZs+erbi4OEnS\n4MGD1djYqKqqKuXm5kqScnNzVVVVpaampojHusIcCQAYFu7pv36/X36/v8N6j8cjj8cTel1bW6uk\npCSVlZVp+/btGjBggGbPnq2+ffsqNTVVbrdbkuR2u5WSkiKfzyfLsiIa83q9ndZLkACAYeGetFVe\nXq6ysrIO6wsLC1VUVBR6HQgEVFtbq1GjRmn+/PnavXu37rvvPq1atcpuyWEhSADAsHDnPWbOnKlp\n06Z1WP+33YgkpaWlKT4+PnQo6tprr9WgQYPUt29fHT9+XIFAQG63W4FAQA0NDUpLS5NlWRGNdYU5\nEgAwzLLCWzwejzIyMjosfx8kXq9X48aN07Zt2yR9ccZVY2Ojhg0bpqysLFVUVEiSKioqlJWVJa/X\nq+Tk5IjGuhJn9eS1++890mNvhd7DfWNJtEuAAwUCwW77WW8vnBrW9uMf3XjJ29bW1urhhx9WS0uL\n4uPjNWfOHE2YMEE1NTVasGCB/H6/PB6PSktLlZmZKUkRj3WGIEHMI0hgQncGyVs/DS9Ibn7s0oPk\ncsAcCQAYxi1SAAC2ECQAAFscniMECQCYxvNIAAC28Mx2AIAtdCQAAFucHSMECQAYR0cCALCFORIA\ngC10JAAAWxyeIwQJAJgWcHiSECQAYBiHtgAAtjg8RwgSADDNcviVJAQJABjG6b8AAFuYIwEA2OLw\nHCFIAMA0OhIAgC3OjhGCBACM41G7AABbHJ4jBAkAmEZHAgCwhSDpRgMmPNaTb4deIuj0q70Q8xye\nI3QkAGAap/8CAGxxeI4QJABgWtDhV5IQJABgGB0JAMAW5kgAALY4PEcIEgAwjTkSAIAtTr/WiSAB\nAMM4tAUAsIXJdgCALcFoF2AYQQIAhtGRAABscXiOECQAYBodCQDAFoef/StXtAsAAKezwvwTibKy\nMo0YMUIHDhyQJO3atUt5eXnKycnRnXfeqcbGxtC2kY51hiABAMMsK7wlXPv27dOuXbuUnp4uSQoG\ng5o7d64WL16syspKZWdna8WKFbbGukKQAIBhgaAV1hKO9vZ2lZSU6JFHHgmt27t3rxITE5WdnS1J\nmj59urZs2WJrrCvMkQCAYeEervL7/fL7/R3WezweeTyeC9atWrVKeXl5ysjICK3z+XwaOnRo6LXX\n61UwGFRLS0vEY0lJSZ3WS5AAgGHhHq4qLy9XWVlZh/WFhYUqKioKvf7zn/+svXv36qGHHrJboi0E\nCQAYFu7pvzNnztS0adM6rP/7buSDDz5QTU2NJk2aJEmqr6/XD3/4Q91xxx2qq6sLbdfU1CSXy6Wk\npCSlpaVFNNYVggQADAv39N+LHcK6mHvuuUf33HNP6PXEiRO1du1afeUrX9GGDRu0Y8cOZWdn68UX\nX9Rtt90mSRo9erTOnj0b9lhXCBIAMKynL0h0uVxatmyZiouL1dbWpvT0dC1fvtzWWFfirB78hAMS\n+/TUW6EXOdN+PtolwIG686uxeFp2WNsveWVHt713T6AjAQDDuEUKAMAWh+cIQQIApgUdniQECQAY\nRpAAAGxxeI4QJABgGh0JAMAWh+cIQQIApkX6jJFYQZAAgGF0JAAAW5gjAQDY4vAcIUgAwDTmSAAA\nttCRAABsYY4EAGBLMNwnW8UYggQADHN2jBAkAGAczyMBANji8CNbBAkAmEZHAgCwxeE5QpAAgGlc\nkAgAsIU5EgCALcyRAABscXiOECQAYFrA4UlCkACAYRzaAgDY4vAcIUgAwDQ6EgCALcFoF2CYK9oF\n9Da/XFeumsNH5TvRqF1792nm/7lTkjRyZJbefvc9Hatv0LH6BlW8vkUjR2ZdsO/Sx36mo3X1OlpX\nr6WP/Swa5SOG5efnq6qqSqdPn9bHH3+sm266Kdol9RqWZYW1xBo6kh62Ylmp7r/3brW3t+urI0Zo\ny9Y/aPeuXTr0SY2+9918HT1yRC6XS/feP0vlz7+gcdk3SJLuvOtu5ebl6ev/Y6wsy9Kmza/r8OHD\n+uVzz0b5EyEW3HrrrSotLVV+fr7ef/99paWlRbukXiUGsyEsdCQ9rLq6Su3t7ZL++r+UzMxMnTx5\nUkePHJEkxcXFKRAIKPPqq0P7fW/GHXpy5UrVffqpfHV1enLlSs244/tR+QyIPUuWLFFJSYm2b98u\ny7JUV1enurq6aJfVazi9IyFIouDnT67WieaT2rVnn+rr61W55fXQ2KfHT6jJf1r//vOVWl76eGh9\n1qhR2vPRR6HXez76SFmjRvVo3YhNLpdL2dnZGjJkiA4ePKja2lqtXr1affv2jXZpvYYV5hJrIg6S\nKVOmdDrm9/t17NixDgu+8OADRUpNHqRbb/mGNr76itra2kJj6alDlDYkWT+aM1u7d+8Krf/Sl74k\nv/9k6LXff1IDBw7s0boRm1JTU5WQkKDvfOc7Gj9+vK677jpdf/31WrhwYbRL6zWClhXWEmu6nCP5\n+OOPOx1rbm7udKy8vFxlZWWRV9ULBINB/endbZpeUKC7771PT6/569/XmTNn9Itnn9GRT30ae+01\nOnHihE6fPq2BAz2hbQYO9OjUqVPRKB0xprW1VZK0evVq1dfXS5KeeOIJLVy4kDDpIb36me25ublK\nT0+/6DG7lpaWTvebOXOmpk2b1mH9iKuHR1Cis8W745WZmdlhvcvlUv/+/TV0aLpOnDih6qoqXTNm\njHbu+ECSNGbMGFVXVfV0uYhBLS0tqq2tveDfcSweh49lTv/r7jJI0tPTtX79eqWmpnYYmzBhQqf7\neTweeTyeTsd7qyFDhmjCN27R65tfU2trqyZOmqT/nZ+vH3x/hiZOmqTGzxq1Z89HGjBggBYvKVFL\nc7P276+WJK1/4XkVzZ6tyi2vy7IsFc2Zo7VPPRXlT4RYsW7dOhUVFWnLli06d+6cHnzwQVVUVES7\nrF4jGJMzH5euyyCZPHmyPv3004sGyTe/+U1jRTmVZVm66557tapsjVwul2qPHtW8h36szRUVmvbt\nf9WKn69UenqGWltbtXPHB7p9Sm5o/uSXzz2r4cOH6/2df5Ykla/7Faf+4pItXbpUgwcP1oEDB3T2\n7Flt2LBBjz32WLTL6jWc3pHEWT3Y4w5I7NNTb4Ve5Ez7+WiXAAfqzq/Gf7n2y2Ftv3n30W57757A\nBYkAYJjTOxKuIwEAw4KywlouVXNzs+6++27l5ORoypQpKiwsVFNTkyRp165dysvLU05Oju688041\nNjaG9ot0rDMECQAYZlnhLZcqLi5Od911lyorK7Vp0yZdeeWVWrFihYLBoObOnavFixersrJS2dnZ\nWrFihSRFPNYVggQADDN1i5SkpCSNGzcu9Pq6665TXV2d9u7dq8TERGVnZ0uSpk+fri1btkhSxGNd\nYY4EAAwLd47E7/fL7/d3WN/VpRXBYFC//e1vNXHiRPl8Pg0dOjQ05vV6FQwG1dLSEvFYUlJSp/US\nJABgWLi3Pens7iCFhYUqKiq66D5Lly5V//79NWPGDL3xxhsR1RkpggQADAs3SDq7O0hn3UhpaamO\nHDmitWvXyuVyKS0t7YK7Ozc1NcnlcikpKSnisa4wRwIAhoU72e7xeJSRkdFhuViQPPHEE9q7d6/W\nrFmjhIQESdLo0aN19uxZ7dixQ5L04osv6rbbbrM11hUuSETM44JEmNCdX403jwjvQWJv/cV3Sdsd\nPHhQubm5GjZsWOixABkZGVqzZo0+/PBDFRcXq62tTenp6Vq+fLkGDx4sSRGPdYYgQcwjSGBCd341\n3vTV8ILknQOXFiSXC+ZIAMAwqzfftBEAYJ/Tb5FCkACAYU5//gtBAgCGOfwBiQQJAJjGHAkAwBaH\nH9kiSADAtIDDj20RJABgGJPtAABbnB0jBAkAGEdHAgCwxeFTJAQJAJhGRwIAsMXZMUKQAIBxdCQA\nAFscniMECQCYFu6jdmMNQQIAhhEkAABbHJ4jBAkAmEZHAgCwxeE5QpAAgGk8jwQAYAsdCQDAFuZI\nAAC2ODxHCBIAMI1bpAAAbHF2jBAkAGAcz2wHANjCoS0AgC0OzxGCBABM44JEAIAtDp8iIUgAwDTm\nSAAAtjg8RwgSADCNORIAgC3MkQAAbGGOBABgi8NzhCABANMCDk8SggQADOPQFgDAFofnCEECAKbR\nkQAAbAlGuwDDCBIAMMzpHUmc5fRPGIP8fr/Ky8s1c+ZMeTyeaJcDh+D3Cqa4ol0AOvL7/SorK5Pf\n7492KXAQfq9gCkECALCFIAEA2EKQAABsIUgAALYQJJchj8ejwsJCzqxBt+L3CqZw+i8AwBY6EgCA\nLQQJAMAWguQyc+jQIeXn5ysnJ0f5+fk6fPhwtEuCA5SWlmrixIkaMWKEDhw4EO1y4DAEyWWmuLhY\nBQUFqqysVEFBgRYvXhztkuAAkyZN0gsvvKD09PRolwIHIkguI42NjaqqqlJubq4kKTc3V1VVVWpq\naopyZYh12dnZSktLi3YZcCiC5DLi8/mUmpoqt9stSXK73UpJSZHP54tyZQDQOYIEAGALQXIZSUtL\n0/HjxxUIBCRJgUBADQ0NHJIAcFkjSC4jycnJysrKUkVFhSSpoqJCWVlZ8nq9Ua4MADrHle2XmZqa\nGi1YsEB+v18ej0elpaXKzMyMdlmIcY8++qi2bt2qzz77TIMGDVJSUpJee+21aJcFhyBIAAC2cGgL\nAGALQQIAsIUgAQDYQpAAAGwhSAAAthAkAABbCBIAgC0ECQDAlv8HSG9KCRc5YP0AAAAASUVORK5C\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0178571</td>\n",
              "      <td>0.987997</td>\n",
              "      <td>0.0118851</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity   Optimize\n",
              "XGBClassifier_optimize            0   0.0178571         0.987997  0.0118851"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 122 \n",
            "\n",
            "First 23 matches\n",
            "23 [  6  22  42  44  52 152 163 193 197 201 219 226 253 259 279 283 291 298\n",
            " 322 352 380 392 498]\n",
            "\n",
            "[0.50080395 0.50334287 0.5052871  0.5025526  0.50145906 0.50012857\n",
            " 0.5027822  0.5054151  0.5028476  0.5026786  0.5015116  0.50095034\n",
            " 0.50019866 0.50203264 0.5010221  0.5063149  0.5008269  0.5005599\n",
            " 0.5006754  0.50294393 0.5028974  0.5049916  0.5015267 ]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[1597  594 2958   42 5301 1531 4060 2479 5311 4609 2107 1949 3105 1047\n",
            "  392 1271 1207 1553  283 1702 3576  193 7181]\n",
            "\n",
            "\n",
            "[0.50449854 0.5045001  0.5048205  0.5052871  0.5084969  0.5076182\n",
            " 0.505373   0.5093117  0.51060724 0.5060254  0.50597495 0.51323\n",
            " 0.5053036  0.5102036  0.5049916  0.5059752  0.5057163  0.5076201\n",
            " 0.5063149  0.5054221  0.5052292  0.5054151  0.5069316 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "43 [ 22  42  44  52 152 163 193 197 201 219 226 253 259 279 283 291 298 322\n",
            " 352 380 392 498 507 540 546 559 567 581 594 604 628 722 731 732 789 838\n",
            " 851 886 895 903 909 923 954]\n",
            "\n",
            "[0.50334287 0.5052871  0.5025526  0.50145906 0.50012857 0.5027822\n",
            " 0.5054151  0.5028476  0.5026786  0.5015116  0.50095034 0.50019866\n",
            " 0.50203264 0.5010221  0.5063149  0.5008269  0.5005599  0.5006754\n",
            " 0.50294393 0.5028974  0.5049916  0.5015267  0.5013166  0.5020532\n",
            " 0.50370383 0.50309426 0.5022585  0.50337917 0.5045001  0.50328803\n",
            " 0.50089115 0.5034196  0.5002502  0.50171685 0.50292975 0.5009198\n",
            " 0.5014762  0.50134075 0.5023796  0.5017521  0.5037188  0.5043214\n",
            " 0.50034714]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.5023722052574158\n",
            "\n",
            "51 [  22   42   44  163  193  197  201  283  352  380  392  546  559  581\n",
            "  594  604  722  789  895  909  923 1047 1115 1207 1217 1271 1531 1553\n",
            " 1597 1702 1949 2107 2479 2936 2958 3034 3105 3576 3968 4060 4479 4609\n",
            " 4663 5261 5301 5311 5442 5615 7181 7303 8051]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.5026209950447083\n",
            "\n",
            "46 [  22   42  163  193  197  201  283  352  380  392  546  559  581  594\n",
            "  604  722  789  909  923 1047 1115 1207 1217 1271 1531 1553 1597 1702\n",
            " 1949 2107 2479 2936 2958 3034 3105 3576 3968 4060 4479 4609 4663 5261\n",
            " 5301 5311 5442 7181]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "122 [   6   22   42   44   52  152  163  193  197  201  219  226  253  259\n",
            "  279  283  291  298  322  352  380  392  498  507  540  546  559  567\n",
            "  581  594  604  628  722  731  732  789  838  851  886  895  903  909\n",
            "  923  954 1043 1047 1093 1115 1198 1207 1217 1271 1300 1410 1474 1478\n",
            " 1531 1553 1554 1555 1597 1651 1702 1710 1723 1914 1949 1950 1952 2107\n",
            " 2117 2122 2278 2326 2342 2479 2571 2603 2692 2749 2786 2872 2936 2958\n",
            " 3003 3034 3090 3091 3105 3113 3211 3319 3387 3576 3664 3968 4060 4166\n",
            " 4465 4479 4609 4663 5067 5261 5301 5311 5442 5501 5506 5615 6941 6962\n",
            " 7081 7106 7181 7249 7294 7303 7320 7733 8051 8556]\n",
            "\n",
            "122 [0.50080395 0.50334287 0.5052871  0.5025526  0.50145906 0.50012857\n",
            " 0.5027822  0.5054151  0.5028476  0.5026786  0.5015116  0.50095034\n",
            " 0.50019866 0.50203264 0.5010221  0.5063149  0.5008269  0.5005599\n",
            " 0.5006754  0.50294393 0.5028974  0.5049916  0.5015267  0.5013166\n",
            " 0.5020532  0.50370383 0.50309426 0.5022585  0.50337917 0.5045001\n",
            " 0.50328803 0.50089115 0.5034196  0.5002502  0.50171685 0.50292975\n",
            " 0.5009198  0.5014762  0.50134075 0.5023796  0.5017521  0.5037188\n",
            " 0.5043214  0.50034714 0.5005333  0.5102036  0.50046784 0.5040719\n",
            " 0.5020989  0.5057163  0.50438243 0.5059752  0.5012187  0.5012836\n",
            " 0.500315   0.5009685  0.5076182  0.5076201  0.502139   0.50100327\n",
            " 0.50449854 0.50159764 0.5054221  0.50016147 0.5002294  0.5021322\n",
            " 0.51323    0.5021135  0.50099105 0.50597495 0.50086206 0.5016362\n",
            " 0.5005217  0.50098836 0.5017434  0.5093117  0.50067973 0.5017201\n",
            " 0.5001491  0.5002624  0.5002722  0.5015337  0.5035061  0.5048205\n",
            " 0.50027496 0.50405777 0.50054413 0.5023407  0.5053036  0.50125796\n",
            " 0.5001046  0.50137544 0.5014853  0.5052292  0.50007194 0.50298065\n",
            " 0.505373   0.50038207 0.5020544  0.50398517 0.5060254  0.50401807\n",
            " 0.500279   0.5027589  0.5084969  0.51060724 0.50369537 0.50126696\n",
            " 0.5013827  0.5024305  0.50138736 0.50011235 0.5012832  0.5013445\n",
            " 0.5069316  0.50047696 0.50177133 0.502416   0.50012463 0.50062895\n",
            " 0.50254613 0.5001779 ]\n",
            "\n",
            "Matched draws\n",
            "Count: 6, Index: (array([   6,   22,   44,  628, 4663, 5501]),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "104635  495719 2019-06-08  ConsolationNo5        6\n",
            "104789  496419 2019-06-22      2ndPrizeNo       44\n",
            "104802  496419 2019-06-22     SpecialNo10       22\n",
            "104856  496619 2019-06-26      SpecialNo9        6\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "        DrawNo   DrawDate    PrizeType  LuckyNo\n",
            "104789  496419 2019-06-22   2ndPrizeNo       44\n",
            "104802  496419 2019-06-22  SpecialNo10       22\n",
            "104900  496819 2019-06-30   SpecialNo7      628\n",
            "\n",
            "\n",
            "All matched\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "104635  495719 2019-06-08  ConsolationNo5        6\n",
            "104789  496419 2019-06-22      2ndPrizeNo       44\n",
            "104802  496419 2019-06-22     SpecialNo10       22\n",
            "104807  496419 2019-06-22      SpecialNo6     4663\n",
            "104856  496619 2019-06-26      SpecialNo9        6\n",
            "104899  496819 2019-06-30      SpecialNo6     5501\n",
            "104900  496819 2019-06-30      SpecialNo7      628\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qktZbi7OGqP3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2de992c9-8f6e-4b6b-e7c4-dad3a7039403"
      },
      "source": [
        "start_mt = pd.datetime(2019,7,1)\n",
        "how_many_mt = 6 \n",
        "for i in range(how_many_mt):\n",
        "  month_to_predict = start_mt + relativedelta(months=i)\n",
        "  print(f\"\\n{month_to_predict}\\n-------------------\\n\")\n",
        "\n",
        "  weight=1.0\n",
        "  decrement = 0.000\n",
        "  to_stop=False\n",
        "\n",
        "  gen_train_test_set(month_to_predict, feature_matrix_selection, file_prefix='test')\n",
        "  while not to_stop:\n",
        "    to_stop = model(month_to_predict, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "    decrement = decrement + 0.001\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "2019-07-01 00:00:00\n",
            "-------------------\n",
            "\n",
            "Training on 899893 observations.\n",
            "Testing on 10000 observations.\n",
            "\n",
            "Data shape before balancing: (899893, 62)\n",
            "\n",
            "Counts of strikes vs non-strikes in previous data:\n",
            "0    869724\n",
            "1     30169\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "NO balancing\n",
            "Data shape after balancing: (899893, 62)\n",
            "\n",
            "Counts of strikes VS non-strikes in new data:\n",
            "0    869724\n",
            "1     30169\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Filtering features\n",
            "\n",
            "Data shape before feature selection: (899893, 62)\n",
            "\n",
            "Counts of strikes vs non-strikes before feature selection:\n",
            "0    869724\n",
            "1     30169\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "\n",
            "Data shape after feature selection: (899893, 32)\n",
            "\n",
            "Counts of strikes vs non-strikes in new data:\n",
            "0    869724\n",
            "1     30169\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "-----------2019-07-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(899893, 31) (899893,) (10000, 31) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 869724, 1: 30169})\n",
            "\n",
            "scale_pos_weight - 28.828400013258644\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 300, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.828400013258644, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.828400013258644]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.445, F-Score=0.06123\n",
            "\n",
            "Recall: 0.09872611464968153\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.97      0.90      0.10      0.93      0.30      0.10      9686\n",
            "          1       0.03      0.10      0.90      0.05      0.30      0.08       314\n",
            "\n",
            "avg / total       0.94      0.87      0.12      0.90      0.30      0.10     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFCQiIhIRBYmIiEREQSIiIhH5X/sxCea9LfbFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0987261</td>\n",
              "      <td>0.898204</td>\n",
              "      <td>0.0586216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity   Optimize\n",
              "XGBClassifier_optimize            0   0.0987261         0.898204  0.0586216"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 1017 \n",
            "\n",
            "First 23 matches\n",
            "23 [ 0  5  9 13 17 18 28 29 33 35 38 42 45 50 52 54 62 64 66 67 72 73 77]\n",
            "\n",
            "[0.50629705 0.5043603  0.50260824 0.5016583  0.5032644  0.5081183\n",
            " 0.5073478  0.5027415  0.5072947  0.5144288  0.5043419  0.50598544\n",
            " 0.5051688  0.5002384  0.5079608  0.5018621  0.50182307 0.50171655\n",
            " 0.50216424 0.50152576 0.5051814  0.5003338  0.50219536]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[ 594  193  817 5311  823 3591   35 5301  851  895  903 1271  905  581\n",
            "  661  950 1949 2479  592  298  479 6114  210]\n",
            "\n",
            "\n",
            "[0.51199836 0.5157651  0.5141168  0.5156203  0.51427156 0.51507765\n",
            " 0.5144288  0.51325536 0.5125559  0.51364636 0.5189871  0.5166403\n",
            " 0.5169881  0.5121261  0.5148447  0.51356447 0.5131091  0.5140965\n",
            " 0.5158624  0.51328874 0.5136101  0.51473165 0.5137958 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "291 [ 13  17  18  28  29  33  35  38  42  45  50  52  54  62  64  66  67  72\n",
            "  73  77  83  90  92 102 104 107 108 111 113 115 116 121 130 131 139 146\n",
            " 148 151 152 157 163 171 173 175 186 191 192 193 195 197 198 201 203 205\n",
            " 210 215 216 219 226 230 239 240 243 244 248 249 253 259 260 261 266 271\n",
            " 272 274 275 277 278 279 281 282 283 288 291 294 298 305 312 314 316 317\n",
            " 320 322 326 328 333 336 344 352 353 356 362 363 365 366 382 384 385 390\n",
            " 391 392 399 414 421 422 425 430 431 436 437 439 446 450 453 458 463 468\n",
            " 476 479 488 489 490 495 496 498 500 502 505 507 510 519 527 531 535 536\n",
            " 537 538 540 544 546 550 553 556 557 559 560 567 568 569 575 580 581 582\n",
            " 586 587 590 591 592 594 595 599 603 604 607 608 613 616 619 621 622 623\n",
            " 624 626 628 630 634 636 637 639 641 644 649 654 657 661 666 667 669 671\n",
            " 674 682 691 692 693 701 705 706 708 709 715 716 720 721 722 731 732 736\n",
            " 743 752 757 759 763 764 767 769 776 777 782 787 789 791 792 793 799 800\n",
            " 802 803 804 806 809 810 811 813 816 817 818 823 827 838 841 843 848 851\n",
            " 854 855 858 859 871 874 877 878 879 886 887 892 893 895 897 903 905 910\n",
            " 921 922 923 925 935 941 944 947 950 952 954 957 961 963 971 974 979 984\n",
            " 993 996 999]\n",
            "\n",
            "[0.5016583  0.5032644  0.5081183  0.5073478  0.5027415  0.5072947\n",
            " 0.5144288  0.5043419  0.50598544 0.5051688  0.5002384  0.5079608\n",
            " 0.5018621  0.50182307 0.50171655 0.50216424 0.50152576 0.5051814\n",
            " 0.5003338  0.50219536 0.501566   0.504204   0.50560343 0.5085875\n",
            " 0.50502163 0.5019132  0.5027433  0.5057595  0.5079452  0.5047901\n",
            " 0.50116736 0.50397795 0.5008532  0.50758815 0.50076586 0.50282824\n",
            " 0.50015384 0.50764054 0.50299263 0.50235236 0.5028694  0.5019658\n",
            " 0.50123614 0.50290364 0.5010757  0.50135976 0.5044784  0.5157651\n",
            " 0.5006484  0.51156795 0.50347066 0.508443   0.50263995 0.5000427\n",
            " 0.5137958  0.50441587 0.51096696 0.5087924  0.503287   0.50543934\n",
            " 0.5013382  0.50110716 0.5075766  0.5049225  0.5010169  0.50439185\n",
            " 0.5062468  0.50648886 0.50023806 0.5020318  0.5017408  0.50358987\n",
            " 0.5108637  0.5001325  0.50074315 0.50266874 0.50606185 0.50498635\n",
            " 0.50323355 0.5058031  0.5112121  0.50335014 0.5070981  0.50433004\n",
            " 0.51328874 0.50443333 0.50040126 0.5069244  0.5053895  0.5036785\n",
            " 0.5012277  0.50514835 0.50281245 0.5009226  0.50093174 0.5015104\n",
            " 0.50588155 0.50563645 0.5021444  0.50004476 0.50446236 0.5044751\n",
            " 0.50365067 0.5008032  0.5001772  0.5067231  0.5029339  0.5001008\n",
            " 0.5045953  0.5113222  0.501099   0.50575304 0.51077807 0.5071202\n",
            " 0.5019255  0.51024765 0.5027095  0.5027446  0.50268143 0.5007768\n",
            " 0.50505245 0.5009673  0.5012055  0.5006244  0.50320196 0.5052089\n",
            " 0.5032187  0.5136101  0.50372595 0.5032487  0.50059265 0.5053765\n",
            " 0.5078516  0.5063673  0.50895137 0.50509495 0.5005466  0.5066976\n",
            " 0.5055716  0.50601137 0.5019748  0.5025525  0.50457096 0.50462717\n",
            " 0.5104539  0.5039558  0.5068725  0.5045577  0.5077504  0.5037696\n",
            " 0.50663924 0.5045938  0.50720793 0.50162137 0.5029691  0.51128596\n",
            " 0.5048339  0.5023113  0.5118391  0.5044018  0.5121261  0.50080436\n",
            " 0.5033392  0.50375736 0.50687915 0.50898623 0.5158624  0.51199836\n",
            " 0.5095318  0.5001287  0.506993   0.50689584 0.504291   0.50002825\n",
            " 0.5015629  0.5021331  0.5075749  0.5015901  0.5022523  0.5060235\n",
            " 0.5008155  0.50101143 0.5047359  0.5061834  0.50178176 0.5022867\n",
            " 0.503116   0.5038149  0.50976825 0.5024678  0.5041073  0.5030727\n",
            " 0.50478643 0.5148447  0.5006706  0.50006783 0.5043828  0.5009354\n",
            " 0.5051451  0.50152695 0.50240684 0.50043523 0.50420207 0.5014977\n",
            " 0.50169814 0.50609016 0.5032953  0.50371546 0.502304   0.50741553\n",
            " 0.50589925 0.5007978  0.50800824 0.50363743 0.5045772  0.50390464\n",
            " 0.5107372  0.50412    0.5076187  0.50087386 0.501974   0.50545394\n",
            " 0.5047687  0.50344217 0.5027711  0.5016146  0.5014778  0.50313973\n",
            " 0.50081235 0.50502926 0.50239986 0.50898904 0.50724417 0.50045794\n",
            " 0.507245   0.50915796 0.5017718  0.50293386 0.50931853 0.50530905\n",
            " 0.5057973  0.50446206 0.50509256 0.5141168  0.5047829  0.51427156\n",
            " 0.50420743 0.50296074 0.50463504 0.50433785 0.50189984 0.5125559\n",
            " 0.5026321  0.5044451  0.50164145 0.5034034  0.502359   0.50521594\n",
            " 0.5055892  0.5006472  0.5013792  0.5035123  0.5011377  0.50236624\n",
            " 0.5041841  0.51364636 0.5079899  0.5189871  0.5169881  0.50409585\n",
            " 0.5046471  0.50545985 0.50945157 0.5106312  0.5037719  0.5003688\n",
            " 0.50189745 0.5068873  0.51356447 0.50820386 0.50507104 0.5026287\n",
            " 0.50044054 0.507846   0.50226337 0.5085113  0.5003926  0.5043138\n",
            " 0.50107855 0.50532585 0.50000066]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.5046612620353699\n",
            "\n",
            "272 [   0   18   28   33   35   42   45   52   72   92  102  104  111  113\n",
            "  115  131  151  193  197  201  210  216  219  230  243  244  253  259\n",
            "  272  278  279  282  283  291  298  314  316  322  344  352  384  392\n",
            "  414  421  422  430  446  468  479  495  496  498  500  502  507  510\n",
            "  519  537  540  546  553  557  567  568  575  581  590  591  592  594\n",
            "  595  603  604  619  623  628  630  641  657  661  674  706  716  720\n",
            "  722  743  757  764  767  791  793  799  802  803  809  810  811  816\n",
            "  817  818  823  851  874  877  895  897  903  905  922  923  925  947\n",
            "  950  952  954  963  974  996 1008 1009 1017 1018 1029 1031 1050 1052\n",
            " 1061 1064 1085 1112 1115 1124 1126 1156 1164 1198 1201 1207 1217 1231\n",
            " 1232 1256 1271 1289 1292 1296 1300 1301 1308 1320 1431 1445 1472 1474\n",
            " 1489 1504 1508 1519 1550 1554 1555 1571 1597 1651 1671 1684 1686 1702\n",
            " 1736 1747 1756 1773 1807 1813 1836 1911 1914 1933 1942 1949 1950 1952\n",
            " 1979 1980 2107 2144 2185 2188 2205 2269 2276 2278 2320 2423 2479 2485\n",
            " 2489 2491 2492 2550 2603 2633 2672 2936 2958 3010 3029 3034 3090 3113\n",
            " 3171 3211 3266 3289 3387 3460 3470 3482 3497 3526 3591 3664 3675 3811\n",
            " 3830 3926 3968 4012 4016 4123 4166 4321 4479 4609 4780 4851 4928 5051\n",
            " 5067 5140 5227 5237 5294 5301 5311 5378 5442 5523 5533 5587 5615 5725\n",
            " 5915 6019 6045 6090 6114 6152 6447 6749 6772 6898 6945 7081 7166 7227\n",
            " 7231 7455 7730 8690 9028 9964]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.503436803817749\n",
            "\n",
            "403 [   0    5   18   28   33   35   38   42   45   52   72   90   92  102\n",
            "  104  111  113  115  121  131  151  192  193  197  198  201  210  215\n",
            "  216  219  230  243  244  249  253  259  271  272  278  279  282  283\n",
            "  291  294  298  305  314  316  317  322  344  352  362  363  365  384\n",
            "  391  392  414  421  422  430  446  468  479  488  495  496  498  500\n",
            "  502  507  510  519  535  536  537  538  540  544  546  550  553  556\n",
            "  557  567  568  575  580  581  587  590  591  592  594  595  603  604\n",
            "  607  619  623  628  630  639  641  649  657  661  669  674  693  706\n",
            "  709  716  720  722  731  732  736  743  752  757  764  767  769  791\n",
            "  793  799  802  803  809  810  811  813  816  817  818  823  827  841\n",
            "  843  851  855  874  877  886  893  895  897  903  905  910  921  922\n",
            "  923  925  935  947  950  952  954  963  974  984  996 1008 1009 1017\n",
            " 1018 1028 1029 1031 1039 1043 1047 1050 1052 1061 1064 1071 1085 1112\n",
            " 1115 1117 1123 1124 1126 1156 1163 1164 1174 1198 1201 1207 1217 1230\n",
            " 1231 1232 1249 1256 1260 1268 1271 1289 1292 1296 1299 1300 1301 1304\n",
            " 1308 1320 1347 1353 1360 1366 1370 1374 1379 1431 1445 1472 1474 1475\n",
            " 1489 1504 1508 1516 1519 1550 1554 1555 1571 1597 1650 1651 1667 1671\n",
            " 1684 1686 1689 1701 1702 1736 1747 1756 1773 1807 1810 1813 1828 1836\n",
            " 1911 1914 1933 1942 1949 1950 1952 1957 1979 1980 2019 2061 2064 2093\n",
            " 2107 2144 2157 2185 2188 2205 2213 2269 2276 2278 2302 2320 2423 2441\n",
            " 2466 2479 2485 2489 2491 2492 2550 2556 2560 2603 2633 2672 2749 2795\n",
            " 2872 2936 2958 2980 3010 3029 3034 3039 3081 3090 3098 3113 3171 3211\n",
            " 3266 3289 3334 3387 3400 3460 3470 3482 3497 3526 3591 3624 3664 3675\n",
            " 3732 3811 3830 3926 3935 3968 4001 4012 4016 4123 4166 4321 4479 4609\n",
            " 4725 4780 4836 4851 4928 4947 5024 5051 5067 5087 5140 5227 5237 5294\n",
            " 5301 5311 5355 5378 5426 5442 5487 5521 5523 5533 5587 5615 5725 5726\n",
            " 5728 5820 5852 5874 5915 5921 6019 6027 6045 6090 6114 6152 6156 6269\n",
            " 6281 6447 6510 6521 6710 6749 6772 6779 6898 6945 6962 6997 7081 7166\n",
            " 7225 7227 7231 7294 7320 7455 7730 8690 9028 9571 9964]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "1017 [   0    5    9   13   17   18   28   29   33   35   38   42   45   50\n",
            "   52   54   62   64   66   67   72   73   77   83   90   92  102  104\n",
            "  107  108  111  113  115  116  121  130  131  139  146  148  151  152\n",
            "  157  163  171  173  175  186  191  192  193  195  197  198  201  203\n",
            "  205  210  215  216  219  226  230  239  240  243  244  248  249  253\n",
            "  259  260  261  266  271  272  274  275  277  278  279  281  282  283\n",
            "  288  291  294  298  305  312  314  316  317  320  322  326  328  333\n",
            "  336  344  352  353  356  362  363  365  366  382  384  385  390  391\n",
            "  392  399  414  421  422  425  430  431  436  437  439  446  450  453\n",
            "  458  463  468  476  479  488  489  490  495  496  498  500  502  505\n",
            "  507  510  519  527  531  535  536  537  538  540  544  546  550  553\n",
            "  556  557  559  560  567  568  569  575  580  581  582  586  587  590\n",
            "  591  592  594  595  599  603  604  607  608  613  616  619  621  622\n",
            "  623  624  626  628  630  634  636  637  639  641  644  649  654  657\n",
            "  661  666  667  669  671  674  682  691  692  693  701  705  706  708\n",
            "  709  715  716  720  721  722  731  732  736  743  752  757  759  763\n",
            "  764  767  769  776  777  782  787  789  791  792  793  799  800  802\n",
            "  803  804  806  809  810  811  813  816  817  818  823  827  838  841\n",
            "  843  848  851  854  855  858  859  871  874  877  878  879  886  887\n",
            "  892  893  895  897  903  905  910  921  922  923  925  935  941  944\n",
            "  947  950  952  954  957  961  963  971  974  979  984  993  996  999\n",
            " 1005 1008 1009 1011 1015 1016 1017 1018 1022 1028 1029 1031 1036 1039\n",
            " 1043 1044 1047 1050 1052 1059 1061 1064 1069 1071 1075 1081 1085 1086\n",
            " 1089 1093 1102 1112 1115 1117 1123 1124 1126 1127 1142 1152 1156 1163\n",
            " 1164 1166 1172 1174 1175 1178 1188 1191 1198 1201 1207 1208 1214 1217\n",
            " 1226 1230 1231 1232 1236 1240 1245 1246 1249 1253 1255 1256 1257 1260\n",
            " 1264 1268 1271 1273 1284 1287 1289 1292 1295 1296 1298 1299 1300 1301\n",
            " 1302 1304 1308 1316 1319 1320 1334 1336 1340 1347 1349 1353 1354 1356\n",
            " 1358 1360 1362 1366 1369 1370 1373 1374 1378 1379 1380 1383 1384 1399\n",
            " 1400 1410 1414 1416 1431 1433 1441 1445 1446 1463 1464 1467 1472 1474\n",
            " 1475 1478 1489 1494 1496 1504 1508 1510 1512 1513 1516 1519 1534 1550\n",
            " 1553 1554 1555 1557 1560 1571 1580 1581 1597 1602 1610 1616 1618 1631\n",
            " 1637 1649 1650 1651 1654 1661 1667 1671 1675 1684 1686 1689 1695 1701\n",
            " 1702 1713 1721 1723 1732 1736 1747 1748 1756 1769 1771 1773 1781 1799\n",
            " 1807 1810 1813 1818 1828 1836 1859 1862 1867 1879 1883 1891 1894 1896\n",
            " 1899 1911 1914 1924 1933 1942 1949 1950 1952 1957 1962 1979 1980 1984\n",
            " 2019 2021 2032 2034 2035 2036 2037 2050 2053 2056 2061 2064 2071 2088\n",
            " 2093 2097 2107 2117 2122 2125 2130 2144 2157 2171 2178 2185 2188 2196\n",
            " 2198 2203 2205 2209 2212 2213 2217 2226 2237 2243 2248 2260 2268 2269\n",
            " 2272 2276 2278 2302 2315 2319 2320 2326 2327 2341 2342 2356 2389 2399\n",
            " 2401 2405 2418 2423 2439 2441 2456 2457 2461 2466 2474 2479 2480 2483\n",
            " 2485 2487 2489 2491 2492 2494 2502 2520 2523 2527 2529 2533 2538 2546\n",
            " 2550 2552 2556 2560 2567 2568 2570 2571 2581 2584 2600 2603 2611 2633\n",
            " 2637 2653 2660 2672 2680 2684 2691 2692 2694 2701 2704 2739 2749 2760\n",
            " 2765 2770 2782 2786 2788 2794 2795 2826 2842 2858 2865 2869 2872 2879\n",
            " 2889 2897 2899 2902 2936 2946 2951 2952 2958 2980 2983 2989 3010 3029\n",
            " 3030 3034 3039 3043 3055 3072 3081 3090 3091 3098 3102 3113 3114 3131\n",
            " 3138 3139 3140 3145 3149 3150 3156 3171 3177 3210 3211 3216 3222 3251\n",
            " 3266 3269 3277 3289 3302 3319 3330 3334 3339 3346 3349 3350 3356 3378\n",
            " 3387 3397 3398 3400 3430 3431 3444 3448 3450 3460 3470 3482 3492 3497\n",
            " 3511 3526 3544 3545 3591 3600 3618 3624 3632 3640 3649 3654 3658 3664\n",
            " 3675 3685 3710 3732 3733 3781 3789 3803 3805 3811 3823 3828 3830 3843\n",
            " 3866 3920 3926 3935 3945 3948 3953 3960 3968 3987 3996 3997 4001 4009\n",
            " 4012 4016 4024 4030 4035 4036 4043 4081 4086 4098 4113 4123 4143 4145\n",
            " 4166 4223 4231 4242 4253 4266 4282 4316 4321 4373 4412 4413 4419 4422\n",
            " 4450 4464 4465 4466 4472 4479 4498 4509 4561 4562 4578 4586 4609 4615\n",
            " 4669 4675 4689 4725 4726 4748 4753 4763 4780 4789 4811 4826 4836 4849\n",
            " 4851 4859 4860 4876 4877 4895 4900 4928 4947 4974 4975 4987 5024 5032\n",
            " 5036 5047 5051 5065 5067 5087 5096 5103 5127 5140 5213 5227 5232 5234\n",
            " 5237 5240 5258 5294 5299 5301 5303 5308 5309 5311 5331 5333 5340 5346\n",
            " 5355 5378 5382 5385 5404 5426 5430 5440 5442 5454 5457 5487 5506 5512\n",
            " 5521 5523 5533 5578 5583 5587 5615 5635 5644 5648 5673 5709 5720 5725\n",
            " 5726 5728 5733 5743 5762 5796 5797 5820 5852 5853 5869 5874 5915 5921\n",
            " 5945 5949 5955 5965 5968 6004 6009 6019 6027 6030 6045 6052 6054 6060\n",
            " 6090 6092 6093 6094 6101 6114 6120 6152 6156 6159 6213 6269 6281 6314\n",
            " 6324 6331 6354 6367 6391 6401 6427 6432 6439 6447 6453 6481 6486 6506\n",
            " 6510 6521 6550 6560 6603 6620 6631 6695 6710 6725 6739 6742 6749 6772\n",
            " 6777 6779 6802 6815 6850 6863 6864 6898 6918 6926 6941 6945 6962 6977\n",
            " 6978 6992 6997 7019 7055 7057 7058 7081 7096 7166 7173 7181 7196 7202\n",
            " 7210 7225 7227 7231 7249 7264 7294 7303 7304 7320 7373 7397 7400 7436\n",
            " 7455 7466 7495 7513 7532 7538 7543 7669 7670 7722 7730 7770 7908 8031\n",
            " 8078 8186 8228 8476 8524 8556 8604 8690 8872 8938 9028 9053 9074 9258\n",
            " 9308 9355 9368 9571 9671 9692 9755 9791 9964]\n",
            "\n",
            "1017 [0.50629705 0.5043603  0.50260824 0.5016583  0.5032644  0.5081183\n",
            " 0.5073478  0.5027415  0.5072947  0.5144288  0.5043419  0.50598544\n",
            " 0.5051688  0.5002384  0.5079608  0.5018621  0.50182307 0.50171655\n",
            " 0.50216424 0.50152576 0.5051814  0.5003338  0.50219536 0.501566\n",
            " 0.504204   0.50560343 0.5085875  0.50502163 0.5019132  0.5027433\n",
            " 0.5057595  0.5079452  0.5047901  0.50116736 0.50397795 0.5008532\n",
            " 0.50758815 0.50076586 0.50282824 0.50015384 0.50764054 0.50299263\n",
            " 0.50235236 0.5028694  0.5019658  0.50123614 0.50290364 0.5010757\n",
            " 0.50135976 0.5044784  0.5157651  0.5006484  0.51156795 0.50347066\n",
            " 0.508443   0.50263995 0.5000427  0.5137958  0.50441587 0.51096696\n",
            " 0.5087924  0.503287   0.50543934 0.5013382  0.50110716 0.5075766\n",
            " 0.5049225  0.5010169  0.50439185 0.5062468  0.50648886 0.50023806\n",
            " 0.5020318  0.5017408  0.50358987 0.5108637  0.5001325  0.50074315\n",
            " 0.50266874 0.50606185 0.50498635 0.50323355 0.5058031  0.5112121\n",
            " 0.50335014 0.5070981  0.50433004 0.51328874 0.50443333 0.50040126\n",
            " 0.5069244  0.5053895  0.5036785  0.5012277  0.50514835 0.50281245\n",
            " 0.5009226  0.50093174 0.5015104  0.50588155 0.50563645 0.5021444\n",
            " 0.50004476 0.50446236 0.5044751  0.50365067 0.5008032  0.5001772\n",
            " 0.5067231  0.5029339  0.5001008  0.5045953  0.5113222  0.501099\n",
            " 0.50575304 0.51077807 0.5071202  0.5019255  0.51024765 0.5027095\n",
            " 0.5027446  0.50268143 0.5007768  0.50505245 0.5009673  0.5012055\n",
            " 0.5006244  0.50320196 0.5052089  0.5032187  0.5136101  0.50372595\n",
            " 0.5032487  0.50059265 0.5053765  0.5078516  0.5063673  0.50895137\n",
            " 0.50509495 0.5005466  0.5066976  0.5055716  0.50601137 0.5019748\n",
            " 0.5025525  0.50457096 0.50462717 0.5104539  0.5039558  0.5068725\n",
            " 0.5045577  0.5077504  0.5037696  0.50663924 0.5045938  0.50720793\n",
            " 0.50162137 0.5029691  0.51128596 0.5048339  0.5023113  0.5118391\n",
            " 0.5044018  0.5121261  0.50080436 0.5033392  0.50375736 0.50687915\n",
            " 0.50898623 0.5158624  0.51199836 0.5095318  0.5001287  0.506993\n",
            " 0.50689584 0.504291   0.50002825 0.5015629  0.5021331  0.5075749\n",
            " 0.5015901  0.5022523  0.5060235  0.5008155  0.50101143 0.5047359\n",
            " 0.5061834  0.50178176 0.5022867  0.503116   0.5038149  0.50976825\n",
            " 0.5024678  0.5041073  0.5030727  0.50478643 0.5148447  0.5006706\n",
            " 0.50006783 0.5043828  0.5009354  0.5051451  0.50152695 0.50240684\n",
            " 0.50043523 0.50420207 0.5014977  0.50169814 0.50609016 0.5032953\n",
            " 0.50371546 0.502304   0.50741553 0.50589925 0.5007978  0.50800824\n",
            " 0.50363743 0.5045772  0.50390464 0.5107372  0.50412    0.5076187\n",
            " 0.50087386 0.501974   0.50545394 0.5047687  0.50344217 0.5027711\n",
            " 0.5016146  0.5014778  0.50313973 0.50081235 0.50502926 0.50239986\n",
            " 0.50898904 0.50724417 0.50045794 0.507245   0.50915796 0.5017718\n",
            " 0.50293386 0.50931853 0.50530905 0.5057973  0.50446206 0.50509256\n",
            " 0.5141168  0.5047829  0.51427156 0.50420743 0.50296074 0.50463504\n",
            " 0.50433785 0.50189984 0.5125559  0.5026321  0.5044451  0.50164145\n",
            " 0.5034034  0.502359   0.50521594 0.5055892  0.5006472  0.5013792\n",
            " 0.5035123  0.5011377  0.50236624 0.5041841  0.51364636 0.5079899\n",
            " 0.5189871  0.5169881  0.50409585 0.5046471  0.50545985 0.50945157\n",
            " 0.5106312  0.5037719  0.5003688  0.50189745 0.5068873  0.51356447\n",
            " 0.50820386 0.50507104 0.5026287  0.50044054 0.507846   0.50226337\n",
            " 0.5085113  0.5003926  0.5043138  0.50107855 0.50532585 0.50000066\n",
            " 0.50032187 0.50501066 0.5118607  0.50025827 0.5009217  0.5033088\n",
            " 0.508256   0.5093824  0.5009345  0.50383747 0.5047272  0.5049696\n",
            " 0.5013301  0.5037639  0.50388813 0.5018575  0.5041673  0.5055873\n",
            " 0.5068104  0.50049615 0.50540274 0.508343   0.500112   0.50409013\n",
            " 0.5017778  0.5005134  0.5072934  0.5017676  0.50202125 0.5019679\n",
            " 0.50028133 0.5075267  0.5061893  0.5043391  0.5035864  0.5069815\n",
            " 0.5056885  0.50108767 0.5026292  0.50312126 0.5058982  0.50380474\n",
            " 0.5091093  0.502505   0.50118965 0.5041316  0.5028299  0.5003265\n",
            " 0.50007737 0.5027571  0.5073772  0.5065711  0.50581765 0.5014236\n",
            " 0.5013053  0.50776273 0.5025125  0.50385976 0.5085244  0.5047305\n",
            " 0.50287205 0.50089455 0.50232446 0.5001853  0.5045539  0.5024719\n",
            " 0.50206345 0.50554055 0.50069666 0.50438845 0.50027317 0.50351226\n",
            " 0.5166403  0.5022121  0.5025286  0.5020109  0.50592387 0.50991625\n",
            " 0.50118554 0.50732726 0.5027834  0.5042112  0.5049911  0.5081466\n",
            " 0.5021519  0.50348395 0.50470364 0.5020761  0.50031066 0.5050609\n",
            " 0.5014862  0.50067264 0.50033414 0.5040008  0.5025175  0.50458306\n",
            " 0.50220484 0.50092155 0.50309414 0.50433934 0.50336254 0.50401735\n",
            " 0.5003793  0.50371355 0.5017768  0.50444    0.50324845 0.5042936\n",
            " 0.50031585 0.501468   0.5026114  0.5017161  0.5002274  0.50087494\n",
            " 0.50241894 0.5032741  0.5056894  0.50149536 0.5013481  0.50630695\n",
            " 0.5033727  0.50154054 0.501409   0.5018182  0.50604093 0.5061081\n",
            " 0.5039397  0.50145537 0.50637305 0.50025225 0.5008938  0.5057954\n",
            " 0.5080024  0.5017187  0.50138366 0.5010978  0.50461817 0.506339\n",
            " 0.5003206  0.506077   0.5017401  0.50911283 0.50681806 0.5027695\n",
            " 0.50175834 0.50599235 0.50188804 0.5033645  0.5092238  0.5001925\n",
            " 0.5001984  0.5013     0.50175756 0.50018716 0.5019806  0.5001634\n",
            " 0.5045828  0.50492483 0.5007229  0.502357   0.5044321  0.50824624\n",
            " 0.5004533  0.50608635 0.5088832  0.50415856 0.5022444  0.50437075\n",
            " 0.508345   0.5006821  0.50202465 0.50329345 0.50207627 0.5050238\n",
            " 0.50656784 0.50088006 0.5047502  0.5015026  0.5019896  0.5060001\n",
            " 0.5002507  0.50063705 0.50571257 0.50363874 0.5090928  0.5018163\n",
            " 0.5045295  0.50527483 0.50015455 0.50056237 0.5009824  0.5002851\n",
            " 0.5005855  0.5026713  0.5020136  0.50242525 0.50081235 0.5066766\n",
            " 0.50869465 0.5022078  0.5055516  0.5077373  0.5131091  0.5052832\n",
            " 0.5076618  0.50457054 0.50285715 0.508427   0.5052715  0.50069135\n",
            " 0.5045385  0.5002026  0.50225914 0.50113136 0.5003777  0.5015102\n",
            " 0.5026329  0.50231653 0.5004795  0.50333655 0.5046404  0.5044824\n",
            " 0.5023874  0.50199115 0.504454   0.50032747 0.5082803  0.50067866\n",
            " 0.502651   0.5031582  0.5002713  0.5060785  0.50357014 0.50170547\n",
            " 0.50115186 0.5085909  0.5060516  0.50088674 0.5011464  0.50060713\n",
            " 0.5047588  0.50088626 0.50186783 0.50398463 0.50140524 0.5012615\n",
            " 0.5015948  0.5007375  0.50262713 0.5032123  0.5014976  0.5081251\n",
            " 0.5004735  0.5054501  0.5067067  0.50460625 0.50049067 0.50089276\n",
            " 0.5048646  0.5007248  0.5003694  0.50340164 0.5013301  0.50060487\n",
            " 0.50058234 0.502579   0.5029273  0.5012133  0.5034277  0.5050136\n",
            " 0.503063   0.50425    0.5001503  0.50234085 0.50175244 0.50361747\n",
            " 0.5002003  0.5140965  0.5012017  0.50125617 0.5053492  0.50101817\n",
            " 0.50646764 0.5062962  0.50529164 0.5003827  0.5021424  0.5032716\n",
            " 0.50095344 0.5014906  0.50106657 0.5011184  0.5007439  0.5020144\n",
            " 0.5089581  0.50339717 0.50460136 0.50344676 0.50070214 0.50075066\n",
            " 0.50332683 0.5029225  0.50116754 0.5019759  0.501985   0.5064692\n",
            " 0.5003933  0.505928   0.501256   0.5010114  0.5033651  0.50592506\n",
            " 0.5028063  0.5009249  0.5004342  0.501715   0.5005569  0.50078803\n",
            " 0.5023677  0.5016946  0.5044054  0.50242025 0.5031914  0.50195765\n",
            " 0.5015365  0.50253546 0.501784   0.5021518  0.5037757  0.5024965\n",
            " 0.50151145 0.50177133 0.50252384 0.501132   0.50353014 0.50262845\n",
            " 0.5013435  0.50252414 0.5022326  0.5024332  0.5066735  0.5017159\n",
            " 0.50246567 0.5032706  0.50866544 0.50397074 0.502122   0.5000756\n",
            " 0.5050037  0.50803334 0.50184166 0.5068606  0.50434417 0.50103074\n",
            " 0.50200725 0.5000657  0.50414085 0.50756854 0.50342226 0.5040897\n",
            " 0.5001378  0.505069   0.50036293 0.5018705  0.50322306 0.50303787\n",
            " 0.5028594  0.5023065  0.5013225  0.5000027  0.50053513 0.5046927\n",
            " 0.5024799  0.5002209  0.5051129  0.5008427  0.50179774 0.5027278\n",
            " 0.5063382  0.5001263  0.50210625 0.50575525 0.50115865 0.502619\n",
            " 0.500784   0.50421154 0.5004588  0.5014122  0.5014875  0.5028407\n",
            " 0.5022712  0.50039905 0.5058919  0.5002204  0.5003509  0.504516\n",
            " 0.5021147  0.50173885 0.50244147 0.50179774 0.50012255 0.50479627\n",
            " 0.5051514  0.5055955  0.5010428  0.50508434 0.5002013  0.50768673\n",
            " 0.50178295 0.50119406 0.51507765 0.5029971  0.50011176 0.5034612\n",
            " 0.5026732  0.5002687  0.50080204 0.5013322  0.5009012  0.5072224\n",
            " 0.50637966 0.50215447 0.50032717 0.503473   0.50235575 0.50055903\n",
            " 0.5003939  0.5015332  0.5007536  0.5056421  0.5014608  0.500057\n",
            " 0.5078809  0.50212157 0.50088817 0.50034374 0.5060145  0.5037719\n",
            " 0.50036865 0.50286716 0.5003745  0.5007266  0.5058108  0.50331014\n",
            " 0.50135416 0.50283855 0.50362146 0.50089544 0.5060584  0.5072958\n",
            " 0.5024222  0.50259817 0.5001317  0.50205106 0.5020037  0.500804\n",
            " 0.50103545 0.5012617  0.5032609  0.50477797 0.50162846 0.50068283\n",
            " 0.50469637 0.5006467  0.502802   0.5014751  0.50287014 0.50032425\n",
            " 0.50020236 0.5007772  0.50617296 0.50204366 0.5011085  0.5023713\n",
            " 0.50074774 0.5000623  0.50000423 0.50102705 0.5005143  0.50158834\n",
            " 0.50103337 0.50688493 0.5004769  0.5002925  0.50046694 0.50192153\n",
            " 0.5015378  0.5019358  0.50507027 0.50092745 0.50334406 0.5025938\n",
            " 0.50067    0.5040732  0.5013711  0.50237066 0.5003504  0.50027174\n",
            " 0.5062489  0.5028954  0.5015798  0.5002323  0.5044484  0.5017745\n",
            " 0.50587636 0.5022385  0.50103444 0.5025679  0.5013957  0.50171024\n",
            " 0.50103766 0.50600076 0.50432336 0.5011972  0.50096786 0.50085396\n",
            " 0.5038795  0.5028872  0.5005312  0.5009046  0.50757176 0.5000744\n",
            " 0.50552565 0.5039223  0.50085765 0.5015273  0.501075   0.50938946\n",
            " 0.50170654 0.50565237 0.5027633  0.50326306 0.5087494  0.50203335\n",
            " 0.5001403  0.50557643 0.50054955 0.51325536 0.50133204 0.50011057\n",
            " 0.50241584 0.5156203  0.50317985 0.50077945 0.50012916 0.50169104\n",
            " 0.50456506 0.5115893  0.5029822  0.50163996 0.5021576  0.50376815\n",
            " 0.50061864 0.50142306 0.5062477  0.502748   0.50281596 0.5035664\n",
            " 0.5028217  0.5011166  0.5035221  0.5068093  0.50638014 0.50281256\n",
            " 0.50116485 0.5059237  0.5047047  0.5011969  0.5020647  0.5029644\n",
            " 0.5018131  0.50063133 0.50120604 0.5061189  0.5044149  0.5036052\n",
            " 0.5000178  0.5001051  0.5003053  0.50078726 0.50240815 0.50347716\n",
            " 0.5041401  0.50191236 0.50085294 0.5042965  0.50511765 0.50346553\n",
            " 0.50025403 0.5008496  0.5010549  0.5008301  0.5011538  0.5033014\n",
            " 0.5003232  0.50493836 0.50370604 0.50157636 0.5056515  0.5021446\n",
            " 0.501141   0.5023915  0.5051294  0.5018439  0.5026813  0.5003983\n",
            " 0.5028864  0.51473165 0.5010199  0.5053731  0.50432986 0.50269645\n",
            " 0.5002247  0.50443345 0.50355506 0.5007793  0.50061864 0.5023162\n",
            " 0.50163436 0.5009109  0.50147444 0.5003074  0.5006398  0.50309485\n",
            " 0.50294346 0.50668263 0.5024367  0.5032675  0.5000982  0.50160193\n",
            " 0.50355774 0.50426567 0.50067866 0.5011755  0.50089693 0.50328845\n",
            " 0.50216323 0.50046486 0.50358677 0.50042653 0.50023645 0.50220215\n",
            " 0.5100301  0.50682044 0.50097704 0.5035938  0.5012445  0.50022745\n",
            " 0.50135696 0.5001297  0.5001609  0.50645    0.50139534 0.5026759\n",
            " 0.50312537 0.50467354 0.50451684 0.50120085 0.5009705  0.5011401\n",
            " 0.5042362  0.50026774 0.5002898  0.50090367 0.50284415 0.50659376\n",
            " 0.50036013 0.5089113  0.50140697 0.50047785 0.50164294 0.50220776\n",
            " 0.5030083  0.503582   0.50771445 0.507137   0.5034333  0.50055736\n",
            " 0.504392   0.5023178  0.5000842  0.5035862  0.5015683  0.5021128\n",
            " 0.5000091  0.5025182  0.5072159  0.50037134 0.5015843  0.5011592\n",
            " 0.5022866  0.50118846 0.50301135 0.50020415 0.5003401  0.50197786\n",
            " 0.5073845  0.5013257  0.500429   0.5016836  0.50019276 0.50112027\n",
            " 0.50020885 0.5027191  0.5001212  0.50000525 0.50097233 0.50650436\n",
            " 0.5013689  0.5013291  0.50506467 0.50307673 0.50020504 0.5005149\n",
            " 0.50271195 0.50163007 0.50226    0.5042193  0.5001954  0.50152683\n",
            " 0.5023235  0.5004832  0.50673765]\n",
            "\n",
            "Matched draws\n",
            "Count: 31, Index: (array([  28,   66,  261,  275,  333,  463,  476,  502,  559,  843,  859,\n",
            "       1085, 1093, 1156, 1178, 1268, 1374, 1618, 2117, 2327, 2405, 2611,\n",
            "       2692, 3150, 3211, 3654, 3664, 6550, 7173, 7436, 8556]),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "104996  497319 2019-07-13      2ndPrizeNo       28\n",
            "105186  498119 2019-07-30  ConsolationNo4       66\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "        DrawNo   DrawDate        PrizeType  LuckyNo\n",
            "104965  497119 2019-07-07       SpecialNo3      275\n",
            "104996  497319 2019-07-13       2ndPrizeNo       28\n",
            "105002  497319 2019-07-13   ConsolationNo4      502\n",
            "105056  497519 2019-07-17       SpecialNo2      333\n",
            "105103  497719 2019-07-21       SpecialNo3      859\n",
            "105124  497819 2019-07-24      SpecialNo10      559\n",
            "105137  497919 2019-07-27  ConsolationNo10      476\n",
            "105179  498119 2019-07-30       1stPrizeNo      261\n",
            "105186  498119 2019-07-30   ConsolationNo4       66\n",
            "105189  498119 2019-07-30   ConsolationNo7      843\n",
            "105208  498219 2019-07-31   ConsolationNo3      463\n",
            "\n",
            "\n",
            "All matched\n",
            "        DrawNo   DrawDate        PrizeType  LuckyNo\n",
            "104918  496919 2019-07-03       SpecialNo2     3654\n",
            "104933  497019 2019-07-06   ConsolationNo4     2405\n",
            "104961  497119 2019-07-07   ConsolationNo9     2692\n",
            "104965  497119 2019-07-07       SpecialNo3      275\n",
            "104979  497219 2019-07-10   ConsolationNo4     7436\n",
            "104986  497219 2019-07-10      SpecialNo10     1085\n",
            "104996  497319 2019-07-13       2ndPrizeNo       28\n",
            "105002  497319 2019-07-13   ConsolationNo4      502\n",
            "105021  497419 2019-07-14   ConsolationNo1     1178\n",
            "105027  497419 2019-07-14   ConsolationNo6     1093\n",
            "105029  497419 2019-07-14   ConsolationNo8     7173\n",
            "105043  497519 2019-07-17       3rdPrizeNo     3664\n",
            "105056  497519 2019-07-17       SpecialNo2      333\n",
            "105063  497519 2019-07-17       SpecialNo9     1618\n",
            "105073  497619 2019-07-20   ConsolationNo6     1156\n",
            "105076  497619 2019-07-20   ConsolationNo9     6550\n",
            "105081  497619 2019-07-20       SpecialNo4     1156\n",
            "105086  497619 2019-07-20       SpecialNo9     3150\n",
            "105090  497719 2019-07-21   ConsolationNo1     8556\n",
            "105103  497719 2019-07-21       SpecialNo3      859\n",
            "105124  497819 2019-07-24      SpecialNo10      559\n",
            "105134  497919 2019-07-27       2ndPrizeNo     2327\n",
            "105137  497919 2019-07-27  ConsolationNo10      476\n",
            "105138  497919 2019-07-27   ConsolationNo2     1374\n",
            "105151  497919 2019-07-27       SpecialNo5     2117\n",
            "105174  498019 2019-07-28       SpecialNo5     1374\n",
            "105179  498119 2019-07-30       1stPrizeNo      261\n",
            "105183  498119 2019-07-30  ConsolationNo10     2611\n",
            "105186  498119 2019-07-30   ConsolationNo4       66\n",
            "105189  498119 2019-07-30   ConsolationNo7      843\n",
            "105193  498119 2019-07-30      SpecialNo10     1268\n",
            "105208  498219 2019-07-31   ConsolationNo3      463\n",
            "105214  498219 2019-07-31   ConsolationNo9     3211\n",
            "\n",
            "-----------2019-07-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(899893, 31) (899893,) (10000, 31) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 869724, 1: 30169})\n",
            "\n",
            "scale_pos_weight - 28.799571613245387\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 300, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.799571613245387, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.799571613245387]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8tcqn4yIl21",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}