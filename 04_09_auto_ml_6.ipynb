{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_02_automated_machine_learning.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengwangk/dl-projects/blob/master/04_09_auto_ml_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4hyoPGdjpqa_"
      },
      "source": [
        "# Machine Learning - Finance 20200712"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SLxr2k_ue8yq",
        "colab": {}
      },
      "source": [
        "COLAB = True\n",
        "\n",
        "DATASET_NAME = '4D.zip'\n",
        "\n",
        "FEATURE_DATASET_PREFIX = 'feature_matrix_d2_v3'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwYshXtLt7b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "6feecc70-97c7-48c9-ac84-8851334164d6"
      },
      "source": [
        "if COLAB:\n",
        "  !pip install -U imblearn\n",
        "  !pip install -U xgboost\n",
        "\n",
        "  !rm -rf dl-projects\n",
        "  !git clone https://github.com/mengwangk/dl-projects\n",
        "  \n",
        "  !cp dl-projects/utils* .\n",
        "  !cp dl-projects/preprocess* .\n",
        "  !cp dl-projects/plot* ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: imblearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied, skipping upgrade: imbalanced-learn in /usr/local/lib/python3.6/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.8.2 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.20 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.13.3 in /usr/local/lib/python3.6/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (0.16.0)\n",
            "Collecting xgboost\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/32/a11befbb003e0e6b7e062a77f010dfcec0ec3589be537b02d2eb2ff93b9a/xgboost-1.1.1-py3-none-manylinux2010_x86_64.whl (127.6MB)\n",
            "\u001b[K     |████████████████████████████████| 127.6MB 30kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.18.5)\n",
            "Installing collected packages: xgboost\n",
            "  Found existing installation: xgboost 0.90\n",
            "    Uninstalling xgboost-0.90:\n",
            "      Successfully uninstalled xgboost-0.90\n",
            "Successfully installed xgboost-1.1.1\n",
            "Cloning into 'dl-projects'...\n",
            "remote: Enumerating objects: 95, done.\u001b[K\n",
            "remote: Counting objects: 100% (95/95), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 2072 (delta 54), reused 25 (delta 10), pack-reused 1977\u001b[K\n",
            "Receiving objects: 100% (2072/2072), 79.15 MiB | 36.46 MiB/s, done.\n",
            "Resolving deltas: 100% (1279/1279), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fC2-l3JBpqbE",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TP7V_IzepqbK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "e16470a6-852b-4ad1-de6b-01b68ab65960"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "import matplotlib\n",
        "import sys\n",
        "import gc\n",
        "import pylab as pl\n",
        "import xgboost as xgb\n",
        "\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import featuretools as ft\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, make_scorer, recall_score, roc_curve, mean_squared_error, accuracy_score, average_precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                     ClusterCentroids,\n",
        "                                     TomekLinks,\n",
        "                                     NeighbourhoodCleaningRule,\n",
        "                                     AllKNN,\n",
        "                                     NearMiss,\n",
        "                                     OneSidedSelection,\n",
        "                                     EditedNearestNeighbours)\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
        "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC, ADASYN \n",
        "from collections import Counter\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "from utils import feature_selection, plot_feature_importances\n",
        "from plot import plot_correlation_matrix, plot_labeled_scatter\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "from preprocess import *\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "plt.style.use('fivethirtyeight')\n",
        "sns.set(style=\"ticks\")\n",
        "\n",
        "# The Answer to the Ultimate Question of Life, the Universe, and Everything.\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3bFT5CoxpqbP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "824f685d-807e-4841-9b31-8c39d017a364"
      },
      "source": [
        "%aimport"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modules to reload:\n",
            "all-except-skipped\n",
            "\n",
            "Modules to skip:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3E16jPVPpqbV"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U421BuhtfYS7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "b07049f3-e133-44f3-b912-1a63b20fc472"
      },
      "source": [
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  GDRIVE_DATASET_FOLDER = Path('gdrive/My Drive/datasets/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9IgnETKkpqbX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "b00f1796-af79-4203-cb22-717b75c1bfba"
      },
      "source": [
        "if COLAB:\n",
        "  DATASET_PATH = GDRIVE_DATASET_FOLDER\n",
        "  ORIGIN_DATASET_PATH = Path('dl-projects/datasets')\n",
        "else:\n",
        "  DATASET_PATH = Path(\"../datasets\")\n",
        "  ORIGIN_DATASET_PATH = Path('datasets')\n",
        "\n",
        "DATASET = DATASET_PATH/f\"{FEATURE_DATASET_PREFIX}.ft\"\n",
        "ORIGIN_DATASET = ORIGIN_DATASET_PATH/DATASET_NAME\n",
        "\n",
        "if COLAB:\n",
        "  !ls -l gdrive/\"My Drive\"/datasets/ --block-size=M\n",
        "  !ls -l dl-projects/datasets --block-size=M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4948M\n",
            "-rw------- 1 root root   17M Jul  6 14:06 feature_matrix_2020_apr.ft\n",
            "-rw------- 1 root root   17M Jul  6 14:06 feature_matrix_2020_apr_orig.pkl\n",
            "-rw------- 1 root root   17M Mar  1 05:47 feature_matrix_2020_feb.ft\n",
            "-rw------- 1 root root   17M Mar  1 05:39 feature_matrix_2020_jan.ft\n",
            "-rw------- 1 root root   17M Jul  7 13:52 feature_matrix_2020_jul.ft\n",
            "-rw------- 1 root root   17M Jul  7 13:52 feature_matrix_2020_jul_orig.pkl\n",
            "-rw------- 1 root root   17M Jul  5 07:30 feature_matrix_2020_jun.ft\n",
            "-rw------- 1 root root   17M Jul  5 07:30 feature_matrix_2020_jun_orig.pkl\n",
            "-rw------- 1 root root   17M Jul  6 13:56 feature_matrix_2020_mar.ft\n",
            "-rw------- 1 root root   17M Jul  6 13:56 feature_matrix_2020_mar_orig.pkl\n",
            "-rw------- 1 root root   17M Jul  7 13:40 feature_matrix_2020_may.ft\n",
            "-rw------- 1 root root   17M Jul  7 13:40 feature_matrix_2020_may_orig.pkl\n",
            "-rw------- 1 root root 2454M Jan 12  2020 feature_matrix_d2_v2.ft\n",
            "-rw------- 1 root root 1585M Jan 12  2020 feature_matrix_d2_v3.ft\n",
            "-rw------- 1 root root   17M Feb 21 13:07 feature_matrix_snapshot.ft\n",
            "-rw------- 1 root root    1M Jul  7 13:43 labels.csv\n",
            "-rw------- 1 root root    5M Jan 30 04:33 orig_X_test.ft\n",
            "-rw------- 1 root root  415M Jan 30 04:33 orig_X_train.ft\n",
            "-rw------- 1 root root    1M Jan 30 04:33 orig_y_test.ft\n",
            "-rw------- 1 root root    7M Jan 30 04:33 orig_y_train.ft\n",
            "-rw------- 1 root root    3M Jul 15 14:45 test_X_test.ft\n",
            "-rw------- 1 root root  259M Jul 15 14:44 test_X_train.ft\n",
            "-rw------- 1 root root    1M Jul 15 14:45 test_y_test.ft\n",
            "-rw------- 1 root root    8M Jul 15 14:45 test_y_train.ft\n",
            "total 26M\n",
            "-rw-r--r-- 1 root root  1M Jul 16 14:08 4D.txt\n",
            "-rw-r--r-- 1 root root  1M Jul 16 14:08 4D.zip\n",
            "-rw-r--r-- 1 root root 25M Jul 16 14:08 labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "urQTD6DQNutw",
        "colab": {}
      },
      "source": [
        "# Read the data\n",
        "data = pd.read_feather(DATASET)\n",
        "origin_data = format_tabular(ORIGIN_DATASET)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djbAQMP7VEsD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Display null rows\n",
        "# data[data.isna().any(axis=1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zov05QHZxxiS"
      },
      "source": [
        "## Add new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "foPB8T1vx2tp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e17e1069-c0b1-4528-f153-44991a81cd40"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(959893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43sc1Eaux25j",
        "colab": {}
      },
      "source": [
        "jan_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_jan.ft\")\n",
        "feb_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_feb.ft\")\n",
        "mar_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_mar.ft\")\n",
        "apr_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_apr.ft\")\n",
        "may_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_may.ft\")\n",
        "jun_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_jun.ft\")\n",
        "jul_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_jul.ft\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2vISxEbsyQG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d57cba2b-689f-42cb-f133-d99fa9ccd161"
      },
      "source": [
        "new_data = data.append(jan_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(feb_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(mar_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(apr_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(may_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(jun_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(jul_2020[data.columns],ignore_index=True)\n",
        "new_data.shape "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1029893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FVVMXCj-zyaW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "347a567b-7dca-44c8-d5fd-6e3e816e88db"
      },
      "source": [
        "data = new_data\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1029893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9o3f2PXXSMJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "a9790d2f-abb9-409d-eb16-cf72ef09a5d9"
      },
      "source": [
        "data.isna().sum().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CUM_SUM(SKEW(Results.TotalStrike))           7685\n",
              "CUM_MEAN(TREND(Results.DrawNo, DrawDate))    7685\n",
              "TREND(Results.CUM_SUM(DrawNo), DrawDate)     7685\n",
              "TREND(Results.CUM_SUM(LuckyNo), DrawDate)    7685\n",
              "CUM_SUM(SKEW(Results.LuckyNo))               7685\n",
              "                                             ... \n",
              "CUM_SUM(MIN(Results.DrawNo))                    0\n",
              "NUM_UNIQUE(Results.DAY(DrawDate))               0\n",
              "NUM_UNIQUE(Results.MONTH(DrawDate))             0\n",
              "SUM(Results.PERCENTILE(LuckyNo))                0\n",
              "NumberId                                        0\n",
              "Length: 217, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUHePaWKSZDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#display(data.head(10).T)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vOYlp-8Br61r"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kHiN1VVlG9Kh"
      },
      "source": [
        "### View data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnQXyVqng5Cm",
        "colab": {}
      },
      "source": [
        "# Feature matrix\n",
        "feature_matrix = data.drop(columns=['NumberId', 'month', 'year'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "apMYVNz9HK9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f4abad73-5406-41a2-a36c-55585ef19250"
      },
      "source": [
        "# Sort data\n",
        "feature_matrix.sort_values(by=['time', 'MAX(Results.LuckyNo)'], inplace=True)\n",
        "feature_matrix.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1029893 entries, 7020 to 1026411\n",
            "Columns: 214 entries, time to LAST(Results.PrizeType)_Prize\n",
            "dtypes: datetime64[ns](1), float64(155), int64(56), uint8(2)\n",
            "memory usage: 1.6 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZKTbWRFJNUq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "79c772de-738b-475f-f91d-578c7f562c30"
      },
      "source": [
        "print('Distribution')\n",
        "print(feature_matrix['Label'].value_counts())\n",
        "print()\n",
        "print('Positive: ' + str(feature_matrix['Label'].value_counts()[0]) + ' which is ', round(feature_matrix['Label'].value_counts()[0]/len(feature_matrix) * 100,2), '% of the dataset')\n",
        "print('Negative: ' + str(feature_matrix['Label'].value_counts()[1]) + ' which is ', round(feature_matrix['Label'].value_counts()[1]/len(feature_matrix) * 100,2), '% of the dataset')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution\n",
            "0    996848\n",
            "1     33045\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Positive: 996848 which is  96.79 % of the dataset\n",
            "Negative: 33045 which is  3.21 % of the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "plplpAQ6JrKb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ca989d45-d018-481b-88b5-06774b9ad66f"
      },
      "source": [
        "feature_matrix.isna().sum().sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))               7685\n",
              "CUM_MEAN(TREND(Results.LuckyNo, DrawDate))                7685\n",
              "TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)    7685\n",
              "TREND(Results.CUM_MEAN(TotalStrike), DrawDate)            7685\n",
              "TREND(Results.CUM_SUM(LuckyNo), DrawDate)                 7685\n",
              "                                                          ... \n",
              "CUM_SUM(MIN(Results.DrawNo))                                 0\n",
              "NUM_UNIQUE(Results.DAY(DrawDate))                            0\n",
              "NUM_UNIQUE(Results.MONTH(DrawDate))                          0\n",
              "SUM(Results.PERCENTILE(LuckyNo))                             0\n",
              "time                                                         0\n",
              "Length: 214, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF_zCRksL1Ls"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1aLGsXSOa9K",
        "colab": {}
      },
      "source": [
        "# Fill all NaN with 0\n",
        "feature_matrix = feature_matrix.fillna(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5A8LZ805MqjP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "823a43cf-d482-4a0a-fabc-40a32bc05428"
      },
      "source": [
        "feature_matrix.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1029893, 214)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rPFOkiGjhuKj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "485862af-436c-40a3-aedc-943a14fc025a"
      },
      "source": [
        "# With feature selection\n",
        "feature_matrix_selection = feature_selection(feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label']))\n",
        "\n",
        "# Without feature selection\n",
        "# feature_matrix_selection = feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape:  (1029893, 211)\n",
            "0 missing columns with threshold: 90.\n",
            "41 zero variance columns.\n",
            "109 collinear columns removed with threshold: 0.95.\n",
            "Total columns removed:  150\n",
            "Shape after feature selection: (1029893, 61).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vT2K0WeJhugH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        },
        "outputId": "c883149a-0c77-4ee3-ac82-db911ce26c45"
      },
      "source": [
        "feature_matrix_selection.shape, feature_matrix_selection.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1029893, 61),\n",
              " Index(['STD(Results.DrawNo)', 'MAX(Results.DrawNo)', 'MAX(Results.LuckyNo)',\n",
              "        'MIN(Results.DrawNo)', 'MEAN(Results.DrawNo)', 'SKEW(Results.DrawNo)',\n",
              "        'AVG_TIME_BETWEEN(Results.DrawDate)', 'COUNT(Results)',\n",
              "        'SUM(Results.DrawNo)', 'SUM(Results.LuckyNo)',\n",
              "        'TREND(Results.DrawNo, DrawDate)', 'MONTH(first_Results_time)',\n",
              "        'DAY(first_Results_time)', 'TIME_SINCE(first_Results_time)',\n",
              "        'TIME_SINCE_PREVIOUS(first_Results_time)',\n",
              "        'STD(Results.PERCENTILE(DrawNo))',\n",
              "        'STD(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'STD(Results.CUM_SUM(DrawNo))', 'STD(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MAX(Results.PERCENTILE(DrawNo))',\n",
              "        'MAX(Results.PERCENTILE(TotalStrike))',\n",
              "        'MAX(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MIN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MIN(Results.CUM_SUM(DrawNo))', 'MIN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MODE(Results.MONTH(DrawDate))', 'MODE(Results.DAY(DrawDate))',\n",
              "        'MEAN(Results.TIME_SINCE(DrawDate))',\n",
              "        'MEAN(Results.PERCENTILE(DrawNo))', 'MEAN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'SKEW(Results.CUM_SUM(DrawNo))', 'SKEW(Results.CUM_MEAN(LuckyNo))',\n",
              "        'LAST(Results.DAY(DrawDate))',\n",
              "        'LAST(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'LAST(Results.MONTH(DrawDate))', 'LAST(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SUM(Results.TIME_SINCE(DrawDate))',\n",
              "        'SUM(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'TREND(Results.CUM_MEAN(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(DrawNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(TotalStrike), DrawDate)',\n",
              "        'TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)',\n",
              "        'TREND(Results.CUM_SUM(DrawNo), DrawDate)',\n",
              "        'NUM_UNIQUE(Results.MONTH(DrawDate))',\n",
              "        'NUM_UNIQUE(Results.DAY(DrawDate))', 'CUM_SUM(MIN(Results.DrawNo))',\n",
              "        'CUM_SUM(SKEW(Results.DrawNo))',\n",
              "        'CUM_MEAN(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'CUM_MEAN(SUM(Results.LuckyNo))', 'CUM_MEAN(SKEW(Results.DrawNo))',\n",
              "        'PERCENTILE(STD(Results.LuckyNo))', 'PERCENTILE(LAST(Results.DrawNo))',\n",
              "        'PERCENTILE(MAX(Results.TotalStrike))',\n",
              "        'PERCENTILE(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'PERCENTILE(COUNT(Results))', 'PERCENTILE(STD(Results.DrawNo))',\n",
              "        'PERCENTILE(SKEW(Results.DrawNo))', 'PERCENTILE(SUM(Results.DrawNo))',\n",
              "        'PERCENTILE(TREND(Results.DrawNo, DrawDate))'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yZUhYrWFiRod",
        "colab": {}
      },
      "source": [
        "feature_matrix_selection['time'] = feature_matrix['time']\n",
        "feature_matrix_selection['TotalStrike'] = feature_matrix['TotalStrike']\n",
        "feature_matrix_selection['Label'] = feature_matrix['Label']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMuvyRxvxFIm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df48f682-2316-4abe-c8d8-14caf73e7d51"
      },
      "source": [
        "feature_matrix_selection.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1029893, 64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "waeD1ED_kqDB"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9yrJyIVLh5So",
        "colab": {}
      },
      "source": [
        "def recall_optim(y_true, y_pred):\n",
        "    \"\"\"Make a scoring function that improves specificity while identifying all strikes\n",
        "    \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Recall will be worth a greater value than specificity\n",
        "    rec = recall_score(y_true, y_pred) * 0.8 \n",
        "    spe = conf_matrix[0,0]/conf_matrix[0,:].sum() * 0.2 \n",
        "    \n",
        "    # Imperfect recalls will lose a penalty\n",
        "    # This means the best results will have perfect recalls and compete for specificity\n",
        "    if rec < 0.8:\n",
        "        rec -= 0.2\n",
        "    return rec + spe \n",
        "\n",
        "# Make a scoring callable from recall_score\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# DataFrame to store classifier performance\n",
        "performance = pd.DataFrame(columns=['Train_Recall','Test_Recall','Test_Specificity', 'Optimize'])\n",
        "\n",
        "def to_labels(pos_probs, threshold):\n",
        "    \"\"\"Apply threshold to positive probabilities to create labels.\n",
        "    \"\"\"\n",
        "    return (pos_probs >= threshold).astype('int')\n",
        " \n",
        "\n",
        "def score_optimization(dt, feature_matrix, clf, params, X_train, \n",
        "                       y_train, X_test, y_test, skip_grid_search_cv=False,\n",
        "                       optimized_scorer=False):\n",
        "    \"\"\"Find the optimized classifier.\n",
        "    \"\"\"\n",
        "    if not skip_grid_search_cv:\n",
        "      print(\"\\nFinding the optimized classifier...\")\n",
        "\n",
        "      # Load GridSearchCV\n",
        "      # search = GridSearchCV(\n",
        "      search = RandomizedSearchCV(\n",
        "            estimator=clf,\n",
        "            #param_grid=params,\n",
        "            param_distributions=params,\n",
        "            n_jobs=4,\n",
        "            scoring=optimize  # Use custom scorer\n",
        "      )\n",
        "\n",
        "      # Train search object\n",
        "      search.fit(X_train, y_train)\n",
        "\n",
        "      # Heading\n",
        "      print('\\n','-'*40,'\\n',clf.__class__.__name__,'\\n','-'*40)\n",
        "\n",
        "      # Extract best estimator\n",
        "      best = search.best_estimator_\n",
        "      print('Best parameters: \\n\\n',search.best_params_,'\\n')\n",
        "    \n",
        "    else:\n",
        "      print(\"\\nUse the passed in classifier...\\n\")\n",
        "      best = clf\n",
        "\n",
        "    # Cross-validate on the train data\n",
        "    if not skip_grid_search_cv: \n",
        "      print(\"TRAIN GROUP\")\n",
        "      #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
        "      cv = 3\n",
        "      if not optimized_scorer:\n",
        "        print('\\nUse default scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  scoring=recall,\n",
        "                                  cv=cv)\n",
        "      else:\n",
        "        print('\\nUse optimized scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  #scoring=optimize,\n",
        "                                  scoring='roc_auc',\n",
        "                                  #scoring=geo_mean_scorer,\n",
        "                                  cv=cv)\n",
        "\n",
        "      print(\"\\nCross-validation recall scores:\",train_cv)\n",
        "      print(\"Mean recall score:\",train_cv.mean())\n",
        "      print('Mean G-Mean: %.3f (%.3f)' % (np.mean(train_cv), np.std(train_cv)))\n",
        "    else:\n",
        "      train_cv = np.zeros(3)\n",
        "\n",
        "    # Now predict on the test group\n",
        "    print(\"\\nTEST GROUP\")\n",
        "    y_pred = best.fit(X_train, y_train).predict(X_test)\n",
        "    # y_pred = best.fit(X_train, y_train,\n",
        "    #                   eval_set=[(X_test, y_test)],\n",
        "    #                   eval_metric='auc',\n",
        "    #                   early_stopping_rounds=10,\n",
        "    #                   verbose=True\n",
        "    #                   ).predict(X_test)\n",
        "\n",
        "    # keep probabilities for the positive outcome only\n",
        "    probas = best.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # define thresholds\n",
        "    thresholds = np.arange(0, 1, 0.001)\n",
        "\n",
        "    # evaluate each threshold\n",
        "    scores = [f1_score(y_test, to_labels(probas, t)) for t in thresholds]\n",
        "\n",
        "    # get best threshold\n",
        "    ix = np.argmax(scores)\n",
        "    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))\n",
        "\n",
        "    # print recall\n",
        "    print(\"\\nRecall:\",recall_score(y_test,y_pred))\n",
        "\n",
        "    # Get imbalanced classification report\n",
        "    print(classification_report_imbalanced(y_test, y_pred))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test,y_pred)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=plt.cm.copper)\n",
        "    plt.show()\n",
        "\n",
        "    # Store results\n",
        "    performance.loc[clf.__class__.__name__+'_optimize',:] = [\n",
        "        train_cv.mean(),\n",
        "        recall_score(y_test,y_pred),\n",
        "        conf_matrix[0,0]/conf_matrix[0,:].sum(),\n",
        "        recall_optim(y_test,y_pred)\n",
        "    ]\n",
        "    # Look at the parameters for the top best scores\n",
        "    if not skip_grid_search_cv:\n",
        "      display(pd.DataFrame(search.cv_results_).iloc[:,4:].sort_values(by='rank_test_score').head())\n",
        "    display(performance)\n",
        "\n",
        "    # Additionl info\n",
        "    print('\\n\\nAdditional Info')\n",
        "    print('='*40)\n",
        "    positive = np.where((y_pred==1))\n",
        "    print(f'Total predicted to be positive: {len(positive[0])} \\n')\n",
        "\n",
        "    pred = np.where((y_pred==1))\n",
        "    all_preds = pred[0]\n",
        "\n",
        "    # Total predicted matches\n",
        "    print('First 23 matches')\n",
        "    print(23, all_preds[0:23])\n",
        "    print(f'\\n{probas[all_preds[0:23]]}\\n') \n",
        "\n",
        "    print(\"\\nTop 23 Probable Matches\")\n",
        "    #print('probas', probas)\n",
        "    topN = np.argpartition(probas, -23)[-23:]\n",
        "    print(f'\\n{topN}\\n')          # Top N most high probability numbers\n",
        "    print(f'\\n{probas[topN]}\\n')  # Top N probability\n",
        "\n",
        "    # Check for 2 to 3 digits range \n",
        "    print('\\n2 To 3 Digits\\n')\n",
        "    idx_range = np.where((all_preds < 1000) & (all_preds >= 10))\n",
        "    #print(idx_range)\n",
        "    range_numbers = all_preds[idx_range]\n",
        "    print(len(range_numbers), range_numbers)\n",
        "    print(f'\\n{probas[range_numbers]}\\n') \n",
        "\n",
        "\n",
        "    # 2 to 3 Digits > Average Probas\n",
        "    print('\\n2 To 3 Digits Average Proba\\n')\n",
        "    avg_proba = np.average(probas[range_numbers])\n",
        "    print(f'Average proba {avg_proba}\\n')\n",
        "    idx_avg_proba = np.where(probas > avg_proba) \n",
        "    print(len(idx_avg_proba[0]), idx_avg_proba[0])\n",
        "\n",
        "    # 2 to 3 Digits > All Average Probas\n",
        "    print('\\n\\nAll Average Proba\\n')\n",
        "    all_avg_proba = np.average(probas[all_preds])\n",
        "    print(f'All average probas {all_avg_proba}\\n')\n",
        "    idx_all_avg_proba = np.where(probas > all_avg_proba) \n",
        "    print(len(idx_all_avg_proba[0]), idx_all_avg_proba[0])\n",
        "\n",
        "\n",
        "    # All predicted matches\n",
        "    print('\\n\\nAll Predictions\\n')\n",
        "    print(len(all_preds), all_preds)\n",
        "    print()\n",
        "    print(len(probas[all_preds]), probas[all_preds])\n",
        "    \n",
        "    #print('Debug')\n",
        "    #print(pred)\n",
        "    \n",
        "    if len(positive[0]) > 0:\n",
        "    \n",
        "      # Matching draws\n",
        "      print('\\nMatched draws')\n",
        "      md = np.where((y_pred==1) & (y_test==1))\n",
        "      print(f\"Count: {len(md[0])}, Index: {md}\")\n",
        "      month_data = feature_matrix.loc[feature_matrix['time'] == dt]\n",
        "      numbers = month_data.iloc[md[0]][['MAX(Results.LuckyNo)']]\n",
        "\n",
        "      print('\\n\\nTop 23 Possibility')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(topN))].head(23))  \n",
        "      \n",
        "      print('\\n\\nFirst 23 Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(pred[0][0:23]))].head(23))    \n",
        "             \n",
        "\n",
        "      print('\\n\\n2 To 3 Digits Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(range_numbers))].head(23))    \n",
        "     \n",
        "\n",
        "      print('\\n\\nAll matched')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(numbers['MAX(Results.LuckyNo)']))].head(100))    \n",
        "                                                  \n",
        "    else:\n",
        "      print('No luck this month')  \n",
        "\n",
        "    if len(range_numbers) >= 50:\n",
        "      return False\n",
        "\n",
        "    return True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrL8gYwjc-hd",
        "colab": {}
      },
      "source": [
        "def remove_outliers(balanced, threshold=0.001, remove=True):\n",
        "    \"\"\"Removing Outliers from high-correlation features.\n",
        "    \"\"\"\n",
        "\n",
        "    if not remove:\n",
        "      return balanced\n",
        "\n",
        "    bal_corr = balanced.corr()\n",
        "    no_outliers=pd.DataFrame(balanced.copy())\n",
        "\n",
        "    cols = bal_corr.Label.index[:-1]\n",
        "\n",
        "    # For each feature correlated with Class...\n",
        "    for col in cols:\n",
        "        # If absolute correlation value is more than X percent...\n",
        "        correlation = bal_corr.loc['Label',col]\n",
        "\n",
        "        if np.absolute(correlation) > threshold:\n",
        "          # Separate the classes of the high-correlation column\n",
        "          nonstrikes = no_outliers.loc[no_outliers.Label==0,col]\n",
        "          strikes = no_outliers.loc[no_outliers.Label==1,col]\n",
        "\n",
        "          # Identify the 25th and 75th quartiles\n",
        "          all_values = no_outliers.loc[:,col]\n",
        "          q25, q75 = np.percentile(all_values, 25), np.percentile(all_values, 75)\n",
        "          # Get the inter quartile range\n",
        "          iqr = q75 - q25\n",
        "          # Smaller cutoffs will remove more outliers\n",
        "          cutoff = iqr * 7\n",
        "          # Set the bounds of the desired portion to keep\n",
        "          lower, upper = q25 - cutoff, q75 + cutoff\n",
        "          \n",
        "          # If positively correlated...\n",
        "          # Drop nonstrikes above upper bound, and strikes below lower bound\n",
        "          if correlation > 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes>upper].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes<lower].index,inplace=True)\n",
        "          \n",
        "          # If negatively correlated...\n",
        "          # Drop non strikes below lower bound, and strikes above upper bound\n",
        "          elif correlation < 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes<lower].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes>upper].index,inplace=True)\n",
        "        \n",
        "    print('\\nData shape before removing outliers:', balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in previous data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "    print('-'*40)\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after removing outliers:', no_outliers.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(no_outliers.Label.value_counts())\n",
        "\n",
        "    # no_outliers.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distributions with Less Outliers', fontsize=17)\n",
        "    # plt.show()\n",
        "    \n",
        "    no_outliers.reset_index(drop=True, inplace=True)\n",
        "    return no_outliers\n",
        "\n",
        "\n",
        "def filter_features(no_outliers, threshold=0.001):\n",
        "    \"\"\"Feature selection.\n",
        "    \"\"\"\n",
        "    feat_sel = pd.DataFrame(no_outliers.copy())\n",
        "\n",
        "    # Feature Selection based on correlation with label\n",
        "    print('\\nData shape before feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes before feature selection:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # Correlation matrix after removing outliers\n",
        "    new_corr = feat_sel.corr()\n",
        "\n",
        "    for col in new_corr.Label.index[:-1]:\n",
        "        # Pick desired cutoff for dropping features. In absolute-value terms.\n",
        "        if np.absolute(new_corr.loc['Label',col]) < threshold:\n",
        "            # Drop the feature if correlation is below cutoff\n",
        "            feat_sel.drop(columns=col,inplace=True)\n",
        "\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in new data:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "\n",
        "    return feat_sel\n",
        "\n",
        "def under_sampler(data, sample_size=20000, sampling=False):\n",
        "    # Undersample model for efficiency and balance classes.\n",
        "\n",
        "    X_train = data.drop('Label',1)\n",
        "    y_train = data.Label\n",
        "\n",
        "    if not sampling:\n",
        "      return X_train, y_train\n",
        "\n",
        "    # After feature-selection, X_test needs to include only the same features as X_train\n",
        "    # cols = X_train.columns\n",
        "    # X_test = X_test[cols]\n",
        "\n",
        "    # Undersample and balance classes\n",
        "    X_train, y_train = RandomUnderSampler(sampling_strategy={1:sample_size,0:sample_size}).fit_resample(X_train,y_train)\n",
        "\n",
        "    print('\\nX_train shape after reduction:', X_train.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in y_train:')\n",
        "    print(np.unique(y_train, return_counts=True))\n",
        "\n",
        "    return X_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pl5ZoepSNPf4",
        "colab": {}
      },
      "source": [
        "def gen_train_test_set(dt, feature_matrix, file_prefix='data'): \n",
        "    \n",
        "    # Subset labels\n",
        "    test_labels = feature_matrix.loc[feature_matrix['time'] == dt, 'Label']\n",
        "    train_labels = feature_matrix.loc[feature_matrix['time'] < dt, 'Label']\n",
        "\n",
        "    # Features\n",
        "    X_train = feature_matrix[feature_matrix['time'] < dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    X_test = feature_matrix[feature_matrix['time'] == dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    feature_names = list(X_train.columns)\n",
        "    \n",
        "    # Labels\n",
        "    y_train = np.array(train_labels).reshape((-1, ))\n",
        "    y_test = np.array(test_labels).reshape((-1, ))\n",
        "    \n",
        "    print('Training on {} observations.'.format(len(X_train)))\n",
        "    print('Testing on {} observations.\\n'.format(len(X_test)))\n",
        "\n",
        "    # Join the train data\n",
        "    train = X_train.join(train_labels)\n",
        "\n",
        "    print('Data shape before balancing:', train.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in previous data:')\n",
        "    print(train.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # sklearn pipeline\n",
        "    pipeline = make_pipeline(\n",
        "        SimpleImputer(strategy = 'constant', fill_value=0),\n",
        "        StandardScaler())\n",
        "    \n",
        "    X_train = pipeline.fit_transform(X_train)\n",
        "    X_test = pipeline.transform(X_test)\n",
        "\n",
        "    # imblearn pipeline\n",
        "    imb_pipeline = make_pipeline_imb(\n",
        "          # NearMiss(version=3, n_neighbors_ver3=3, n_jobs=4)\n",
        "          # SMOTE(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # ADASYN(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # OneSidedSelection(n_neighbors=1, n_seeds_S=200, random_state=42, n_jobs=4)\n",
        "          SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "    )\n",
        "     \n",
        "    # Balance the data\n",
        "    print('\\nNo balancing')\n",
        "    X_bal = pd.DataFrame(X_train,columns=feature_names)\n",
        "    y_bal = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    balanced = X_bal.join(y_bal)\n",
        "\n",
        "    # print('-'*40)\n",
        "    print('Data shape after balancing:',balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "\n",
        "    # Remove high correlation outliers\n",
        "    no_outliers = remove_outliers(balanced, remove=False)\n",
        "   \n",
        "    # Remove features with low correlation\n",
        "    remove_features = True\n",
        "    if remove_features:\n",
        "      print('\\nFiltering features')\n",
        "      features_selected = filter_features(no_outliers)\n",
        "    else:\n",
        "      print('\\nNO filtering')\n",
        "      features_selected = no_outliers \n",
        "\n",
        "    columns_selected = features_selected.drop('Label',1).columns\n",
        "\n",
        "    # Under sampling\n",
        "    X_train, y_train = under_sampler(features_selected, sampling=False) \n",
        "    X_train = pd.DataFrame(X_train,columns=columns_selected)\n",
        "    y_train = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    # For X_test, now only use the selected features\n",
        "    X_test = pd.DataFrame(X_test,columns=feature_names)\n",
        "    X_test = X_test[columns_selected]\n",
        "    y_test = pd.DataFrame(y_test,columns=['Label'])\n",
        "\n",
        "    #print(X_train.describe())\n",
        "    #return\n",
        "\n",
        "    # Save data\n",
        "    # print(X_train.head(10))\n",
        "    # print(y_train.head(10)) \n",
        "\n",
        "    # print(X_test.head(10))\n",
        "    # print(y_test.head(10)) \n",
        "    X_train.to_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "    y_train.to_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "   \n",
        "    X_test.to_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "    y_test.to_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "\n",
        "    gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PcKlL67TP9UM",
        "colab": {}
      },
      "source": [
        "def model(dt, feature_matrix, file_prefix='data', class_weight=1.0, estimators=300, depth=3):\n",
        "    \"\"\"Predict for a particular month.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load train test\n",
        "    X_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "    y_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "    X_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "    y_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "\n",
        "    print(f'\\n-----------{dt}-----------------\\n')\n",
        "\n",
        "    # Reshape\n",
        "    y_train = np.array(y_train).reshape((-1, ))\n",
        "    y_test = np.array(y_test).reshape((-1, ))\n",
        "    \n",
        "    print('Data shape')\n",
        "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "    # print(X_train.describe())\n",
        "    # return\n",
        "\n",
        "    # Calculate hit ratio for xgboost classifier\n",
        "    print(\"\\nCalculating scale pos weight\")\n",
        "    counter = Counter(y_train)\n",
        "    print(Counter(y_train))\n",
        "    #scale_pos_weight = float(counter[0] / counter[1])\n",
        "    scale_pos_weight = (float(counter[0] / counter[1])) * class_weight\n",
        "    print(f\"\\nscale_pos_weight - {scale_pos_weight}\\n\")\n",
        "    \n",
        "     # Modeling\n",
        "    eval_set=[(X_train, y_train), (X_test, y_test)]\n",
        "    clf = xgb.XGBClassifier(\n",
        "                    n_jobs=4, \n",
        "                    random_state=42,\n",
        "                    objective='binary:logistic',\n",
        "                    subsample=0.55, \n",
        "                    #n_estimators=300,\n",
        "                    n_estimators=estimators,\n",
        "                    #n_estimators=550,\n",
        "                    min_child_weight=1,\n",
        "                    #max_depth=3, \n",
        "                    max_depth=depth, \n",
        "                    learning_rate=0.007,\n",
        "                    gamma=0.1, \n",
        "                    colsample_bytree=0.95,\n",
        "                    tree_method='hist',\n",
        "                    booster='dart',\n",
        "                    scale_pos_weight=scale_pos_weight\n",
        "                    #eval_metric='auc',\n",
        "                    #eval_set=eval_set,\n",
        "                    #early_stopping_rounds=10\n",
        "                    )\n",
        "\n",
        "    clf_params = clf.get_params()\n",
        "    print(clf_params)\n",
        "\n",
        "    # Set parameters\n",
        "    #clf_params['max_depth'] = 10\n",
        "    #clf.set_params(clf_params)\n",
        "\n",
        "    # Parameters to compare\n",
        "    weights = [i for i in range(1,36,1)]\n",
        "    weights.append(scale_pos_weight)\n",
        "    learn_params = {\n",
        "        'n_estimators': [100, 300, 500, 800, 1000], \n",
        "        'max_depth': range(3,10,2),\n",
        "        'min_child_weight': range(1,6,2),\n",
        "        #'gamma':[i/10.0 for i in range(0,5)],\n",
        "        'subsample':[i/100.0 for i in range(55,70,5)],\n",
        "        'colsample_bytree':[i/100.0 for i in range(85,100,5)],\n",
        "        #'learning_rate':[i/1000.0 for i in range(5,20,2)],\n",
        "        'scale_pos_weight': weights\n",
        "    }\n",
        "    print(f'Parameter distribution: {learn_params}')\n",
        "    \n",
        "    # Test and validate\n",
        "    ret_val = score_optimization(dt,\n",
        "                       feature_matrix,\n",
        "                       clf, \n",
        "                       learn_params,  \n",
        "                       X_train, \n",
        "                       y_train, \n",
        "                       X_test, \n",
        "                       y_test, \n",
        "                       skip_grid_search_cv=True,\n",
        "                       optimized_scorer=True)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return ret_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ns3Puh7Gnxl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "430d53fc-20ac-460b-ca9c-77039ba38081"
      },
      "source": [
        "target_mth = pd.datetime(2020,2,1)\n",
        "%time gen_train_test_set(target_mth, feature_matrix_selection, file_prefix='test')\n",
        "\n",
        "#for (estimators, depth) in ((300,3), (500,6), (550,6), (600,6)):\n",
        "for (estimators, depth) in ((300,3),):\n",
        "  %time model(target_mth, feature_matrix_selection, file_prefix='test', estimators=estimators, depth=depth)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 969893 observations.\n",
            "Testing on 10000 observations.\n",
            "\n",
            "Data shape before balancing: (969893, 62)\n",
            "\n",
            "Counts of strikes vs non-strikes in previous data:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "NO balancing\n",
            "Data shape after balancing: (969893, 62)\n",
            "\n",
            "Counts of strikes VS non-strikes in new data:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Filtering features\n",
            "\n",
            "Data shape before feature selection: (969893, 62)\n",
            "\n",
            "Counts of strikes vs non-strikes before feature selection:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "\n",
            "Data shape after feature selection: (969893, 36)\n",
            "\n",
            "Counts of strikes vs non-strikes in new data:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "CPU times: user 13.4 s, sys: 382 ms, total: 13.8 s\n",
            "Wall time: 14.4 s\n",
            "\n",
            "-----------2020-02-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(969893, 35) (969893,) (10000, 35) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 937508, 1: 32385})\n",
            "\n",
            "scale_pos_weight - 28.94883433688436\n",
            "\n",
            "{'objective': 'binary:logistic', 'base_score': None, 'booster': 'dart', 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': 0.95, 'gamma': 0.1, 'gpu_id': None, 'importance_type': 'gain', 'interaction_constraints': None, 'learning_rate': 0.007, 'max_delta_step': None, 'max_depth': 3, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 300, 'n_jobs': 4, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': None, 'reg_lambda': None, 'scale_pos_weight': 28.94883433688436, 'subsample': 0.55, 'tree_method': 'hist', 'validate_parameters': None, 'verbosity': None}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.94883433688436]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.473, F-Score=0.06273\n",
            "\n",
            "Recall: 0.05362776025236593\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.97      0.94      0.05      0.95      0.22      0.05      9683\n",
            "          1       0.03      0.05      0.94      0.04      0.22      0.05       317\n",
            "\n",
            "avg / total       0.94      0.91      0.08      0.92      0.22      0.05     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0536278</td>\n",
              "      <td>0.940308</td>\n",
              "      <td>0.0309638</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity   Optimize\n",
              "XGBClassifier_optimize            0   0.0536278         0.940308  0.0309638"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 595 \n",
            "\n",
            "First 23 matches\n",
            "23 [  0  13  19  28  29  33  35  42  52  54  72  92 129 131 151 163 185 193\n",
            " 197 199 201 210 216]\n",
            "\n",
            "[0.5053911  0.5023211  0.52201146 0.5008016  0.50219554 0.506599\n",
            " 0.5057873  0.50578946 0.50611764 0.50479656 0.51033074 0.5035855\n",
            " 0.5003583  0.501757   0.5037642  0.50157    0.50098574 0.50731844\n",
            " 0.5051709  0.5038188  0.50012565 0.5095717  0.5053153 ]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[ 895 3111 4465 7836 1933 2479  925 4562 8938 1271 8981 1047 8690   19\n",
            " 5301 5378 7455  950  823 6114 1949 5311  592]\n",
            "\n",
            "\n",
            "[0.51466405 0.51983446 0.51778823 0.5161645  0.5158739  0.5161403\n",
            " 0.51478326 0.52038753 0.52230006 0.52606606 0.5262078  0.52109915\n",
            " 0.5256418  0.52201146 0.5270517  0.5206844  0.5272057  0.52723074\n",
            " 0.5365867  0.52861756 0.5344132  0.5306532  0.53885746]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "182 [ 13  19  28  29  33  35  42  52  54  72  92 129 131 151 163 185 193 197\n",
            " 199 201 210 216 219 226 230 253 270 271 272 278 279 282 283 291 298 302\n",
            " 314 317 325 328 330 336 344 352 354 363 365 366 372 384 392 421 422 430\n",
            " 431 437 446 451 468 479 490 495 496 498 500 502 507 510 520 535 537 538\n",
            " 540 546 553 557 560 567 568 569 572 573 575 580 581 584 587 590 591 592\n",
            " 594 599 603 604 607 608 613 616 618 621 622 623 624 625 626 628 630 634\n",
            " 637 641 644 650 661 666 674 691 692 697 706 708 716 720 728 731 736 743\n",
            " 744 749 752 757 764 765 769 776 787 792 799 802 803 804 806 809 810 811\n",
            " 816 818 823 839 847 854 855 859 868 874 877 885 886 887 892 895 903 905\n",
            " 921 922 923 925 928 929 942 947 950 952 954 957 961 963 974 985 987 988\n",
            " 993 994]\n",
            "\n",
            "[0.5023211  0.52201146 0.5008016  0.50219554 0.506599   0.5057873\n",
            " 0.50578946 0.50611764 0.50479656 0.51033074 0.5035855  0.5003583\n",
            " 0.501757   0.5037642  0.50157    0.50098574 0.50731844 0.5051709\n",
            " 0.5038188  0.50012565 0.5095717  0.5053153  0.5063129  0.50174016\n",
            " 0.5035596  0.50009733 0.5019054  0.5014507  0.50794494 0.5028437\n",
            " 0.5028367  0.5010004  0.50825745 0.5027347  0.5081236  0.5003783\n",
            " 0.50153774 0.5039602  0.5046874  0.50167996 0.5041121  0.5019007\n",
            " 0.5031331  0.5049874  0.5056475  0.5023136  0.50339913 0.50162274\n",
            " 0.5004955  0.50249636 0.51151687 0.50740033 0.50752056 0.50209475\n",
            " 0.5033542  0.5037086  0.50259304 0.50195634 0.5026632  0.5036901\n",
            " 0.5034497  0.5059254  0.5069066  0.50827795 0.5045648  0.5019335\n",
            " 0.51012826 0.5039018  0.5049851  0.50775075 0.50507915 0.5025945\n",
            " 0.50288785 0.5044469  0.5029266  0.5086694  0.5014178  0.5094018\n",
            " 0.50527114 0.5017007  0.50080395 0.5011398  0.5076281  0.503105\n",
            " 0.50389934 0.5006988  0.5003809  0.500123   0.5016588  0.53885746\n",
            " 0.5091121  0.5016115  0.5011755  0.50579816 0.501253   0.50459516\n",
            " 0.5013137  0.5024149  0.5057929  0.5018788  0.5132565  0.5023817\n",
            " 0.5014735  0.5065278  0.50436765 0.50038683 0.50645995 0.5043953\n",
            " 0.50441724 0.5103754  0.50067675 0.50052834 0.50693744 0.5033964\n",
            " 0.5004503  0.50193936 0.5024571  0.5015403  0.50763965 0.5088636\n",
            " 0.5019083  0.50211716 0.50101185 0.50108826 0.50442874 0.5031113\n",
            " 0.50196654 0.50164866 0.5016538  0.5059423  0.5016376  0.5031575\n",
            " 0.5022226  0.5040454  0.50067496 0.5009757  0.5079549  0.5072881\n",
            " 0.50462544 0.50280166 0.5035726  0.5043313  0.5050077  0.500109\n",
            " 0.50235087 0.50019467 0.5365867  0.50922143 0.50181746 0.507621\n",
            " 0.5009384  0.5015431  0.5002851  0.50378555 0.5017731  0.5023961\n",
            " 0.50442976 0.5029555  0.50223243 0.51466405 0.5133867  0.5122643\n",
            " 0.5047615  0.5043256  0.5071186  0.51478326 0.5039612  0.5032702\n",
            " 0.50069875 0.50163776 0.52723074 0.51023746 0.50424355 0.5001381\n",
            " 0.500963   0.50622684 0.50714606 0.501376   0.50194    0.5008018\n",
            " 0.5046382  0.5018275 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.5045644640922546\n",
            "\n",
            "170 [   0   19   33   35   42   52   54   72  193  197  210  216  219  272\n",
            "  283  298  325  352  354  392  421  422  495  496  498  500  507  520\n",
            "  535  537  557  567  568  575  592  594  604  608  618  622  625  630\n",
            "  641  661  706  708  757  799  802  803  810  823  839  854  895  903\n",
            "  905  921  923  925  950  952  963  974  993 1009 1047 1085 1090 1115\n",
            " 1126 1230 1231 1271 1304 1320 1378 1438 1508 1519 1563 1597 1650 1671\n",
            " 1684 1686 1754 1807 1933 1949 1950 1979 2011 2029 2030 2036 2144 2158\n",
            " 2172 2185 2190 2209 2213 2260 2266 2449 2466 2479 2485 2532 2536 2609\n",
            " 2620 2759 2769 2936 2958 3111 3138 3224 3350 3411 3772 3927 3932 4016\n",
            " 4339 4369 4465 4562 4578 4609 4855 4859 4923 5045 5070 5084 5154 5301\n",
            " 5311 5378 5464 5825 6045 6114 6215 6326 6772 6874 7139 7166 7222 7227\n",
            " 7231 7302 7339 7402 7455 7836 7904 8499 8690 8938 8981 9028 9254 9258\n",
            " 9595 9610]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.5039308667182922\n",
            "\n",
            "203 [   0   19   33   35   42   52   54   72  193  197  210  216  219  272\n",
            "  283  298  317  325  330  352  354  392  421  422  495  496  498  500\n",
            "  507  520  535  537  546  557  567  568  575  592  594  604  608  618\n",
            "  622  625  626  630  634  637  641  661  706  708  736  757  776  799\n",
            "  802  803  809  810  823  839  854  886  895  903  905  921  922  923\n",
            "  925  928  950  952  954  963  974  993 1009 1039 1047 1050 1085 1090\n",
            " 1115 1126 1156 1230 1231 1271 1289 1304 1320 1378 1438 1445 1508 1519\n",
            " 1563 1597 1650 1671 1684 1686 1754 1807 1933 1949 1950 1979 1984 2011\n",
            " 2029 2030 2036 2071 2144 2153 2158 2172 2185 2190 2209 2213 2243 2260\n",
            " 2266 2449 2466 2479 2485 2497 2517 2532 2536 2550 2603 2609 2620 2759\n",
            " 2769 2936 2958 3090 3111 3138 3163 3183 3224 3350 3411 3772 3927 3932\n",
            " 4012 4016 4116 4339 4369 4465 4562 4578 4609 4855 4859 4923 5045 5070\n",
            " 5084 5154 5301 5311 5378 5464 5825 6045 6114 6215 6326 6772 6874 7139\n",
            " 7166 7222 7227 7231 7302 7339 7402 7455 7730 7836 7904 8499 8645 8690\n",
            " 8938 8981 9028 9254 9258 9595 9610]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "595 [   0   13   19   28   29   33   35   42   52   54   72   92  129  131\n",
            "  151  163  185  193  197  199  201  210  216  219  226  230  253  270\n",
            "  271  272  278  279  282  283  291  298  302  314  317  325  328  330\n",
            "  336  344  352  354  363  365  366  372  384  392  421  422  430  431\n",
            "  437  446  451  468  479  490  495  496  498  500  502  507  510  520\n",
            "  535  537  538  540  546  553  557  560  567  568  569  572  573  575\n",
            "  580  581  584  587  590  591  592  594  599  603  604  607  608  613\n",
            "  616  618  621  622  623  624  625  626  628  630  634  637  641  644\n",
            "  650  661  666  674  691  692  697  706  708  716  720  728  731  736\n",
            "  743  744  749  752  757  764  765  769  776  787  792  799  802  803\n",
            "  804  806  809  810  811  816  818  823  839  847  854  855  859  868\n",
            "  874  877  885  886  887  892  895  903  905  921  922  923  925  928\n",
            "  929  942  947  950  952  954  957  961  963  974  985  987  988  993\n",
            "  994 1005 1009 1017 1018 1027 1029 1031 1039 1040 1044 1047 1050 1055\n",
            " 1064 1071 1083 1085 1086 1088 1090 1103 1112 1115 1121 1123 1126 1156\n",
            " 1175 1178 1198 1207 1228 1230 1231 1253 1255 1256 1260 1271 1289 1296\n",
            " 1298 1304 1308 1316 1320 1351 1353 1360 1378 1399 1414 1423 1432 1438\n",
            " 1445 1463 1474 1475 1489 1506 1508 1513 1516 1519 1531 1534 1550 1553\n",
            " 1555 1563 1571 1577 1583 1597 1615 1647 1650 1671 1684 1686 1689 1695\n",
            " 1702 1723 1736 1747 1754 1769 1773 1789 1800 1807 1810 1811 1822 1828\n",
            " 1830 1834 1862 1867 1879 1884 1888 1896 1905 1907 1914 1917 1933 1940\n",
            " 1942 1943 1949 1950 1962 1979 1984 1992 2011 2014 2016 2029 2030 2036\n",
            " 2056 2061 2064 2071 2075 2076 2078 2105 2144 2145 2153 2157 2158 2172\n",
            " 2178 2185 2188 2190 2198 2209 2213 2243 2250 2255 2260 2266 2269 2272\n",
            " 2276 2302 2319 2326 2331 2353 2376 2398 2399 2425 2441 2446 2449 2451\n",
            " 2461 2466 2479 2485 2495 2497 2502 2509 2517 2525 2532 2533 2536 2550\n",
            " 2571 2603 2609 2620 2633 2641 2660 2679 2680 2701 2704 2723 2749 2759\n",
            " 2760 2765 2769 2782 2786 2792 2794 2842 2865 2869 2889 2897 2899 2908\n",
            " 2936 2939 2958 3009 3029 3033 3034 3043 3081 3090 3091 3111 3113 3114\n",
            " 3138 3139 3163 3171 3183 3224 3269 3306 3319 3334 3350 3356 3387 3400\n",
            " 3411 3428 3431 3448 3482 3494 3526 3591 3600 3618 3675 3733 3772 3791\n",
            " 3805 3830 3927 3932 3963 3968 4012 4016 4036 4060 4080 4116 4123 4166\n",
            " 4222 4336 4339 4369 4373 4465 4479 4562 4578 4592 4609 4669 4725 4763\n",
            " 4789 4851 4855 4859 4923 4981 5032 5045 5067 5070 5074 5084 5087 5154\n",
            " 5210 5213 5294 5301 5311 5331 5338 5371 5378 5404 5437 5442 5457 5464\n",
            " 5506 5587 5629 5648 5725 5726 5820 5825 5956 5960 6019 6045 6114 6134\n",
            " 6156 6165 6210 6215 6281 6326 6391 6439 6447 6449 6481 6559 6586 6603\n",
            " 6620 6642 6663 6749 6772 6806 6874 6918 6926 6945 6960 6962 6997 7058\n",
            " 7081 7097 7124 7128 7139 7166 7222 7227 7231 7249 7302 7317 7318 7320\n",
            " 7339 7385 7397 7402 7405 7410 7433 7455 7457 7486 7532 7682 7730 7836\n",
            " 7904 7991 8009 8037 8078 8186 8260 8307 8360 8446 8499 8524 8645 8648\n",
            " 8690 8919 8938 8981 9028 9192 9223 9254 9258 9282 9364 9465 9467 9526\n",
            " 9561 9595 9610 9671 9755 9813 9963]\n",
            "\n",
            "595 [0.5053911  0.5023211  0.52201146 0.5008016  0.50219554 0.506599\n",
            " 0.5057873  0.50578946 0.50611764 0.50479656 0.51033074 0.5035855\n",
            " 0.5003583  0.501757   0.5037642  0.50157    0.50098574 0.50731844\n",
            " 0.5051709  0.5038188  0.50012565 0.5095717  0.5053153  0.5063129\n",
            " 0.50174016 0.5035596  0.50009733 0.5019054  0.5014507  0.50794494\n",
            " 0.5028437  0.5028367  0.5010004  0.50825745 0.5027347  0.5081236\n",
            " 0.5003783  0.50153774 0.5039602  0.5046874  0.50167996 0.5041121\n",
            " 0.5019007  0.5031331  0.5049874  0.5056475  0.5023136  0.50339913\n",
            " 0.50162274 0.5004955  0.50249636 0.51151687 0.50740033 0.50752056\n",
            " 0.50209475 0.5033542  0.5037086  0.50259304 0.50195634 0.5026632\n",
            " 0.5036901  0.5034497  0.5059254  0.5069066  0.50827795 0.5045648\n",
            " 0.5019335  0.51012826 0.5039018  0.5049851  0.50775075 0.50507915\n",
            " 0.5025945  0.50288785 0.5044469  0.5029266  0.5086694  0.5014178\n",
            " 0.5094018  0.50527114 0.5017007  0.50080395 0.5011398  0.5076281\n",
            " 0.503105   0.50389934 0.5006988  0.5003809  0.500123   0.5016588\n",
            " 0.53885746 0.5091121  0.5016115  0.5011755  0.50579816 0.501253\n",
            " 0.50459516 0.5013137  0.5024149  0.5057929  0.5018788  0.5132565\n",
            " 0.5023817  0.5014735  0.5065278  0.50436765 0.50038683 0.50645995\n",
            " 0.5043953  0.50441724 0.5103754  0.50067675 0.50052834 0.50693744\n",
            " 0.5033964  0.5004503  0.50193936 0.5024571  0.5015403  0.50763965\n",
            " 0.5088636  0.5019083  0.50211716 0.50101185 0.50108826 0.50442874\n",
            " 0.5031113  0.50196654 0.50164866 0.5016538  0.5059423  0.5016376\n",
            " 0.5031575  0.5022226  0.5040454  0.50067496 0.5009757  0.5079549\n",
            " 0.5072881  0.50462544 0.50280166 0.5035726  0.5043313  0.5050077\n",
            " 0.500109   0.50235087 0.50019467 0.5365867  0.50922143 0.50181746\n",
            " 0.507621   0.5009384  0.5015431  0.5002851  0.50378555 0.5017731\n",
            " 0.5023961  0.50442976 0.5029555  0.50223243 0.51466405 0.5133867\n",
            " 0.5122643  0.5047615  0.5043256  0.5071186  0.51478326 0.5039612\n",
            " 0.5032702  0.50069875 0.50163776 0.52723074 0.51023746 0.50424355\n",
            " 0.5001381  0.500963   0.50622684 0.50714606 0.501376   0.50194\n",
            " 0.5008018  0.5046382  0.5018275  0.50126237 0.5050152  0.5033145\n",
            " 0.50180906 0.501125   0.5020505  0.5009337  0.5042214  0.50115937\n",
            " 0.5020086  0.52109915 0.5039353  0.5007635  0.5015     0.50239575\n",
            " 0.5001715  0.50667393 0.5018433  0.5038477  0.50713634 0.50061494\n",
            " 0.5029346  0.50634575 0.5007288  0.50172526 0.5056801  0.50415236\n",
            " 0.50141484 0.50276333 0.5036603  0.5019564  0.5020154  0.5075972\n",
            " 0.5052885  0.5000045  0.5022334  0.50191075 0.50155866 0.52606606\n",
            " 0.5044278  0.5038813  0.50065017 0.50529474 0.50369835 0.5000128\n",
            " 0.50482434 0.50073403 0.5019467  0.50075406 0.5057431  0.50041294\n",
            " 0.50344944 0.50322175 0.50321317 0.5061729  0.5043888  0.500806\n",
            " 0.5037742  0.50210273 0.5035093  0.5011894  0.5095188  0.5012756\n",
            " 0.5032455  0.50817    0.5009272  0.50158346 0.5030355  0.5011384\n",
            " 0.50346774 0.506999   0.50007224 0.501854   0.500193   0.50681233\n",
            " 0.5020376  0.5019031  0.50608295 0.50684786 0.50516474 0.50797594\n",
            " 0.5019436  0.50027776 0.5008015  0.5009919  0.5014634  0.5012981\n",
            " 0.50649357 0.5023531  0.5034332  0.50030994 0.50056165 0.507083\n",
            " 0.50288856 0.5005852  0.5000163  0.5000339  0.50059897 0.5000246\n",
            " 0.5013564  0.50010324 0.50030214 0.50206256 0.5008874  0.50110775\n",
            " 0.50231636 0.50175637 0.5012991  0.500364   0.5158739  0.5002991\n",
            " 0.50024694 0.50013596 0.5344132  0.5047878  0.5031546  0.50510865\n",
            " 0.5044023  0.5005707  0.5064791  0.50109166 0.5003894  0.50623196\n",
            " 0.5087027  0.50500447 0.5037411  0.50054485 0.5015814  0.50425106\n",
            " 0.50004166 0.5003641  0.5015039  0.50015414 0.5081236  0.5007046\n",
            " 0.5041809  0.50063413 0.50651926 0.5080685  0.50087    0.50517684\n",
            " 0.5000575  0.5060527  0.5015853  0.5051804  0.50578165 0.50450724\n",
            " 0.5001607  0.50253564 0.50486094 0.5067663  0.5034309  0.50214356\n",
            " 0.5023775  0.5006344  0.5000032  0.5003857  0.5006293  0.5002679\n",
            " 0.5003523  0.5023338  0.50050867 0.5011816  0.50134915 0.5003143\n",
            " 0.51048976 0.5034516  0.50062364 0.5061468  0.5161403  0.5057277\n",
            " 0.5002309  0.50401616 0.5004739  0.50195307 0.5042106  0.5013165\n",
            " 0.5096066  0.50114995 0.50889736 0.50412256 0.50132227 0.5044171\n",
            " 0.5078239  0.50500745 0.50358886 0.50085616 0.5006077  0.50319034\n",
            " 0.5024775  0.5015565  0.5008082  0.50159466 0.50249386 0.5069943\n",
            " 0.50171155 0.50077516 0.5054873  0.50276834 0.5000514  0.5008262\n",
            " 0.50114805 0.5000121  0.50320506 0.5019501  0.50072646 0.5001558\n",
            " 0.5001595  0.50133497 0.5047294  0.5021518  0.5063189  0.50376755\n",
            " 0.5036797  0.5009677  0.5022233  0.5036221  0.50044614 0.5045593\n",
            " 0.5016784  0.51983446 0.50110227 0.5037447  0.5047516  0.50183535\n",
            " 0.5041045  0.5023599  0.5044007  0.5055794  0.5012032  0.5028478\n",
            " 0.5031797  0.5033663  0.50751084 0.50079745 0.5000496  0.50167143\n",
            " 0.5073774  0.50148976 0.5006006  0.50174403 0.50171965 0.50137764\n",
            " 0.5029724  0.5029095  0.50080496 0.50032496 0.5010488  0.5018411\n",
            " 0.5092632  0.50099915 0.5011717  0.503668   0.5059994  0.5058466\n",
            " 0.50118166 0.5015532  0.50419223 0.50464326 0.5034505  0.50116944\n",
            " 0.50041956 0.50401884 0.50112414 0.50017923 0.50006247 0.50185996\n",
            " 0.50622916 0.5054979  0.5007915  0.51778823 0.501678   0.52038753\n",
            " 0.5047434  0.5006787  0.50804585 0.500584   0.5028906  0.5036948\n",
            " 0.50168663 0.5021578  0.5087566  0.5049288  0.505311   0.500721\n",
            " 0.50004756 0.5059916  0.50186807 0.50473076 0.5012369  0.5065474\n",
            " 0.5004561  0.50596344 0.5002767  0.50091153 0.5014338  0.5270517\n",
            " 0.5306532  0.5009462  0.5022109  0.5014032  0.5206844  0.5000952\n",
            " 0.5021879  0.5014367  0.50191754 0.5054623  0.50089073 0.5009995\n",
            " 0.50361246 0.50168794 0.5035898  0.501416   0.50042033 0.5048523\n",
            " 0.50159144 0.5000578  0.5010476  0.5075848  0.52861756 0.500846\n",
            " 0.5001912  0.5014474  0.503887   0.50634193 0.5005771  0.5112278\n",
            " 0.50221545 0.50284535 0.50049216 0.5006471  0.50082976 0.5036438\n",
            " 0.50320387 0.50193775 0.5011506  0.50350523 0.50227344 0.50161237\n",
            " 0.5049745  0.50269115 0.50950015 0.50073963 0.50050604 0.500634\n",
            " 0.5028069  0.502273   0.50355285 0.5006838  0.5030509  0.5006643\n",
            " 0.50021935 0.5021945  0.50849515 0.50536025 0.5048091  0.50723284\n",
            " 0.5062522  0.5000957  0.5053391  0.5002411  0.50007635 0.5009975\n",
            " 0.5052637  0.5000059  0.5000572  0.50824195 0.5010561  0.50016505\n",
            " 0.5011971  0.5272057  0.50064665 0.50101143 0.50202626 0.5000363\n",
            " 0.5043537  0.5161645  0.5060163  0.5017065  0.5034599  0.5010207\n",
            " 0.50300235 0.5016706  0.5025436  0.5023568  0.5030245  0.5007667\n",
            " 0.51434624 0.5023918  0.5043792  0.50264025 0.5256418  0.5003321\n",
            " 0.52230006 0.5262078  0.5047145  0.5009249  0.50040454 0.50623065\n",
            " 0.5125854  0.5028515  0.50081265 0.5023544  0.50242823 0.50035465\n",
            " 0.50045335 0.5134939  0.505521   0.5025821  0.5022677  0.50230557\n",
            " 0.5005038 ]\n",
            "\n",
            "Matched draws\n",
            "Count: 17, Index: (array([ 392,  422, 1228, 1475, 1519, 1789, 1884, 2030, 2145, 2517, 2889,\n",
            "       2908, 3224, 3400, 3927, 4116, 6439]),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "107279  507220 2020-02-08  ConsolationNo4      422\n",
            "107356  507520 2020-02-15      SpecialNo2      392\n",
            "\n",
            "\n",
            "All matched\n",
            "        DrawNo   DrawDate        PrizeType  LuckyNo\n",
            "107187  506820 2020-02-01   ConsolationNo4     1789\n",
            "107199  506820 2020-02-01       SpecialNo6     2030\n",
            "107206  506920 2020-02-02   ConsolationNo1     1519\n",
            "107243  507020 2020-02-04       SpecialNo4     1884\n",
            "107260  507120 2020-02-05   ConsolationNo8     2908\n",
            "107279  507220 2020-02-08   ConsolationNo4      422\n",
            "107289  507220 2020-02-08       SpecialNo4     4116\n",
            "107356  507520 2020-02-15       SpecialNo2      392\n",
            "107365  507620 2020-02-16       2ndPrizeNo     1228\n",
            "107385  507620 2020-02-16       SpecialNo8     3400\n",
            "107396  507720 2020-02-19   ConsolationNo6     2517\n",
            "107402  507720 2020-02-19       SpecialNo2     2145\n",
            "107414  507820 2020-02-22  ConsolationNo10     2889\n",
            "107456  508020 2020-02-26       1stPrizeNo     3927\n",
            "107466  508020 2020-02-26   ConsolationNo7     1475\n",
            "107471  508020 2020-02-26       SpecialNo2     3224\n",
            "107475  508020 2020-02-26       SpecialNo6     6439\n",
            "CPU times: user 37min 15s, sys: 560 ms, total: 37min 15s\n",
            "Wall time: 9min 28s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y22VT9u7xR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}