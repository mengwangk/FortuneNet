{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04_02_automated_machine_learning.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengwangk/dl-projects/blob/master/04_09_auto_ml_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4hyoPGdjpqa_"
      },
      "source": [
        "# Automated ML - Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SLxr2k_ue8yq",
        "colab": {}
      },
      "source": [
        "COLAB = True\n",
        "\n",
        "DATASET_NAME = '4D.zip'\n",
        "\n",
        "FEATURE_DATASET_PREFIX = 'feature_matrix_d2_v3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwYshXtLt7b7",
        "colab": {}
      },
      "source": [
        "#!pip install -U imblearn\n",
        "#!pip install -U xgboost\n",
        "# !pip install -U featuretools\n",
        "\n",
        "# https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28\n",
        "# https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
        "# https://machinelearningmastery.com/imbalanced-classification-model-to-detect-oil-spills/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oy5ww2zRfFGG",
        "outputId": "1fa144ef-d5d3-4984-af27-dfb19a7323a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "if COLAB:\n",
        "  !rm -rf dl-projects\n",
        "  !git clone https://github.com/mengwangk/dl-projects"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dl-projects'...\n",
            "remote: Enumerating objects: 118, done.\u001b[K\n",
            "remote: Counting objects: 100% (118/118), done.\u001b[K\n",
            "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
            "remote: Total 1920 (delta 73), reused 15 (delta 6), pack-reused 1802\u001b[K\n",
            "Receiving objects: 100% (1920/1920), 78.07 MiB | 13.03 MiB/s, done.\n",
            "Resolving deltas: 100% (1184/1184), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G2xin10SfozR",
        "colab": {}
      },
      "source": [
        "if COLAB:\n",
        "  !cp dl-projects/utils* .\n",
        "  !cp dl-projects/preprocess* .\n",
        "  !cp dl-projects/plot* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fC2-l3JBpqbE",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "# %reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TP7V_IzepqbK",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "import matplotlib\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import featuretools as ft\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, make_scorer, recall_score, roc_curve, mean_squared_error, accuracy_score, average_precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                     ClusterCentroids,\n",
        "                                     TomekLinks,\n",
        "                                     NeighbourhoodCleaningRule,\n",
        "                                     AllKNN,\n",
        "                                     NearMiss,\n",
        "                                     OneSidedSelection,\n",
        "                                     EditedNearestNeighbours)\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
        "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC, ADASYN \n",
        "import pylab as pl\n",
        "import xgboost as xgb\n",
        "from collections import Counter\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "# from skopt import BayesSearchCV\n",
        "# from skopt.space import Real, Categorical, Integer\n",
        "# from scikitplot.plotters import plot_precision_recall_curve\n",
        "\n",
        "from utils import feature_selection, plot_feature_importances\n",
        "from preprocess import *\n",
        "from plot import plot_correlation_matrix, plot_labeled_scatter\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "sns.set(style=\"ticks\")\n",
        "\n",
        "# The Answer to the Ultimate Question of Life, the Universe, and Everything.\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3bFT5CoxpqbP",
        "outputId": "2307f6cb-890e-4078-f2f1-181084249c93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%aimport"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modules to reload:\n",
            "all-except-skipped\n",
            "\n",
            "Modules to skip:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3E16jPVPpqbV"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U421BuhtfYS7",
        "outputId": "aedae685-5d4a-4ec4-b9aa-dcf75ebd80e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  GDRIVE_DATASET_FOLDER = Path('gdrive/My Drive/datasets/')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9IgnETKkpqbX",
        "outputId": "7a8a6d14-db5f-4b87-a4a5-6a439493a8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "if COLAB:\n",
        "  DATASET_PATH = GDRIVE_DATASET_FOLDER\n",
        "  ORIGIN_DATASET_PATH = Path('dl-projects/datasets')\n",
        "else:\n",
        "  DATASET_PATH = Path(\"../datasets\")\n",
        "  ORIGIN_DATASET_PATH = Path('datasets')\n",
        "\n",
        "DATASET = DATASET_PATH/f\"{FEATURE_DATASET_PREFIX}.ft\"\n",
        "ORIGIN_DATASET = ORIGIN_DATASET_PATH/DATASET_NAME\n",
        "\n",
        "if COLAB:\n",
        "  !ls -l gdrive/\"My Drive\"/datasets/ --block-size=M\n",
        "  !ls -l dl-projects/datasets --block-size=M"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4830M\n",
            "-rw------- 1 root root   17M Mar  1 05:47 feature_matrix_2020_feb.ft\n",
            "-rw------- 1 root root   17M Mar  1 05:47 feature_matrix_2020_feb_orig.pkl\n",
            "-rw------- 1 root root   17M Mar  1 05:39 feature_matrix_2020_jan.ft\n",
            "-rw------- 1 root root   17M Mar  1 05:39 feature_matrix_2020_jan_orig.pkl\n",
            "-rw------- 1 root root   17M Mar  1 05:56 feature_matrix_2020_mar.ft\n",
            "-rw------- 1 root root   17M Mar  1 05:56 feature_matrix_2020_mar_orig.pkl\n",
            "-rw------- 1 root root 2454M Jan 12 01:24 feature_matrix_d2_v2.ft\n",
            "-rw------- 1 root root 1585M Jan 12 23:39 feature_matrix_d2_v3.ft\n",
            "-rw------- 1 root root   17M Feb 21 13:07 feature_matrix_snapshot.ft\n",
            "-rw------- 1 root root   17M Feb 21 13:07 feature_matrix_snapshot_origin.pkl\n",
            "-rw------- 1 root root    1M Mar  1 05:49 labels.csv\n",
            "-rw------- 1 root root    5M Jan 30 04:33 orig_X_test.ft\n",
            "-rw------- 1 root root  415M Jan 30 04:33 orig_X_train.ft\n",
            "-rw------- 1 root root    1M Jan 30 04:33 orig_y_test.ft\n",
            "-rw------- 1 root root    7M Jan 30 04:33 orig_y_train.ft\n",
            "-rw------- 1 root root    3M Feb 29 22:38 test_X_test.ft\n",
            "-rw------- 1 root root  225M Feb 29 22:38 test_X_train.ft\n",
            "-rw------- 1 root root    1M Feb 29 22:38 test_y_test.ft\n",
            "-rw------- 1 root root    8M Feb 29 22:38 test_y_train.ft\n",
            "total 25M\n",
            "-rw-r--r-- 1 root root  1M Mar  1 06:25 4D.zip\n",
            "-rw-r--r-- 1 root root 25M Mar  1 06:25 labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "urQTD6DQNutw",
        "colab": {}
      },
      "source": [
        "# Read the data\n",
        "data = pd.read_feather(DATASET)\n",
        "origin_data = format_tabular(ORIGIN_DATASET)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zov05QHZxxiS",
        "colab_type": "text"
      },
      "source": [
        "## Add new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foPB8T1vx2tp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0d11a2f1-2f2e-4d83-9065-38d23c955278"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(959893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43sc1Eaux25j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jan_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_jan.ft\")\n",
        "feb_2020= pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_feb.ft\")\n",
        "mar_2020= pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_mar.ft\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vISxEbsyQG1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "112f57e0-cd6a-46f7-a4c3-eebe616a9914"
      },
      "source": [
        "new_data = data.append(jan_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(feb_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(mar_2020[data.columns],ignore_index=True)\n",
        "new_data.shape "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVVMXCj-zyaW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3ed94b2-f88e-4185-838e-cec2b0dce77a"
      },
      "source": [
        "data = new_data\n",
        "data.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vOYlp-8Br61r"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kHiN1VVlG9Kh"
      },
      "source": [
        "### View data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnQXyVqng5Cm",
        "colab": {}
      },
      "source": [
        "# Feature matrix\n",
        "feature_matrix = data.drop(columns=['NumberId', 'month', 'year'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "apMYVNz9HK9e",
        "outputId": "f539bbd5-9a0d-41ff-dbdd-80a3d789ae0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Sort data\n",
        "feature_matrix.sort_values(by=['time', 'MAX(Results.LuckyNo)'], inplace=True)\n",
        "feature_matrix.info()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 989893 entries, 7020 to 986394\n",
            "Columns: 214 entries, time to LAST(Results.PrizeType)_Prize\n",
            "dtypes: datetime64[ns](1), float64(155), int64(56), uint8(2)\n",
            "memory usage: 1.6 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZKTbWRFJNUq",
        "outputId": "675acbb0-59fb-44a7-bc58-f3ea0df633e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "print('Distribution')\n",
        "print(feature_matrix['Label'].value_counts())\n",
        "print()\n",
        "print('Positive: ' + str(feature_matrix['Label'].value_counts()[0]) + ' which is ', round(feature_matrix['Label'].value_counts()[0]/len(feature_matrix) * 100,2), '% of the dataset')\n",
        "print('Negative: ' + str(feature_matrix['Label'].value_counts()[1]) + ' which is ', round(feature_matrix['Label'].value_counts()[1]/len(feature_matrix) * 100,2), '% of the dataset')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution\n",
            "0    957191\n",
            "1     32702\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Positive: 957191 which is  96.7 % of the dataset\n",
            "Negative: 32702 which is  3.3 % of the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "plplpAQ6JrKb",
        "outputId": "8477d67a-0326-4a5d-8400-915d85bce6fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "feature_matrix.isna().sum().sort_values(ascending=False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))               7657\n",
              "CUM_MEAN(TREND(Results.LuckyNo, DrawDate))                7657\n",
              "TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)    7657\n",
              "TREND(Results.CUM_MEAN(TotalStrike), DrawDate)            7657\n",
              "TREND(Results.CUM_SUM(LuckyNo), DrawDate)                 7657\n",
              "                                                          ... \n",
              "CUM_SUM(MIN(Results.DrawNo))                                 0\n",
              "NUM_UNIQUE(Results.DAY(DrawDate))                            0\n",
              "NUM_UNIQUE(Results.MONTH(DrawDate))                          0\n",
              "SUM(Results.PERCENTILE(LuckyNo))                             0\n",
              "time                                                         0\n",
              "Length: 214, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF_zCRksL1Ls"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1aLGsXSOa9K",
        "colab": {}
      },
      "source": [
        "# Fill all NaN with 0\n",
        "feature_matrix = feature_matrix.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5A8LZ805MqjP",
        "outputId": "a78c92c9-a138-40ee-f17c-b14f476a238b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "feature_matrix.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 214)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rPFOkiGjhuKj",
        "outputId": "4ee02aa1-8481-47a2-8ee8-94ef7d69708c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "feature_matrix_selection = feature_selection(feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label']))\n",
        "# feature_matrix_selection = feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label'])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape:  (989893, 211)\n",
            "0 missing columns with threshold: 90.\n",
            "41 zero variance columns.\n",
            "108 collinear columns removed with threshold: 0.95.\n",
            "Total columns removed:  149\n",
            "Shape after feature selection: (989893, 62).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vT2K0WeJhugH",
        "outputId": "1c76b3d1-6f7c-4a2f-d3e8-fc97b25e870c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "feature_matrix_selection.shape, feature_matrix_selection.columns"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((989893, 62),\n",
              " Index(['STD(Results.DrawNo)', 'MAX(Results.DrawNo)', 'MAX(Results.LuckyNo)',\n",
              "        'MIN(Results.DrawNo)', 'MEAN(Results.DrawNo)', 'SKEW(Results.DrawNo)',\n",
              "        'AVG_TIME_BETWEEN(Results.DrawDate)', 'COUNT(Results)',\n",
              "        'SUM(Results.DrawNo)', 'SUM(Results.LuckyNo)',\n",
              "        'TREND(Results.DrawNo, DrawDate)', 'MONTH(first_Results_time)',\n",
              "        'DAY(first_Results_time)', 'TIME_SINCE(first_Results_time)',\n",
              "        'TIME_SINCE_PREVIOUS(first_Results_time)',\n",
              "        'STD(Results.PERCENTILE(DrawNo))',\n",
              "        'STD(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'STD(Results.CUM_SUM(DrawNo))', 'STD(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MAX(Results.PERCENTILE(DrawNo))',\n",
              "        'MAX(Results.PERCENTILE(TotalStrike))',\n",
              "        'MAX(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MIN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MIN(Results.CUM_SUM(DrawNo))', 'MIN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MODE(Results.MONTH(DrawDate))', 'MODE(Results.DAY(DrawDate))',\n",
              "        'MEAN(Results.TIME_SINCE(DrawDate))',\n",
              "        'MEAN(Results.PERCENTILE(DrawNo))',\n",
              "        'MEAN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MEAN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'SKEW(Results.CUM_SUM(DrawNo))', 'SKEW(Results.CUM_MEAN(LuckyNo))',\n",
              "        'LAST(Results.DAY(DrawDate))',\n",
              "        'LAST(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'LAST(Results.MONTH(DrawDate))', 'LAST(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SUM(Results.TIME_SINCE(DrawDate))',\n",
              "        'SUM(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'TREND(Results.CUM_MEAN(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(DrawNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(TotalStrike), DrawDate)',\n",
              "        'TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)',\n",
              "        'TREND(Results.CUM_SUM(DrawNo), DrawDate)',\n",
              "        'NUM_UNIQUE(Results.MONTH(DrawDate))',\n",
              "        'NUM_UNIQUE(Results.DAY(DrawDate))', 'CUM_SUM(MIN(Results.DrawNo))',\n",
              "        'CUM_SUM(SKEW(Results.DrawNo))',\n",
              "        'CUM_MEAN(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'CUM_MEAN(SUM(Results.LuckyNo))', 'CUM_MEAN(SKEW(Results.DrawNo))',\n",
              "        'PERCENTILE(STD(Results.LuckyNo))', 'PERCENTILE(LAST(Results.DrawNo))',\n",
              "        'PERCENTILE(MAX(Results.TotalStrike))',\n",
              "        'PERCENTILE(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'PERCENTILE(COUNT(Results))', 'PERCENTILE(STD(Results.DrawNo))',\n",
              "        'PERCENTILE(SKEW(Results.DrawNo))', 'PERCENTILE(SUM(Results.DrawNo))',\n",
              "        'PERCENTILE(TREND(Results.DrawNo, DrawDate))'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yZUhYrWFiRod",
        "colab": {}
      },
      "source": [
        "feature_matrix_selection['time'] = feature_matrix['time']\n",
        "feature_matrix_selection['TotalStrike'] = feature_matrix['TotalStrike']\n",
        "feature_matrix_selection['Label'] = feature_matrix['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hugygOqSiR6K"
      },
      "source": [
        "### Feature Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "loagcqTEKOkO",
        "colab": {}
      },
      "source": [
        "#feature_matrix.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7Ha8Zlkhuoe",
        "colab": {}
      },
      "source": [
        "# Check with feature selection\n",
        "#corrs = feature_matrix_selection.corr().sort_values('Label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EWRODfAdPk6j",
        "colab": {}
      },
      "source": [
        "#corrs['Label'].head(60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "waeD1ED_kqDB"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9yrJyIVLh5So",
        "colab": {}
      },
      "source": [
        "def recall_optim(y_true, y_pred):\n",
        "    \"\"\"Make a scoring function that improves specificity while identifying all strikes\n",
        "    \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Recall will be worth a greater value than specificity\n",
        "    rec = recall_score(y_true, y_pred) * 0.8 \n",
        "    spe = conf_matrix[0,0]/conf_matrix[0,:].sum() * 0.2 \n",
        "    \n",
        "    # Imperfect recalls will lose a penalty\n",
        "    # This means the best results will have perfect recalls and compete for specificity\n",
        "    if rec < 0.8:\n",
        "        rec -= 0.2\n",
        "    return rec + spe \n",
        "\n",
        "\n",
        "# Make a scoring callable from recall_score\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Create a scoring callable based on the scoring function\n",
        "optimize = make_scorer(recall_optim)\n",
        "\n",
        "# Geometric mean scorer\n",
        "geo_mean_scorer = make_scorer(geometric_mean_score)\n",
        "\n",
        "# DataFrame to store classifier performance\n",
        "performance = pd.DataFrame(columns=['Train_Recall','Test_Recall','Test_Specificity', 'Optimize'])\n",
        "\n",
        "def to_labels(pos_probs, threshold):\n",
        "    \"\"\"Apply threshold to positive probabilities to create labels.\n",
        "    \"\"\"\n",
        "    return (pos_probs >= threshold).astype('int')\n",
        " \n",
        "\n",
        "def score_optimization(dt, feature_matrix, clf, params, X_train, y_train, X_test, y_test, skip_grid_search_cv=False, optimized_scorer=False):\n",
        "    \"\"\"Find the optimized classifier.\n",
        "    \"\"\"\n",
        "    if not skip_grid_search_cv:\n",
        "      print(\"\\nFinding the optimized classifier...\")\n",
        "\n",
        "      # Load GridSearchCV\n",
        "      # search = GridSearchCV(\n",
        "      search = RandomizedSearchCV(\n",
        "            estimator=clf,\n",
        "            #param_grid=params,\n",
        "            param_distributions=params,\n",
        "            n_jobs=4,\n",
        "            scoring=optimize  # Use custom scorer\n",
        "      )\n",
        "\n",
        "      # Train search object\n",
        "      search.fit(X_train, y_train)\n",
        "\n",
        "      # Heading\n",
        "      print('\\n','-'*40,'\\n',clf.__class__.__name__,'\\n','-'*40)\n",
        "\n",
        "      # Extract best estimator\n",
        "      best = search.best_estimator_\n",
        "      print('Best parameters: \\n\\n',search.best_params_,'\\n')\n",
        "    \n",
        "    else:\n",
        "      print(\"\\nUse the passed in classifier...\\n\")\n",
        "      best = clf\n",
        "\n",
        "    # Cross-validate on the train data\n",
        "    if not skip_grid_search_cv: \n",
        "      print(\"TRAIN GROUP\")\n",
        "      #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
        "      cv = 3\n",
        "      if not optimized_scorer:\n",
        "        print('\\nUse default scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  scoring=recall,\n",
        "                                  cv=cv)\n",
        "      else:\n",
        "        print('\\nUse optimized scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  #scoring=optimize,\n",
        "                                  scoring='roc_auc',\n",
        "                                  #scoring=geo_mean_scorer,\n",
        "                                  cv=cv)\n",
        "\n",
        "      print(\"\\nCross-validation recall scores:\",train_cv)\n",
        "      print(\"Mean recall score:\",train_cv.mean())\n",
        "      print('Mean G-Mean: %.3f (%.3f)' % (np.mean(train_cv), np.std(train_cv)))\n",
        "    else:\n",
        "      train_cv = np.zeros(3)\n",
        "\n",
        "    # Now predict on the test group\n",
        "    print(\"\\nTEST GROUP\")\n",
        "    y_pred = best.fit(X_train, y_train).predict(X_test)\n",
        "    # y_pred = best.fit(X_train, y_train,\n",
        "    #                   eval_set=[(X_test, y_test)],\n",
        "    #                   eval_metric='auc',\n",
        "    #                   early_stopping_rounds=10,\n",
        "    #                   verbose=True\n",
        "    #                   ).predict(X_test)\n",
        "\n",
        "    # keep probabilities for the positive outcome only\n",
        "    probas = best.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # define thresholds\n",
        "    thresholds = np.arange(0, 1, 0.001)\n",
        "\n",
        "    # evaluate each threshold\n",
        "    scores = [f1_score(y_test, to_labels(probas, t)) for t in thresholds]\n",
        "\n",
        "    # get best threshold\n",
        "    ix = np.argmax(scores)\n",
        "    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))\n",
        "\n",
        "    # print recall\n",
        "    print(\"\\nRecall:\",recall_score(y_test,y_pred))\n",
        "\n",
        "    # Get imbalanced classification report\n",
        "    print(classification_report_imbalanced(y_test, y_pred))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test,y_pred)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=plt.cm.copper)\n",
        "    plt.show()\n",
        "\n",
        "    # Store results\n",
        "    performance.loc[clf.__class__.__name__+'_optimize',:] = [\n",
        "        train_cv.mean(),\n",
        "        recall_score(y_test,y_pred),\n",
        "        conf_matrix[0,0]/conf_matrix[0,:].sum(),\n",
        "        recall_optim(y_test,y_pred)\n",
        "    ]\n",
        "    # Look at the parameters for the top best scores\n",
        "    if not skip_grid_search_cv:\n",
        "      display(pd.DataFrame(search.cv_results_).iloc[:,4:].sort_values(by='rank_test_score').head())\n",
        "    display(performance)\n",
        "\n",
        "    # Additionl info\n",
        "    print('\\n\\nAdditional Info')\n",
        "    print('='*40)\n",
        "    positive = np.where((y_pred==1))\n",
        "    print(f'Total predicted to be positive: {len(positive[0])} \\n')\n",
        "\n",
        "    pred = np.where((y_pred==1))\n",
        "    all_preds = pred[0]\n",
        "\n",
        "    # Total predicted matches\n",
        "    print('First 23 matches')\n",
        "    print(23, all_preds[0:23])\n",
        "    print(f'\\n{probas[all_preds[0:23]]}\\n') \n",
        "\n",
        "    print(\"\\nTop 23 Probable Matches\")\n",
        "    #print('probas', probas)\n",
        "    topN = np.argpartition(probas, -23)[-23:]\n",
        "    print(f'\\n{topN}\\n')          # Top N most high probability numbers\n",
        "    print(f'\\n{probas[topN]}\\n')  # Top N probability\n",
        "\n",
        "    # Check for 2 to 3 digits range \n",
        "    print('\\n2 To 3 Digits\\n')\n",
        "    idx_range = np.where((all_preds < 1000) & (all_preds >= 10))\n",
        "    #print(idx_range)\n",
        "    range_numbers = all_preds[idx_range]\n",
        "    print(len(range_numbers), range_numbers)\n",
        "    print(f'\\n{probas[range_numbers]}\\n') \n",
        "\n",
        "\n",
        "    # 2 to 3 Digits > Average Probas\n",
        "    print('\\n2 To 3 Digits Average Proba\\n')\n",
        "    avg_proba = np.average(probas[range_numbers])\n",
        "    print(f'Average proba {avg_proba}\\n')\n",
        "    idx_avg_proba = np.where(probas > avg_proba) \n",
        "    print(len(idx_avg_proba[0]), idx_avg_proba[0])\n",
        "\n",
        "    # 2 to 3 Digits > All Average Probas\n",
        "    print('\\n\\nAll Average Proba\\n')\n",
        "    all_avg_proba = np.average(probas[all_preds])\n",
        "    print(f'All average probas {all_avg_proba}\\n')\n",
        "    idx_all_avg_proba = np.where(probas > all_avg_proba) \n",
        "    print(len(idx_all_avg_proba[0]), idx_all_avg_proba[0])\n",
        "\n",
        "\n",
        "    # All predicted matches\n",
        "    print('\\n\\nAll Predictions\\n')\n",
        "    print(len(all_preds), all_preds)\n",
        "    print()\n",
        "    print(len(probas[all_preds]), probas[all_preds])\n",
        "    \n",
        "    #print('Debug')\n",
        "    #print(pred)\n",
        "    \n",
        "    if len(positive[0]) > 0:\n",
        "    \n",
        "      # Matching draws\n",
        "      print('\\nMatched draws')\n",
        "      md = np.where((y_pred==1) & (y_test==1))\n",
        "      print(f\"Count: {len(md[0])}, Index: {md}\")\n",
        "      month_data = feature_matrix.loc[feature_matrix['time'] == dt]\n",
        "      numbers = month_data.iloc[md[0]][['MAX(Results.LuckyNo)']]\n",
        "\n",
        "      print('\\n\\nTop 23 Possibility')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(topN))].head(23))  \n",
        "      \n",
        "      print('\\n\\nFirst 23 Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(pred[0][0:23]))].head(23))    \n",
        "             \n",
        "\n",
        "      print('\\n\\n2 To 3 Digits Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(range_numbers))].head(23))    \n",
        "     \n",
        "\n",
        "      print('\\n\\nAll matched')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(numbers['MAX(Results.LuckyNo)']))].head(100))    \n",
        "                                                  \n",
        "    else:\n",
        "      print('No luck this month')  \n",
        "\n",
        "    if len(range_numbers) >= 50:\n",
        "      return False\n",
        "\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrL8gYwjc-hd",
        "colab": {}
      },
      "source": [
        "def remove_outliers(balanced, threshold=0.001, remove=True):\n",
        "    \"\"\"Removing Outliers from high-correlation features.\n",
        "    \"\"\"\n",
        "\n",
        "    if not remove:\n",
        "      return balanced\n",
        "\n",
        "    bal_corr = balanced.corr()\n",
        "    no_outliers=pd.DataFrame(balanced.copy())\n",
        "\n",
        "    cols = bal_corr.Label.index[:-1]\n",
        "\n",
        "    # For each feature correlated with Class...\n",
        "    for col in cols:\n",
        "        # If absolute correlation value is more than X percent...\n",
        "        correlation = bal_corr.loc['Label',col]\n",
        "\n",
        "        if np.absolute(correlation) > threshold:\n",
        "          # Separate the classes of the high-correlation column\n",
        "          nonstrikes = no_outliers.loc[no_outliers.Label==0,col]\n",
        "          strikes = no_outliers.loc[no_outliers.Label==1,col]\n",
        "\n",
        "          # Identify the 25th and 75th quartiles\n",
        "          all_values = no_outliers.loc[:,col]\n",
        "          q25, q75 = np.percentile(all_values, 25), np.percentile(all_values, 75)\n",
        "          # Get the inter quartile range\n",
        "          iqr = q75 - q25\n",
        "          # Smaller cutoffs will remove more outliers\n",
        "          cutoff = iqr * 7\n",
        "          # Set the bounds of the desired portion to keep\n",
        "          lower, upper = q25 - cutoff, q75 + cutoff\n",
        "          \n",
        "          # If positively correlated...\n",
        "          # Drop nonstrikes above upper bound, and strikes below lower bound\n",
        "          if correlation > 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes>upper].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes<lower].index,inplace=True)\n",
        "          \n",
        "          # If negatively correlated...\n",
        "          # Drop non strikes below lower bound, and strikes above upper bound\n",
        "          elif correlation < 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes<lower].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes>upper].index,inplace=True)\n",
        "        \n",
        "    print('\\nData shape before removing outliers:', balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in previous data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "    print('-'*40)\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after removing outliers:', no_outliers.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(no_outliers.Label.value_counts())\n",
        "\n",
        "    # no_outliers.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distributions with Less Outliers', fontsize=17)\n",
        "    # plt.show()\n",
        "    \n",
        "    no_outliers.reset_index(drop=True, inplace=True)\n",
        "    return no_outliers\n",
        "\n",
        "\n",
        "def filter_features(no_outliers, threshold=0.001):\n",
        "    \"\"\"Feature selection.\n",
        "    \"\"\"\n",
        "    feat_sel = pd.DataFrame(no_outliers.copy())\n",
        "\n",
        "    # Make a dataframe with the label-correlations before removing outliers\n",
        "    # corr_change = pd.DataFrame()\n",
        "    # corr_change['correlation']= bal_corr.Label\n",
        "    # corr_change['origin']= 'w/outliers'\n",
        "\n",
        "    # Make a dataframe with label-correlations after removing outliers \n",
        "    # corr_other = pd.DataFrame()\n",
        "    # corr_other['correlation']= feat_sel.corr().Label\n",
        "    # corr_other['origin']= 'no_outliers'\n",
        "\n",
        "    # Join them\n",
        "    # corr_change = corr_change.append(corr_other)\n",
        "\n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.xticks(rotation=90)\n",
        "\n",
        "    # Plot them\n",
        "    # sns.set_style('darkgrid')\n",
        "    # plt.title('Label correlation per feature. With vs without outliers', fontsize=17)\n",
        "    # sns.barplot(data=corr_change,x=corr_change.index,y='correlation',hue='origin')\n",
        "    # plt.show()\n",
        "\n",
        "    # Feature Selection based on correlation with label\n",
        "\n",
        "    print('\\nData shape before feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes before feature selection:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # Correlation matrix after removing outliers\n",
        "    new_corr = feat_sel.corr()\n",
        "\n",
        "    for col in new_corr.Label.index[:-1]:\n",
        "        # Pick desired cutoff for dropping features. In absolute-value terms.\n",
        "        if np.absolute(new_corr.loc['Label',col]) < threshold:\n",
        "            # Drop the feature if correlation is below cutoff\n",
        "            feat_sel.drop(columns=col,inplace=True)\n",
        "\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in new data:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "\n",
        "    return feat_sel\n",
        "\n",
        "    # feat_sel.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distribution of Features Selected', fontsize=17)\n",
        "    # plt.show()\n",
        "\n",
        "def under_sampler(data, sample_size=20000, sampling=False):\n",
        "    # Undersample model for efficiency and balance classes.\n",
        "\n",
        "    X_train = data.drop('Label',1)\n",
        "    y_train = data.Label\n",
        "\n",
        "    if not sampling:\n",
        "      return X_train, y_train\n",
        "\n",
        "    # After feature-selection, X_test needs to include only the same features as X_train\n",
        "    # cols = X_train.columns\n",
        "    # X_test = X_test[cols]\n",
        "\n",
        "    # Undersample and balance classes\n",
        "    X_train, y_train = RandomUnderSampler(sampling_strategy={1:sample_size,0:sample_size}).fit_resample(X_train,y_train)\n",
        "\n",
        "    print('\\nX_train shape after reduction:', X_train.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in y_train:')\n",
        "    print(np.unique(y_train, return_counts=True))\n",
        "\n",
        "    return X_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pl5ZoepSNPf4",
        "colab": {}
      },
      "source": [
        "def gen_train_test_set(dt, feature_matrix, file_prefix='data'): \n",
        "    \n",
        "    # Subset labels\n",
        "    test_labels = feature_matrix.loc[feature_matrix['time'] == dt, 'Label']\n",
        "    train_labels = feature_matrix.loc[feature_matrix['time'] < dt, 'Label']\n",
        "\n",
        "    # Features\n",
        "    X_train = feature_matrix[feature_matrix['time'] < dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    X_test = feature_matrix[feature_matrix['time'] == dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    feature_names = list(X_train.columns)\n",
        "    \n",
        "    # Labels\n",
        "    y_train = np.array(train_labels).reshape((-1, ))\n",
        "    y_test = np.array(test_labels).reshape((-1, ))\n",
        "    \n",
        "    print('Training on {} observations.'.format(len(X_train)))\n",
        "    print('Testing on {} observations.\\n'.format(len(X_test)))\n",
        "\n",
        "    # Join the train data\n",
        "    train = X_train.join(train_labels)\n",
        "\n",
        "    print('Data shape before balancing:', train.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in previous data:')\n",
        "    print(train.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # sklearn pipeline\n",
        "    pipeline = make_pipeline(\n",
        "        SimpleImputer(strategy = 'constant', fill_value=0),\n",
        "        StandardScaler())\n",
        "    \n",
        "    X_train = pipeline.fit_transform(X_train)\n",
        "    X_test = pipeline.transform(X_test)\n",
        "\n",
        "    # imblearn pipeline\n",
        "    imb_pipeline = make_pipeline_imb(\n",
        "          # NearMiss(version=3, n_neighbors_ver3=3, n_jobs=4)\n",
        "          # SMOTE(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # ADASYN(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # OneSidedSelection(n_neighbors=1, n_seeds_S=200, random_state=42, n_jobs=4)\n",
        "          SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "    )\n",
        "     \n",
        "    # Balance the data\n",
        "    to_balanced = False\n",
        "    if to_balanced:\n",
        "      print('\\nBalancing data')\n",
        "      X_bal, y_bal = imb_pipeline.fit_resample(X_train, y_train)\n",
        "      X_bal = pd.DataFrame(X_bal,columns=feature_names)\n",
        "      y_bal = pd.DataFrame(y_bal,columns=['Label'])\n",
        "    else:\n",
        "      print('\\nNO balancing')\n",
        "      X_bal = pd.DataFrame(X_train,columns=feature_names)\n",
        "      y_bal = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    balanced = X_bal.join(y_bal)\n",
        "\n",
        "    # print('-'*40)\n",
        "    print('Data shape after balancing:',balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "\n",
        "    # Remove high correlation outliers\n",
        "    no_outliers = remove_outliers(balanced, remove=False)\n",
        "   \n",
        "    # Remove features with low correlation\n",
        "    remove_features = True\n",
        "    if remove_features:\n",
        "      print('\\nFiltering features')\n",
        "      features_selected = filter_features(no_outliers)\n",
        "    else:\n",
        "      print('\\nNO filtering')\n",
        "      features_selected = no_outliers \n",
        "\n",
        "    columns_selected = features_selected.drop('Label',1).columns\n",
        "\n",
        "    # Under sampling\n",
        "    X_train, y_train = under_sampler(features_selected, sampling=False) \n",
        "    X_train = pd.DataFrame(X_train,columns=columns_selected)\n",
        "    y_train = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    # For X_test, now only use the selected features\n",
        "    X_test = pd.DataFrame(X_test,columns=feature_names)\n",
        "    X_test = X_test[columns_selected]\n",
        "    y_test = pd.DataFrame(y_test,columns=['Label'])\n",
        "\n",
        "    #print(X_train.describe())\n",
        "    #return\n",
        "\n",
        "    # Save data\n",
        "    # print(X_train.head(10))\n",
        "    # print(y_train.head(10)) \n",
        "\n",
        "    # print(X_test.head(10))\n",
        "    # print(y_test.head(10)) \n",
        "    X_train.to_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "    y_train.to_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "   \n",
        "    X_test.to_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "    y_test.to_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "\n",
        "    gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PcKlL67TP9UM",
        "colab": {}
      },
      "source": [
        "def model(dt, feature_matrix, file_prefix='data', csv=False, class_weight=1.0):\n",
        "    \"\"\"Predict for a particular month.\n",
        "\n",
        "    - https://www.kaggle.com/miguelniblock/optimizing-imbalanced-classification-100-recall\n",
        "    - https://www.kaggle.com/saxinou/imbalanced-data-xgboost-tunning\n",
        "    - https://www.kaggle.com/andreanuzzo/balance-the-imbalanced-rf-and-xgboost-with-smote\n",
        "    - https://github.com/mengwangk/FraudDetection/blob/master/05_Sampling_techniques_for_extremely_imbalanced_data.ipynb\n",
        "    - https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
        "    - https://github.com/coding-maniacs/over-under-sampling/blob/master/src/main.py\n",
        "    - https://github.com/scikit-learn-contrib/imbalanced-learn/issues/552#issuecomment-466348310\n",
        "    - https://stackoverflow.com/questions/52499788/smotetomek-how-to-set-ratio-as-dictionary-for-fixed-balance\n",
        "    - https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.OneSidedSelection.html#imblearn.under_sampling.OneSidedSelection\n",
        "    - https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
        "    - https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/\n",
        "    - https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "    - https://stats.stackexchange.com/questions/243207/what-is-the-proper-usage-of-scale-pos-weight-in-xgboost-for-imbalanced-datasets\n",
        "    - https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py\n",
        "    - https://machinelearningmastery.com/cost-sensitive-logistic-regression/\n",
        "    \n",
        "    - https://datascience.stackexchange.com/questions/28285/what-is-the-best-way-to-deal-with-imbalanced-data-for-xgboost/28292\n",
        "    - https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
        "    - https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Read data\n",
        "    if not csv:\n",
        "      X_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "      y_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "    \n",
        "      X_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "      y_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "    else:\n",
        "      X_train = pd.read_csv(DATASET_PATH/f\"{file_prefix}_X_train.csv\", header=0, sep=',', quotechar='\"')\n",
        "      y_train = pd.read_csv(DATASET_PATH/f\"{file_prefix}_y_train.csv\", header=0, sep=',', quotechar='\"')\n",
        "    \n",
        "      X_test = pd.read_csv(DATASET_PATH/f\"{file_prefix}_X_test.csv\", header=0, sep=',', quotechar='\"')\n",
        "      y_test = pd.read_csv(DATASET_PATH/f\"{file_prefix}_y_test.csv\", header=0, sep=',', quotechar='\"')\n",
        "\n",
        "\n",
        "    print(f'\\n-----------{dt}-----------------\\n')\n",
        "\n",
        "    # Reshape\n",
        "    y_train = np.array(y_train).reshape((-1, ))\n",
        "    y_test = np.array(y_test).reshape((-1, ))\n",
        "    \n",
        "    print('Data shape')\n",
        "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "    # print(X_train.describe())\n",
        "    # return\n",
        "\n",
        "    # Calculate hit ratio for xgboost classifier\n",
        "    print(\"\\nCalculating scale pos weight\")\n",
        "    counter = Counter(y_train)\n",
        "    print(Counter(y_train))\n",
        "    #scale_pos_weight = float(counter[0] / counter[1])\n",
        "    scale_pos_weight = (float(counter[0] / counter[1])) * class_weight\n",
        "    print(f\"\\nscale_pos_weight - {scale_pos_weight}\\n\")\n",
        "    \n",
        "     # Modeling\n",
        "#     clf = xgb.XGBClassifier(\n",
        "#               n_jobs=4, \n",
        "#               random_state=42,\n",
        "#               #learning_rate=0.1,\n",
        "#               #n_estimators=500,\n",
        "#               #max_depth=6, \n",
        "#               #min_child_weight=3, \n",
        "#               #gamma=0,\n",
        "#               #subsample=0.8,\n",
        "#               #colsample_bytree=0.8,\n",
        "#               objective='binary:logistic', \n",
        "#               scale_pos_weight=scale_pos_weight,\n",
        "#               ##eval_metric=\"auc\",\n",
        "#               ##max_delta_step=1,\n",
        "#               seed=27)\n",
        "#     clf = xgb.XGBClassifier(n_jobs=4, \n",
        "#                             random_state=42,\n",
        "#                             objective='binary:logistic', \n",
        "#                             #scale_pos_weight=28)\n",
        "#                             scale_pos_weight=scale_pos_weight)\n",
        "    \n",
        "    clf = xgb.XGBClassifier(\n",
        "                    n_jobs=4, \n",
        "                    random_state=42,\n",
        "                    objective='binary:logistic',\n",
        "                    subsample=0.55, \n",
        "                    n_estimators=300,\n",
        "                    #n_estimators=500,\n",
        "                    min_child_weight=1,\n",
        "                    max_depth=3, \n",
        "                    learning_rate=0.007,\n",
        "                    gamma=0.1, \n",
        "                    colsample_bytree=0.95,\n",
        "                    tree_method='hist',\n",
        "                    booster='dart',\n",
        "                    scale_pos_weight=scale_pos_weight\n",
        "                    )\n",
        "\n",
        "    clf_params = clf.get_params()\n",
        "    print(clf_params)\n",
        "\n",
        "    # Set parameters\n",
        "    #clf_params['max_depth'] = 10\n",
        "    #clf.set_params(clf_params)\n",
        "\n",
        "    # Parameters to compare\n",
        "    weights = [i for i in range(1,36,1)]\n",
        "    weights.append(scale_pos_weight)\n",
        "    learn_params = {\n",
        "        'n_estimators': [100, 300, 500, 800, 1000], \n",
        "        'max_depth': range(3,10,2),\n",
        "        'min_child_weight': range(1,6,2),\n",
        "        #'gamma':[i/10.0 for i in range(0,5)],\n",
        "        'subsample':[i/100.0 for i in range(55,70,5)],\n",
        "        'colsample_bytree':[i/100.0 for i in range(85,100,5)],\n",
        "        #'learning_rate':[i/1000.0 for i in range(5,20,2)],\n",
        "        'scale_pos_weight': weights\n",
        "    }\n",
        "    print(f'Parameter distribution: {learn_params}')\n",
        "    \n",
        "    # Test and validate\n",
        "    ret_val = score_optimization(dt,\n",
        "                       feature_matrix,\n",
        "                       clf, \n",
        "                       learn_params,  \n",
        "                       X_train, \n",
        "                       y_train, \n",
        "                       X_test, \n",
        "                       y_test, \n",
        "                       skip_grid_search_cv=True,\n",
        "                       optimized_scorer=True)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return ret_val\n",
        "    \n",
        "    # clf.fit(X_train, y_train)\n",
        "    # y_pred = clf.predict(X_test)\n",
        "\n",
        "    # # ROC score\n",
        "    # auc = roc_auc_score(y_test, y_pred)\n",
        "    # print(\"ROC score: \", auc)\n",
        "\n",
        "    # # Print confusion matrix\n",
        "    # conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    # sns.heatmap(conf_matrix, annot=True,fmt='d', cmap=plt.cm.copper)\n",
        "    # plt.show()\n",
        "\n",
        "    # Parameters to compare\n",
        "    # params = {\n",
        "    #     'criterion':['entropy','gini'],\n",
        "    #     'class_weight':[{1:1,0:0.3},{1:1,0:0.4},{1:1,0:0.5},{1:1,0:0.6},{1:1,0:7}]\n",
        "    # }\n",
        "\n",
        "    # Implement the classifier\n",
        "    # clf = RandomForestClassifier(\n",
        "    #     n_estimators=100,\n",
        "    #     max_features=None,\n",
        "    #     n_jobs=4,\n",
        "    # )\n",
        "\n",
        "    # # Test and validate\n",
        "    # score_optimization(clf, params, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m9UobqUWMI9b",
        "jupyter": {
          "source_hidden": true
        },
        "colab": {}
      },
      "source": [
        "# Predict for a particular month\n",
        "\n",
        "# %time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection)\n",
        "\n",
        "#%time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='test')\n",
        "#%time model(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='orig')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ns3Puh7Gnxl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "faa5166d-5a9d-4e4d-96d0-d7719d884758"
      },
      "source": [
        "%time gen_train_test_set(pd.datetime(2020,1,1), feature_matrix_selection, file_prefix='test')\n",
        "%time model(pd.datetime(2020,1,1), feature_matrix_selection, file_prefix='test')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 959893 observations.\n",
            "Testing on 10000 observations.\n",
            "\n",
            "Data shape before balancing: (959893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes in previous data:\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "NO balancing\n",
            "Data shape after balancing: (959893, 63)\n",
            "\n",
            "Counts of strikes VS non-strikes in new data:\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Filtering features\n",
            "\n",
            "Data shape before feature selection: (959893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes before feature selection:\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "\n",
            "Data shape after feature selection: (959893, 33)\n",
            "\n",
            "Counts of strikes vs non-strikes in new data:\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "CPU times: user 13 s, sys: 135 ms, total: 13.1 s\n",
            "Wall time: 20.5 s\n",
            "\n",
            "-----------2020-01-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(959893, 32) (959893,) (10000, 32) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 927839, 1: 32054})\n",
            "\n",
            "scale_pos_weight - 28.946122168840084\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 300, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.946122168840084, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.946122168840084]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.486, F-Score=0.06715\n",
            "\n",
            "Recall: 0.03625377643504532\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.97      0.95      0.04      0.96      0.19      0.04      9669\n",
            "          1       0.02      0.04      0.95      0.03      0.19      0.03       331\n",
            "\n",
            "avg / total       0.94      0.92      0.07      0.93      0.19      0.04     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0362538</td>\n",
              "      <td>0.949633</td>\n",
              "      <td>0.0189296</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity   Optimize\n",
              "XGBClassifier_optimize            0   0.0362538         0.949633  0.0189296"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 499 \n",
            "\n",
            "First 23 matches\n",
            "23 [  0  19  28  33  35  42  52  54  72  83 113 121 131 151 152 163 193 197\n",
            " 201 203 210 216 219]\n",
            "\n",
            "[0.5003511  0.52248204 0.50784427 0.50191045 0.5057217  0.5048611\n",
            " 0.50451064 0.5039963  0.509949   0.5020125  0.5027646  0.5010643\n",
            " 0.5022977  0.5021906  0.5024378  0.50260496 0.5020971  0.5164497\n",
            " 0.50408435 0.50168    0.5068371  0.5028062  0.5048865 ]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[5378  535  854 5301 3114 5311 7455 1271  950 4609 8938 4562 7227 1949\n",
            " 1933   19  592  594 9258 2479  197 7139  903]\n",
            "\n",
            "\n",
            "[0.51189953 0.513232   0.51331276 0.5274233  0.5207647  0.5264696\n",
            " 0.53518885 0.5227687  0.51804614 0.5193527  0.5237212  0.524247\n",
            " 0.5159437  0.5180875  0.5142077  0.52248204 0.5338774  0.5165382\n",
            " 0.5216545  0.5173233  0.5164497  0.51689523 0.5152363 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "139 [ 19  28  33  35  42  52  54  72  83 113 121 131 151 152 163 193 197 201\n",
            " 203 210 216 219 226 230 249 270 271 272 279 282 283 291 298 314 317 323\n",
            " 328 330 344 352 363 382 384 385 391 392 421 430 431 437 446 450 468 479\n",
            " 495 496 498 500 507 510 519 535 537 538 544 546 557 559 567 568 569 575\n",
            " 581 586 591 592 594 603 604 613 616 622 623 628 630 634 637 641 649 650\n",
            " 657 661 669 674 691 706 708 709 716 720 731 736 743 744 757 769 771 799\n",
            " 802 803 806 810 816 817 818 823 854 859 862 871 874 877 886 892 895 903\n",
            " 905 921 922 923 925 947 950 952 954 961 963 974 987]\n",
            "\n",
            "[0.52248204 0.50784427 0.50191045 0.5057217  0.5048611  0.50451064\n",
            " 0.5039963  0.509949   0.5020125  0.5027646  0.5010643  0.5022977\n",
            " 0.5021906  0.5024378  0.50260496 0.5020971  0.5164497  0.50408435\n",
            " 0.50168    0.5068371  0.5028062  0.5048865  0.50117046 0.5003884\n",
            " 0.5025136  0.5028138  0.50133765 0.5030081  0.50470847 0.5055\n",
            " 0.50871474 0.5023006  0.5014946  0.50675607 0.5011072  0.5016996\n",
            " 0.5018122  0.5025067  0.5013677  0.50133723 0.5067313  0.5027823\n",
            " 0.5012851  0.5021433  0.5073786  0.5073692  0.50053626 0.5026247\n",
            " 0.50536674 0.5053073  0.50418335 0.50007397 0.5000084  0.5058769\n",
            " 0.5062603  0.5100016  0.5045754  0.501044   0.51017076 0.50104684\n",
            " 0.50578594 0.513232   0.5063866  0.5053521  0.5027292  0.51045257\n",
            " 0.50993836 0.5049929  0.51028883 0.50314444 0.5026871  0.5063198\n",
            " 0.5037849  0.50220287 0.5042706  0.5338774  0.5165382  0.5028768\n",
            " 0.5070117  0.50277925 0.5035427  0.51019454 0.5031667  0.5073112\n",
            " 0.5076562  0.50011355 0.5019239  0.5080432  0.50286573 0.5026484\n",
            " 0.50329924 0.5031065  0.50078833 0.50025135 0.5018161  0.50466675\n",
            " 0.51111573 0.5002238  0.5056728  0.503027   0.50159967 0.5016565\n",
            " 0.50640553 0.5018991  0.5060473  0.5023689  0.5017106  0.5051317\n",
            " 0.5034798  0.5044686  0.50603133 0.50975513 0.50053    0.50272375\n",
            " 0.5001502  0.5076552  0.51331276 0.50349104 0.50172275 0.50040615\n",
            " 0.50272894 0.5014006  0.50064915 0.5023373  0.5099846  0.5152363\n",
            " 0.509494   0.50127655 0.50588864 0.5073575  0.5114115  0.50101775\n",
            " 0.51804614 0.50017285 0.5008312  0.5047453  0.5074354  0.5058613\n",
            " 0.50231266]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.504860520362854\n",
            "\n",
            "125 [  19   28   35   42   72  197  210  219  282  283  314  363  391  392\n",
            "  431  437  479  495  496  507  519  535  537  538  546  557  559  567\n",
            "  575  592  594  604  622  628  630  641  708  716  743  757  799  806\n",
            "  810  823  854  895  903  905  922  923  925  950  963  974 1009 1017\n",
            " 1047 1085 1090 1112 1115 1124 1231 1255 1271 1445 1508 1519 1563 1597\n",
            " 1671 1684 1686 1702 1747 1773 1810 1933 1949 1979 2107 2144 2172 2185\n",
            " 2213 2302 2466 2479 2620 2679 3114 3591 3830 4012 4016 4267 4465 4562\n",
            " 4609 4709 4851 5084 5220 5227 5301 5311 5378 6045 6114 6968 6997 7139\n",
            " 7166 7227 7231 7455 7586 7730 7836 8476 8499 8690 8938 9028 9258]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.5037346482276917\n",
            "\n",
            "161 [  19   28   35   42   52   54   72  197  201  210  219  279  282  283\n",
            "  314  363  391  392  431  437  446  479  495  496  498  507  519  535\n",
            "  537  538  546  557  559  567  575  581  591  592  594  604  622  628\n",
            "  630  641  706  708  716  743  757  799  803  806  810  823  854  895\n",
            "  903  905  922  923  925  950  961  963  974 1009 1017 1047 1064 1085\n",
            " 1088 1090 1112 1115 1124 1152 1231 1255 1271 1308 1445 1489 1508 1519\n",
            " 1550 1563 1597 1671 1684 1686 1702 1747 1773 1810 1933 1949 1979 2056\n",
            " 2107 2144 2172 2185 2213 2302 2466 2479 2489 2550 2620 2679 2842 2865\n",
            " 2958 3043 3090 3114 3359 3591 3830 3932 4012 4016 4267 4465 4562 4609\n",
            " 4709 4851 5084 5220 5227 5301 5311 5378 6045 6114 6391 6772 6806 6876\n",
            " 6945 6968 6997 7139 7166 7227 7231 7455 7586 7730 7775 7836 8476 8499\n",
            " 8690 8938 8981 9028 9258 9282 9755]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "499 [   0   19   28   33   35   42   52   54   72   83  113  121  131  151\n",
            "  152  163  193  197  201  203  210  216  219  226  230  249  270  271\n",
            "  272  279  282  283  291  298  314  317  323  328  330  344  352  363\n",
            "  382  384  385  391  392  421  430  431  437  446  450  468  479  495\n",
            "  496  498  500  507  510  519  535  537  538  544  546  557  559  567\n",
            "  568  569  575  581  586  591  592  594  603  604  613  616  622  623\n",
            "  628  630  634  637  641  649  650  657  661  669  674  691  706  708\n",
            "  709  716  720  731  736  743  744  757  769  771  799  802  803  806\n",
            "  810  816  817  818  823  854  859  862  871  874  877  886  892  895\n",
            "  903  905  921  922  923  925  947  950  952  954  961  963  974  987\n",
            " 1009 1017 1039 1047 1050 1061 1064 1085 1088 1090 1112 1115 1117 1124\n",
            " 1126 1152 1156 1164 1174 1175 1198 1216 1231 1249 1253 1255 1257 1260\n",
            " 1271 1287 1289 1296 1298 1304 1308 1320 1334 1340 1353 1378 1388 1445\n",
            " 1467 1472 1474 1475 1489 1508 1515 1516 1519 1531 1550 1555 1557 1563\n",
            " 1571 1580 1583 1593 1597 1615 1650 1671 1684 1686 1689 1700 1701 1702\n",
            " 1723 1736 1747 1766 1769 1773 1810 1828 1857 1862 1867 1879 1884 1933\n",
            " 1940 1942 1949 1950 1952 1957 1962 1979 1980 1984 1992 2011 2019 2036\n",
            " 2037 2053 2056 2064 2071 2097 2105 2107 2115 2144 2157 2171 2172 2185\n",
            " 2188 2209 2213 2237 2238 2241 2255 2260 2269 2272 2276 2278 2302 2326\n",
            " 2341 2393 2399 2403 2405 2418 2423 2441 2451 2461 2466 2479 2483 2489\n",
            " 2491 2509 2533 2536 2550 2552 2562 2572 2603 2620 2633 2645 2660 2679\n",
            " 2692 2704 2770 2786 2795 2833 2842 2865 2869 2871 2872 2889 2908 2936\n",
            " 2958 3009 3010 3029 3033 3034 3043 3072 3086 3090 3091 3094 3099 3113\n",
            " 3114 3138 3139 3145 3171 3216 3230 3251 3266 3269 3319 3334 3350 3356\n",
            " 3359 3400 3431 3482 3492 3494 3526 3545 3567 3591 3600 3618 3624 3675\n",
            " 3685 3732 3733 3781 3811 3830 3843 3919 3932 3960 3968 3988 4001 4012\n",
            " 4013 4016 4036 4098 4102 4123 4166 4267 4373 4435 4465 4479 4562 4567\n",
            " 4609 4615 4709 4725 4780 4789 4808 4813 4851 4859 4877 5067 5084 5213\n",
            " 5220 5227 5234 5237 5240 5301 5309 5311 5347 5378 5440 5442 5506 5628\n",
            " 5644 5673 5709 5725 5726 5797 5820 5825 5840 5858 5922 6019 6045 6055\n",
            " 6114 6152 6222 6286 6367 6379 6391 6481 6559 6620 6692 6742 6749 6772\n",
            " 6806 6876 6882 6918 6926 6928 6945 6962 6968 6978 6992 6997 7081 7097\n",
            " 7111 7126 7128 7139 7166 7210 7227 7231 7249 7294 7297 7320 7397 7402\n",
            " 7433 7455 7457 7492 7532 7539 7586 7730 7738 7775 7836 7943 7983 8009\n",
            " 8051 8186 8290 8307 8476 8490 8499 8524 8690 8701 8723 8938 8981 8983\n",
            " 9028 9258 9282 9372 9465 9595 9671 9692 9755]\n",
            "\n",
            "499 [0.5003511  0.52248204 0.50784427 0.50191045 0.5057217  0.5048611\n",
            " 0.50451064 0.5039963  0.509949   0.5020125  0.5027646  0.5010643\n",
            " 0.5022977  0.5021906  0.5024378  0.50260496 0.5020971  0.5164497\n",
            " 0.50408435 0.50168    0.5068371  0.5028062  0.5048865  0.50117046\n",
            " 0.5003884  0.5025136  0.5028138  0.50133765 0.5030081  0.50470847\n",
            " 0.5055     0.50871474 0.5023006  0.5014946  0.50675607 0.5011072\n",
            " 0.5016996  0.5018122  0.5025067  0.5013677  0.50133723 0.5067313\n",
            " 0.5027823  0.5012851  0.5021433  0.5073786  0.5073692  0.50053626\n",
            " 0.5026247  0.50536674 0.5053073  0.50418335 0.50007397 0.5000084\n",
            " 0.5058769  0.5062603  0.5100016  0.5045754  0.501044   0.51017076\n",
            " 0.50104684 0.50578594 0.513232   0.5063866  0.5053521  0.5027292\n",
            " 0.51045257 0.50993836 0.5049929  0.51028883 0.50314444 0.5026871\n",
            " 0.5063198  0.5037849  0.50220287 0.5042706  0.5338774  0.5165382\n",
            " 0.5028768  0.5070117  0.50277925 0.5035427  0.51019454 0.5031667\n",
            " 0.5073112  0.5076562  0.50011355 0.5019239  0.5080432  0.50286573\n",
            " 0.5026484  0.50329924 0.5031065  0.50078833 0.50025135 0.5018161\n",
            " 0.50466675 0.51111573 0.5002238  0.5056728  0.503027   0.50159967\n",
            " 0.5016565  0.50640553 0.5018991  0.5060473  0.5023689  0.5017106\n",
            " 0.5051317  0.5034798  0.5044686  0.50603133 0.50975513 0.50053\n",
            " 0.50272375 0.5001502  0.5076552  0.51331276 0.50349104 0.50172275\n",
            " 0.50040615 0.50272894 0.5014006  0.50064915 0.5023373  0.5099846\n",
            " 0.5152363  0.509494   0.50127655 0.50588864 0.5073575  0.5114115\n",
            " 0.50101775 0.51804614 0.50017285 0.5008312  0.5047453  0.5074354\n",
            " 0.5058613  0.50231266 0.50519043 0.5067419  0.5027104  0.50537485\n",
            " 0.5033197  0.5011895  0.5045382  0.50516135 0.50429684 0.5053881\n",
            " 0.50654525 0.50639546 0.500511   0.5113807  0.50303376 0.50468665\n",
            " 0.50250125 0.5003006  0.5008098  0.5001686  0.50284076 0.50357056\n",
            " 0.50489    0.501008   0.5008789  0.5052995  0.501116   0.5003353\n",
            " 0.5227687  0.50267607 0.50279516 0.5034297  0.5006537  0.5026675\n",
            " 0.5042548  0.50348127 0.50075406 0.5010153  0.5016317  0.5021651\n",
            " 0.5007681  0.504919   0.5011431  0.50122833 0.5001213  0.50217646\n",
            " 0.50449264 0.50611395 0.503413   0.5017338  0.5059886  0.50228363\n",
            " 0.50436264 0.5012527  0.5003366  0.50488573 0.50229263 0.50039124\n",
            " 0.5028025  0.50042796 0.5088883  0.5022468  0.50371754 0.5067246\n",
            " 0.5056808  0.5080082  0.50017947 0.50166845 0.50234133 0.50694084\n",
            " 0.5014281  0.5004545  0.50666535 0.50012976 0.5010901  0.50532013\n",
            " 0.5049537  0.501482   0.5008749  0.5009867  0.50006473 0.50255376\n",
            " 0.50075465 0.5142077  0.5035889  0.5033696  0.5180875  0.50297505\n",
            " 0.5029536  0.5007569  0.5015399  0.50581276 0.5007272  0.5000616\n",
            " 0.5007575  0.5008069  0.5005511  0.50108254 0.5016802  0.50256294\n",
            " 0.5044535  0.50257367 0.5011153  0.50141895 0.50132126 0.5064769\n",
            " 0.5019719  0.50708884 0.5008176  0.50159174 0.5096441  0.5060774\n",
            " 0.500904   0.5012206  0.505403   0.50247496 0.5006065  0.50128055\n",
            " 0.5019138  0.50114095 0.5018933  0.50179833 0.5017493  0.5014772\n",
            " 0.5056648  0.5001841  0.501524   0.5017295  0.5005708  0.50076395\n",
            " 0.5004045  0.50109017 0.5013791  0.50073427 0.5036622  0.5016156\n",
            " 0.5105219  0.5173233  0.50286525 0.50399023 0.5004119  0.50244206\n",
            " 0.5026509  0.50363785 0.5039497  0.5003184  0.50167805 0.5008776\n",
            " 0.5028903  0.507734   0.5025961  0.50050765 0.50345844 0.5055035\n",
            " 0.5014045  0.50179446 0.5026105  0.5004396  0.50279456 0.50096923\n",
            " 0.50455225 0.5048195  0.5002355  0.50181985 0.5006636  0.50078875\n",
            " 0.501681   0.5031641  0.50439185 0.5027303  0.5014436  0.5019937\n",
            " 0.50033325 0.5024191  0.5046285  0.500464   0.5028169  0.50389045\n",
            " 0.5032875  0.5032902  0.5025971  0.5017341  0.5207647  0.50149924\n",
            " 0.50013566 0.5012895  0.5032843  0.50029504 0.50225574 0.501756\n",
            " 0.50262296 0.50022054 0.5002448  0.50361246 0.5010592  0.50252795\n",
            " 0.50381094 0.5024969  0.50079846 0.50357217 0.50142336 0.5016774\n",
            " 0.5019194  0.50274986 0.500615   0.506279   0.5004397  0.50329953\n",
            " 0.5027782  0.5014529  0.5018564  0.5005106  0.5017879  0.50016177\n",
            " 0.50219464 0.5054565  0.50021964 0.5024495  0.5043269  0.5014249\n",
            " 0.50286686 0.50046194 0.5001995  0.5059587  0.50059366 0.50557494\n",
            " 0.50212014 0.5003603  0.50139415 0.50347596 0.50072443 0.5061113\n",
            " 0.50204647 0.5003465  0.5116624  0.5028855  0.524247   0.5037081\n",
            " 0.5193527  0.50112814 0.5060504  0.5004221  0.5012994  0.500213\n",
            " 0.5004896  0.5013476  0.5050086  0.50244296 0.5004675  0.5015824\n",
            " 0.5054314  0.5011358  0.5054858  0.5062073  0.50075305 0.5022744\n",
            " 0.50322175 0.5274233  0.5006698  0.5264696  0.50023097 0.51189953\n",
            " 0.50091743 0.50145924 0.5023199  0.50187606 0.5017801  0.5009645\n",
            " 0.5002382  0.50354016 0.50336486 0.5009919  0.50129896 0.5011878\n",
            " 0.5007111  0.5004217  0.5009532  0.50176543 0.50515676 0.5005477\n",
            " 0.5052177  0.5005183  0.5002723  0.50106156 0.50115174 0.50006264\n",
            " 0.50409156 0.50152296 0.5009079  0.50010085 0.50152975 0.5001474\n",
            " 0.50032455 0.50444067 0.5045199  0.50385445 0.5017019  0.5006076\n",
            " 0.5015045  0.5013372  0.5041872  0.50187016 0.50622576 0.5030804\n",
            " 0.50161535 0.5055178  0.50123614 0.50154245 0.5003346  0.5025405\n",
            " 0.5020591  0.51689523 0.5072355  0.5002202  0.5159437  0.5063022\n",
            " 0.50137895 0.5026437  0.50030047 0.5032854  0.5010801  0.5014058\n",
            " 0.5016636  0.53518885 0.5007284  0.50276446 0.50244975 0.5012055\n",
            " 0.5067268  0.5056736  0.50070006 0.50376034 0.50869143 0.5006901\n",
            " 0.5000065  0.502742   0.50026447 0.50015754 0.5002711  0.50342256\n",
            " 0.50493324 0.5000145  0.5071388  0.5014634  0.5074262  0.50271624\n",
            " 0.5011228  0.5237212  0.5041998  0.50299394 0.5062647  0.5216545\n",
            " 0.5042311  0.5006895  0.503648   0.5021829  0.5013047  0.50121516\n",
            " 0.50419456]\n",
            "\n",
            "Matched draws\n",
            "Count: 12, Index: (array([ 203,  216,  450,  905, 1340, 1701, 2483, 2795, 3960, 5227, 7210,\n",
            "       7294]),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "106955  505820 2020-01-12  ConsolationNo2      203\n",
            "107125  506520 2020-01-26     SpecialNo10      203\n",
            "107163  506720 2020-01-29  ConsolationNo3      216\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "106858  505420 2020-01-04      1stPrizeNo      450\n",
            "106955  505820 2020-01-12  ConsolationNo2      203\n",
            "106961  505820 2020-01-12  ConsolationNo8      905\n",
            "107125  506520 2020-01-26     SpecialNo10      203\n",
            "107163  506720 2020-01-29  ConsolationNo3      216\n",
            "\n",
            "\n",
            "All matched\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "106856  505320 2020-01-01      SpecialNo8     3960\n",
            "106858  505420 2020-01-04      1stPrizeNo      450\n",
            "106859  505420 2020-01-04      2ndPrizeNo     2483\n",
            "106863  505420 2020-01-04  ConsolationNo2     7294\n",
            "106917  505620 2020-01-08      SpecialNo1     5227\n",
            "106940  505720 2020-01-11      SpecialNo1     1701\n",
            "106955  505820 2020-01-12  ConsolationNo2      203\n",
            "106961  505820 2020-01-12  ConsolationNo8      905\n",
            "106965  505820 2020-01-12      SpecialNo2     1340\n",
            "107077  506320 2020-01-22  ConsolationNo9     7210\n",
            "107119  506520 2020-01-26  ConsolationNo5     2795\n",
            "107125  506520 2020-01-26     SpecialNo10      203\n",
            "107163  506720 2020-01-29  ConsolationNo3      216\n",
            "CPU times: user 32min 25s, sys: 997 ms, total: 32min 26s\n",
            "Wall time: 8min 15s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VE6Xbz-IyvLj",
        "outputId": "01253b36-c7e7-4e4b-e459-a1088e1fa3a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        }
      },
      "source": [
        "weight=1.0\n",
        "decrement = 0.000\n",
        "to_stop=False\n",
        "\n",
        "dt = pd.datetime(2020,1,1)\n",
        "%time gen_train_test_set(dt, feature_matrix_selection, file_prefix='test')\n",
        "while not to_stop:\n",
        "  to_stop = model(dt, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "  decrement = decrement + 0.005"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 959893 observations.\n",
            "Testing on 10000 observations.\n",
            "\n",
            "Data shape before balancing: (959893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes in previous data:\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "NO balancing\n",
            "Data shape after balancing: (959893, 63)\n",
            "\n",
            "Counts of strikes VS non-strikes in new data:\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Filtering features\n",
            "\n",
            "Data shape before feature selection: (959893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes before feature selection:\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "\n",
            "Data shape after feature selection: (959893, 33)\n",
            "\n",
            "Counts of strikes vs non-strikes in new data:\n",
            "0    927839\n",
            "1     32054\n",
            "Name: Label, dtype: int64\n",
            "CPU times: user 12.3 s, sys: 168 ms, total: 12.4 s\n",
            "Wall time: 12.5 s\n",
            "\n",
            "-----------2020-01-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(959893, 32) (959893,) (10000, 32) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 927839, 1: 32054})\n",
            "\n",
            "scale_pos_weight - 28.946122168840084\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 300, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.946122168840084, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.946122168840084]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qktZbi7OGqP3",
        "colab": {}
      },
      "source": [
        "# start_mt = pd.datetime(2019,7,1)\n",
        "# how_many_mt = 6 \n",
        "# for i in range(how_many_mt):\n",
        "#   month_to_predict = start_mt + relativedelta(months=i)\n",
        "#   print(f\"\\n{month_to_predict}\\n-------------------\\n\")\n",
        "\n",
        "#   weight=1.0\n",
        "#   decrement = 0.000\n",
        "#   to_stop=False\n",
        "\n",
        "#   gen_train_test_set(month_to_predict, feature_matrix_selection, file_prefix='test')\n",
        "#   while not to_stop:\n",
        "#     to_stop = model(month_to_predict, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#     decrement = decrement + 0.001\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N8tcqn4yIl21",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}