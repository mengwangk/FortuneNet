{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04_02_automated_machine_learning.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengwangk/dl-projects/blob/master/04_09_auto_ml_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4hyoPGdjpqa_"
      },
      "source": [
        "# Automated ML - Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SLxr2k_ue8yq",
        "colab": {}
      },
      "source": [
        "COLAB = True\n",
        "\n",
        "DATASET_NAME = '4D.zip'\n",
        "\n",
        "FEATURE_DATASET_PREFIX = 'feature_matrix_d2_v3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwYshXtLt7b7",
        "colab": {}
      },
      "source": [
        "#!pip install -U imblearn\n",
        "#!pip install -U xgboost\n",
        "# !pip install -U featuretools\n",
        "\n",
        "# https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28\n",
        "# https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
        "# https://machinelearningmastery.com/imbalanced-classification-model-to-detect-oil-spills/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oy5ww2zRfFGG",
        "outputId": "919c19c5-2298-4dfd-a396-6983312ce8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "if COLAB:\n",
        "  !rm -rf dl-projects\n",
        "  !git clone https://github.com/mengwangk/dl-projects"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dl-projects'...\n",
            "remote: Enumerating objects: 160, done.\u001b[K\n",
            "remote: Counting objects: 100% (160/160), done.\u001b[K\n",
            "remote: Compressing objects: 100% (153/153), done.\u001b[K\n",
            "remote: Total 1962 (delta 100), reused 17 (delta 7), pack-reused 1802\u001b[K\n",
            "Receiving objects: 100% (1962/1962), 78.26 MiB | 29.65 MiB/s, done.\n",
            "Resolving deltas: 100% (1211/1211), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G2xin10SfozR",
        "colab": {}
      },
      "source": [
        "if COLAB:\n",
        "  !cp dl-projects/utils* .\n",
        "  !cp dl-projects/preprocess* .\n",
        "  !cp dl-projects/plot* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fC2-l3JBpqbE",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "# %reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TP7V_IzepqbK",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "import matplotlib\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import featuretools as ft\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, make_scorer, recall_score, roc_curve, mean_squared_error, accuracy_score, average_precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                     ClusterCentroids,\n",
        "                                     TomekLinks,\n",
        "                                     NeighbourhoodCleaningRule,\n",
        "                                     AllKNN,\n",
        "                                     NearMiss,\n",
        "                                     OneSidedSelection,\n",
        "                                     EditedNearestNeighbours)\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
        "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC, ADASYN \n",
        "import pylab as pl\n",
        "import xgboost as xgb\n",
        "from collections import Counter\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "# from skopt import BayesSearchCV\n",
        "# from skopt.space import Real, Categorical, Integer\n",
        "# from scikitplot.plotters import plot_precision_recall_curve\n",
        "\n",
        "from utils import feature_selection, plot_feature_importances\n",
        "from preprocess import *\n",
        "from plot import plot_correlation_matrix, plot_labeled_scatter\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "sns.set(style=\"ticks\")\n",
        "\n",
        "# The Answer to the Ultimate Question of Life, the Universe, and Everything.\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3bFT5CoxpqbP",
        "outputId": "51fef430-9050-4e00-f5d7-3b7aa47b2618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%aimport"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modules to reload:\n",
            "all-except-skipped\n",
            "\n",
            "Modules to skip:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3E16jPVPpqbV"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U421BuhtfYS7",
        "outputId": "07aa2870-b572-492a-d045-4915eb678f5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  GDRIVE_DATASET_FOLDER = Path('gdrive/My Drive/datasets/')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9IgnETKkpqbX",
        "outputId": "198e3fb0-0683-44e0-f257-7fa2094c1031",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "if COLAB:\n",
        "  DATASET_PATH = GDRIVE_DATASET_FOLDER\n",
        "  ORIGIN_DATASET_PATH = Path('dl-projects/datasets')\n",
        "else:\n",
        "  DATASET_PATH = Path(\"../datasets\")\n",
        "  ORIGIN_DATASET_PATH = Path('datasets')\n",
        "\n",
        "DATASET = DATASET_PATH/f\"{FEATURE_DATASET_PREFIX}.ft\"\n",
        "ORIGIN_DATASET = ORIGIN_DATASET_PATH/DATASET_NAME\n",
        "\n",
        "if COLAB:\n",
        "  !ls -l gdrive/\"My Drive\"/datasets/ --block-size=M\n",
        "  !ls -l dl-projects/datasets --block-size=M"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4800M\n",
            "-rw------- 1 root root   17M Mar  1 05:47 feature_matrix_2020_feb.ft\n",
            "-rw------- 1 root root   17M Mar  1 05:39 feature_matrix_2020_jan.ft\n",
            "-rw------- 1 root root   17M Mar  1 05:56 feature_matrix_2020_mar.ft\n",
            "-rw------- 1 root root 2454M Jan 12 01:24 feature_matrix_d2_v2.ft\n",
            "-rw------- 1 root root 1585M Jan 12 23:39 feature_matrix_d2_v3.ft\n",
            "-rw------- 1 root root   17M Feb 21 13:07 feature_matrix_snapshot.ft\n",
            "-rw------- 1 root root    5M Jan 30 04:33 orig_X_test.ft\n",
            "-rw------- 1 root root  415M Jan 30 04:33 orig_X_train.ft\n",
            "-rw------- 1 root root    1M Jan 30 04:33 orig_y_test.ft\n",
            "-rw------- 1 root root    7M Jan 30 04:33 orig_y_train.ft\n",
            "-rw------- 1 root root    3M Mar  2 15:06 test_X_test.ft\n",
            "-rw------- 1 root root  259M Mar  2 15:06 test_X_train.ft\n",
            "-rw------- 1 root root    1M Mar  2 15:06 test_y_test.ft\n",
            "-rw------- 1 root root    8M Mar  2 15:06 test_y_train.ft\n",
            "total 25M\n",
            "-rw-r--r-- 1 root root  1M Mar  3 12:51 4D.zip\n",
            "-rw-r--r-- 1 root root 25M Mar  3 12:51 labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "urQTD6DQNutw",
        "colab": {}
      },
      "source": [
        "# Read the data\n",
        "data = pd.read_feather(DATASET)\n",
        "origin_data = format_tabular(ORIGIN_DATASET)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zov05QHZxxiS"
      },
      "source": [
        "## Add new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "foPB8T1vx2tp",
        "outputId": "1bb3742a-1d16-4244-b5c0-fd7f0f15a3e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(959893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43sc1Eaux25j",
        "colab": {}
      },
      "source": [
        "jan_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_jan.ft\")\n",
        "feb_2020= pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_feb.ft\")\n",
        "mar_2020= pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_mar.ft\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2vISxEbsyQG1",
        "outputId": "174b3338-0a68-421e-d458-039e233fcb8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_data = data.append(jan_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(feb_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(mar_2020[data.columns],ignore_index=True)\n",
        "new_data.shape "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FVVMXCj-zyaW",
        "outputId": "50878247-7e0b-4785-e092-565334a23438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = new_data\n",
        "data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vOYlp-8Br61r"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kHiN1VVlG9Kh"
      },
      "source": [
        "### View data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnQXyVqng5Cm",
        "colab": {}
      },
      "source": [
        "# Feature matrix\n",
        "feature_matrix = data.drop(columns=['NumberId', 'month', 'year'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "apMYVNz9HK9e",
        "outputId": "16ca4ca0-2a71-4ff4-a337-58a135d90cd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Sort data\n",
        "feature_matrix.sort_values(by=['time', 'MAX(Results.LuckyNo)'], inplace=True)\n",
        "feature_matrix.info()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 989893 entries, 7020 to 986394\n",
            "Columns: 214 entries, time to LAST(Results.PrizeType)_Prize\n",
            "dtypes: datetime64[ns](1), float64(155), int64(56), uint8(2)\n",
            "memory usage: 1.6 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZKTbWRFJNUq",
        "outputId": "86ac8530-aac4-43ae-9cb2-831d76c9876a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print('Distribution')\n",
        "print(feature_matrix['Label'].value_counts())\n",
        "print()\n",
        "print('Positive: ' + str(feature_matrix['Label'].value_counts()[0]) + ' which is ', round(feature_matrix['Label'].value_counts()[0]/len(feature_matrix) * 100,2), '% of the dataset')\n",
        "print('Negative: ' + str(feature_matrix['Label'].value_counts()[1]) + ' which is ', round(feature_matrix['Label'].value_counts()[1]/len(feature_matrix) * 100,2), '% of the dataset')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution\n",
            "0    957191\n",
            "1     32702\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Positive: 957191 which is  96.7 % of the dataset\n",
            "Negative: 32702 which is  3.3 % of the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "plplpAQ6JrKb",
        "outputId": "aa8fa0c2-181c-48f7-eb8b-cc30e29cf72c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "feature_matrix.isna().sum().sort_values(ascending=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))               7657\n",
              "CUM_MEAN(TREND(Results.LuckyNo, DrawDate))                7657\n",
              "TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)    7657\n",
              "TREND(Results.CUM_MEAN(TotalStrike), DrawDate)            7657\n",
              "TREND(Results.CUM_SUM(LuckyNo), DrawDate)                 7657\n",
              "                                                          ... \n",
              "CUM_SUM(MIN(Results.DrawNo))                                 0\n",
              "NUM_UNIQUE(Results.DAY(DrawDate))                            0\n",
              "NUM_UNIQUE(Results.MONTH(DrawDate))                          0\n",
              "SUM(Results.PERCENTILE(LuckyNo))                             0\n",
              "time                                                         0\n",
              "Length: 214, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF_zCRksL1Ls"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1aLGsXSOa9K",
        "colab": {}
      },
      "source": [
        "# Fill all NaN with 0\n",
        "feature_matrix = feature_matrix.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5A8LZ805MqjP",
        "outputId": "5af0fcf4-12bd-4782-9d85-93f98735f545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "feature_matrix.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 214)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rPFOkiGjhuKj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7abb050d-39f4-411b-ac4d-e8871f8edb8d"
      },
      "source": [
        "feature_matrix_selection = feature_selection(feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label']))\n",
        "# feature_matrix_selection = feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape:  (989893, 211)\n",
            "0 missing columns with threshold: 90.\n",
            "41 zero variance columns.\n",
            "108 collinear columns removed with threshold: 0.95.\n",
            "Total columns removed:  149\n",
            "Shape after feature selection: (989893, 62).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vT2K0WeJhugH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "440d7511-d600-441d-ba4f-9d00555310d1"
      },
      "source": [
        "feature_matrix_selection.shape, feature_matrix_selection.columns"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((989893, 62),\n",
              " Index(['STD(Results.DrawNo)', 'MAX(Results.DrawNo)', 'MAX(Results.LuckyNo)',\n",
              "        'MIN(Results.DrawNo)', 'MEAN(Results.DrawNo)', 'SKEW(Results.DrawNo)',\n",
              "        'AVG_TIME_BETWEEN(Results.DrawDate)', 'COUNT(Results)',\n",
              "        'SUM(Results.DrawNo)', 'SUM(Results.LuckyNo)',\n",
              "        'TREND(Results.DrawNo, DrawDate)', 'MONTH(first_Results_time)',\n",
              "        'DAY(first_Results_time)', 'TIME_SINCE(first_Results_time)',\n",
              "        'TIME_SINCE_PREVIOUS(first_Results_time)',\n",
              "        'STD(Results.PERCENTILE(DrawNo))',\n",
              "        'STD(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'STD(Results.CUM_SUM(DrawNo))', 'STD(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MAX(Results.PERCENTILE(DrawNo))',\n",
              "        'MAX(Results.PERCENTILE(TotalStrike))',\n",
              "        'MAX(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MIN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MIN(Results.CUM_SUM(DrawNo))', 'MIN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MODE(Results.MONTH(DrawDate))', 'MODE(Results.DAY(DrawDate))',\n",
              "        'MEAN(Results.TIME_SINCE(DrawDate))',\n",
              "        'MEAN(Results.PERCENTILE(DrawNo))',\n",
              "        'MEAN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MEAN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'SKEW(Results.CUM_SUM(DrawNo))', 'SKEW(Results.CUM_MEAN(LuckyNo))',\n",
              "        'LAST(Results.DAY(DrawDate))',\n",
              "        'LAST(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'LAST(Results.MONTH(DrawDate))', 'LAST(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SUM(Results.TIME_SINCE(DrawDate))',\n",
              "        'SUM(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'TREND(Results.CUM_MEAN(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(DrawNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(TotalStrike), DrawDate)',\n",
              "        'TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)',\n",
              "        'TREND(Results.CUM_SUM(DrawNo), DrawDate)',\n",
              "        'NUM_UNIQUE(Results.MONTH(DrawDate))',\n",
              "        'NUM_UNIQUE(Results.DAY(DrawDate))', 'CUM_SUM(MIN(Results.DrawNo))',\n",
              "        'CUM_SUM(SKEW(Results.DrawNo))',\n",
              "        'CUM_MEAN(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'CUM_MEAN(SUM(Results.LuckyNo))', 'CUM_MEAN(SKEW(Results.DrawNo))',\n",
              "        'PERCENTILE(STD(Results.LuckyNo))', 'PERCENTILE(LAST(Results.DrawNo))',\n",
              "        'PERCENTILE(MAX(Results.TotalStrike))',\n",
              "        'PERCENTILE(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'PERCENTILE(COUNT(Results))', 'PERCENTILE(STD(Results.DrawNo))',\n",
              "        'PERCENTILE(SKEW(Results.DrawNo))', 'PERCENTILE(SUM(Results.DrawNo))',\n",
              "        'PERCENTILE(TREND(Results.DrawNo, DrawDate))'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yZUhYrWFiRod",
        "colab": {}
      },
      "source": [
        "feature_matrix_selection['time'] = feature_matrix['time']\n",
        "feature_matrix_selection['TotalStrike'] = feature_matrix['TotalStrike']\n",
        "feature_matrix_selection['Label'] = feature_matrix['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hugygOqSiR6K"
      },
      "source": [
        "### Feature Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "loagcqTEKOkO",
        "colab": {}
      },
      "source": [
        "#feature_matrix.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7Ha8Zlkhuoe",
        "colab": {}
      },
      "source": [
        "# Check with feature selection\n",
        "#corrs = feature_matrix_selection.corr().sort_values('Label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EWRODfAdPk6j",
        "colab": {}
      },
      "source": [
        "#corrs['Label'].head(60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "waeD1ED_kqDB"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9yrJyIVLh5So",
        "colab": {}
      },
      "source": [
        "def recall_optim(y_true, y_pred):\n",
        "    \"\"\"Make a scoring function that improves specificity while identifying all strikes\n",
        "    \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Recall will be worth a greater value than specificity\n",
        "    rec = recall_score(y_true, y_pred) * 0.8 \n",
        "    spe = conf_matrix[0,0]/conf_matrix[0,:].sum() * 0.2 \n",
        "    \n",
        "    # Imperfect recalls will lose a penalty\n",
        "    # This means the best results will have perfect recalls and compete for specificity\n",
        "    if rec < 0.8:\n",
        "        rec -= 0.2\n",
        "    return rec + spe \n",
        "\n",
        "\n",
        "# Make a scoring callable from recall_score\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Create a scoring callable based on the scoring function\n",
        "optimize = make_scorer(recall_optim)\n",
        "\n",
        "# Geometric mean scorer\n",
        "geo_mean_scorer = make_scorer(geometric_mean_score)\n",
        "\n",
        "# DataFrame to store classifier performance\n",
        "performance = pd.DataFrame(columns=['Train_Recall','Test_Recall','Test_Specificity', 'Optimize'])\n",
        "\n",
        "def to_labels(pos_probs, threshold):\n",
        "    \"\"\"Apply threshold to positive probabilities to create labels.\n",
        "    \"\"\"\n",
        "    return (pos_probs >= threshold).astype('int')\n",
        " \n",
        "\n",
        "def score_optimization(dt, feature_matrix, clf, params, X_train, y_train, X_test, y_test, skip_grid_search_cv=False, optimized_scorer=False):\n",
        "    \"\"\"Find the optimized classifier.\n",
        "    \"\"\"\n",
        "    if not skip_grid_search_cv:\n",
        "      print(\"\\nFinding the optimized classifier...\")\n",
        "\n",
        "      # Load GridSearchCV\n",
        "      # search = GridSearchCV(\n",
        "      search = RandomizedSearchCV(\n",
        "            estimator=clf,\n",
        "            #param_grid=params,\n",
        "            param_distributions=params,\n",
        "            n_jobs=4,\n",
        "            scoring=optimize  # Use custom scorer\n",
        "      )\n",
        "\n",
        "      # Train search object\n",
        "      search.fit(X_train, y_train)\n",
        "\n",
        "      # Heading\n",
        "      print('\\n','-'*40,'\\n',clf.__class__.__name__,'\\n','-'*40)\n",
        "\n",
        "      # Extract best estimator\n",
        "      best = search.best_estimator_\n",
        "      print('Best parameters: \\n\\n',search.best_params_,'\\n')\n",
        "    \n",
        "    else:\n",
        "      print(\"\\nUse the passed in classifier...\\n\")\n",
        "      best = clf\n",
        "\n",
        "    # Cross-validate on the train data\n",
        "    if not skip_grid_search_cv: \n",
        "      print(\"TRAIN GROUP\")\n",
        "      #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
        "      cv = 3\n",
        "      if not optimized_scorer:\n",
        "        print('\\nUse default scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  scoring=recall,\n",
        "                                  cv=cv)\n",
        "      else:\n",
        "        print('\\nUse optimized scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  #scoring=optimize,\n",
        "                                  scoring='roc_auc',\n",
        "                                  #scoring=geo_mean_scorer,\n",
        "                                  cv=cv)\n",
        "\n",
        "      print(\"\\nCross-validation recall scores:\",train_cv)\n",
        "      print(\"Mean recall score:\",train_cv.mean())\n",
        "      print('Mean G-Mean: %.3f (%.3f)' % (np.mean(train_cv), np.std(train_cv)))\n",
        "    else:\n",
        "      train_cv = np.zeros(3)\n",
        "\n",
        "    # Now predict on the test group\n",
        "    print(\"\\nTEST GROUP\")\n",
        "    y_pred = best.fit(X_train, y_train).predict(X_test)\n",
        "    # y_pred = best.fit(X_train, y_train,\n",
        "    #                   eval_set=[(X_test, y_test)],\n",
        "    #                   eval_metric='auc',\n",
        "    #                   early_stopping_rounds=10,\n",
        "    #                   verbose=True\n",
        "    #                   ).predict(X_test)\n",
        "\n",
        "    # keep probabilities for the positive outcome only\n",
        "    probas = best.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # define thresholds\n",
        "    thresholds = np.arange(0, 1, 0.001)\n",
        "\n",
        "    # evaluate each threshold\n",
        "    scores = [f1_score(y_test, to_labels(probas, t)) for t in thresholds]\n",
        "\n",
        "    # get best threshold\n",
        "    ix = np.argmax(scores)\n",
        "    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))\n",
        "\n",
        "    # print recall\n",
        "    print(\"\\nRecall:\",recall_score(y_test,y_pred))\n",
        "\n",
        "    # Get imbalanced classification report\n",
        "    print(classification_report_imbalanced(y_test, y_pred))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test,y_pred)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=plt.cm.copper)\n",
        "    plt.show()\n",
        "\n",
        "    # Store results\n",
        "    performance.loc[clf.__class__.__name__+'_optimize',:] = [\n",
        "        train_cv.mean(),\n",
        "        recall_score(y_test,y_pred),\n",
        "        conf_matrix[0,0]/conf_matrix[0,:].sum(),\n",
        "        recall_optim(y_test,y_pred)\n",
        "    ]\n",
        "    # Look at the parameters for the top best scores\n",
        "    if not skip_grid_search_cv:\n",
        "      display(pd.DataFrame(search.cv_results_).iloc[:,4:].sort_values(by='rank_test_score').head())\n",
        "    display(performance)\n",
        "\n",
        "    # Additionl info\n",
        "    print('\\n\\nAdditional Info')\n",
        "    print('='*40)\n",
        "    positive = np.where((y_pred==1))\n",
        "    print(f'Total predicted to be positive: {len(positive[0])} \\n')\n",
        "\n",
        "    pred = np.where((y_pred==1))\n",
        "    all_preds = pred[0]\n",
        "\n",
        "    # Total predicted matches\n",
        "    print('First 23 matches')\n",
        "    print(23, all_preds[0:23])\n",
        "    print(f'\\n{probas[all_preds[0:23]]}\\n') \n",
        "\n",
        "    print(\"\\nTop 23 Probable Matches\")\n",
        "    #print('probas', probas)\n",
        "    topN = np.argpartition(probas, -23)[-23:]\n",
        "    print(f'\\n{topN}\\n')          # Top N most high probability numbers\n",
        "    print(f'\\n{probas[topN]}\\n')  # Top N probability\n",
        "\n",
        "    # Check for 2 to 3 digits range \n",
        "    print('\\n2 To 3 Digits\\n')\n",
        "    idx_range = np.where((all_preds < 1000) & (all_preds >= 10))\n",
        "    #print(idx_range)\n",
        "    range_numbers = all_preds[idx_range]\n",
        "    print(len(range_numbers), range_numbers)\n",
        "    print(f'\\n{probas[range_numbers]}\\n') \n",
        "\n",
        "\n",
        "    # 2 to 3 Digits > Average Probas\n",
        "    print('\\n2 To 3 Digits Average Proba\\n')\n",
        "    avg_proba = np.average(probas[range_numbers])\n",
        "    print(f'Average proba {avg_proba}\\n')\n",
        "    idx_avg_proba = np.where(probas > avg_proba) \n",
        "    print(len(idx_avg_proba[0]), idx_avg_proba[0])\n",
        "\n",
        "    # 2 to 3 Digits > All Average Probas\n",
        "    print('\\n\\nAll Average Proba\\n')\n",
        "    all_avg_proba = np.average(probas[all_preds])\n",
        "    print(f'All average probas {all_avg_proba}\\n')\n",
        "    idx_all_avg_proba = np.where(probas > all_avg_proba) \n",
        "    print(len(idx_all_avg_proba[0]), idx_all_avg_proba[0])\n",
        "\n",
        "\n",
        "    # All predicted matches\n",
        "    print('\\n\\nAll Predictions\\n')\n",
        "    print(len(all_preds), all_preds)\n",
        "    print()\n",
        "    print(len(probas[all_preds]), probas[all_preds])\n",
        "    \n",
        "    #print('Debug')\n",
        "    #print(pred)\n",
        "    \n",
        "    if len(positive[0]) > 0:\n",
        "    \n",
        "      # Matching draws\n",
        "      print('\\nMatched draws')\n",
        "      md = np.where((y_pred==1) & (y_test==1))\n",
        "      print(f\"Count: {len(md[0])}, Index: {md}\")\n",
        "      month_data = feature_matrix.loc[feature_matrix['time'] == dt]\n",
        "      numbers = month_data.iloc[md[0]][['MAX(Results.LuckyNo)']]\n",
        "\n",
        "      print('\\n\\nTop 23 Possibility')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(topN))].head(23))  \n",
        "      \n",
        "      print('\\n\\nFirst 23 Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(pred[0][0:23]))].head(23))    \n",
        "             \n",
        "\n",
        "      print('\\n\\n2 To 3 Digits Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(range_numbers))].head(23))    \n",
        "     \n",
        "\n",
        "      print('\\n\\nAll matched')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(numbers['MAX(Results.LuckyNo)']))].head(100))    \n",
        "                                                  \n",
        "    else:\n",
        "      print('No luck this month')  \n",
        "\n",
        "    if len(range_numbers) >= 50:\n",
        "      return False\n",
        "\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrL8gYwjc-hd",
        "colab": {}
      },
      "source": [
        "def remove_outliers(balanced, threshold=0.001, remove=True):\n",
        "    \"\"\"Removing Outliers from high-correlation features.\n",
        "    \"\"\"\n",
        "\n",
        "    if not remove:\n",
        "      return balanced\n",
        "\n",
        "    bal_corr = balanced.corr()\n",
        "    no_outliers=pd.DataFrame(balanced.copy())\n",
        "\n",
        "    cols = bal_corr.Label.index[:-1]\n",
        "\n",
        "    # For each feature correlated with Class...\n",
        "    for col in cols:\n",
        "        # If absolute correlation value is more than X percent...\n",
        "        correlation = bal_corr.loc['Label',col]\n",
        "\n",
        "        if np.absolute(correlation) > threshold:\n",
        "          # Separate the classes of the high-correlation column\n",
        "          nonstrikes = no_outliers.loc[no_outliers.Label==0,col]\n",
        "          strikes = no_outliers.loc[no_outliers.Label==1,col]\n",
        "\n",
        "          # Identify the 25th and 75th quartiles\n",
        "          all_values = no_outliers.loc[:,col]\n",
        "          q25, q75 = np.percentile(all_values, 25), np.percentile(all_values, 75)\n",
        "          # Get the inter quartile range\n",
        "          iqr = q75 - q25\n",
        "          # Smaller cutoffs will remove more outliers\n",
        "          cutoff = iqr * 7\n",
        "          # Set the bounds of the desired portion to keep\n",
        "          lower, upper = q25 - cutoff, q75 + cutoff\n",
        "          \n",
        "          # If positively correlated...\n",
        "          # Drop nonstrikes above upper bound, and strikes below lower bound\n",
        "          if correlation > 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes>upper].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes<lower].index,inplace=True)\n",
        "          \n",
        "          # If negatively correlated...\n",
        "          # Drop non strikes below lower bound, and strikes above upper bound\n",
        "          elif correlation < 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes<lower].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes>upper].index,inplace=True)\n",
        "        \n",
        "    print('\\nData shape before removing outliers:', balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in previous data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "    print('-'*40)\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after removing outliers:', no_outliers.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(no_outliers.Label.value_counts())\n",
        "\n",
        "    # no_outliers.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distributions with Less Outliers', fontsize=17)\n",
        "    # plt.show()\n",
        "    \n",
        "    no_outliers.reset_index(drop=True, inplace=True)\n",
        "    return no_outliers\n",
        "\n",
        "\n",
        "def filter_features(no_outliers, threshold=0.001):\n",
        "    \"\"\"Feature selection.\n",
        "    \"\"\"\n",
        "    feat_sel = pd.DataFrame(no_outliers.copy())\n",
        "\n",
        "    # Make a dataframe with the label-correlations before removing outliers\n",
        "    # corr_change = pd.DataFrame()\n",
        "    # corr_change['correlation']= bal_corr.Label\n",
        "    # corr_change['origin']= 'w/outliers'\n",
        "\n",
        "    # Make a dataframe with label-correlations after removing outliers \n",
        "    # corr_other = pd.DataFrame()\n",
        "    # corr_other['correlation']= feat_sel.corr().Label\n",
        "    # corr_other['origin']= 'no_outliers'\n",
        "\n",
        "    # Join them\n",
        "    # corr_change = corr_change.append(corr_other)\n",
        "\n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.xticks(rotation=90)\n",
        "\n",
        "    # Plot them\n",
        "    # sns.set_style('darkgrid')\n",
        "    # plt.title('Label correlation per feature. With vs without outliers', fontsize=17)\n",
        "    # sns.barplot(data=corr_change,x=corr_change.index,y='correlation',hue='origin')\n",
        "    # plt.show()\n",
        "\n",
        "    # Feature Selection based on correlation with label\n",
        "\n",
        "    print('\\nData shape before feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes before feature selection:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # Correlation matrix after removing outliers\n",
        "    new_corr = feat_sel.corr()\n",
        "\n",
        "    for col in new_corr.Label.index[:-1]:\n",
        "        # Pick desired cutoff for dropping features. In absolute-value terms.\n",
        "        if np.absolute(new_corr.loc['Label',col]) < threshold:\n",
        "            # Drop the feature if correlation is below cutoff\n",
        "            feat_sel.drop(columns=col,inplace=True)\n",
        "\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in new data:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "\n",
        "    return feat_sel\n",
        "\n",
        "    # feat_sel.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distribution of Features Selected', fontsize=17)\n",
        "    # plt.show()\n",
        "\n",
        "def under_sampler(data, sample_size=20000, sampling=False):\n",
        "    # Undersample model for efficiency and balance classes.\n",
        "\n",
        "    X_train = data.drop('Label',1)\n",
        "    y_train = data.Label\n",
        "\n",
        "    if not sampling:\n",
        "      return X_train, y_train\n",
        "\n",
        "    # After feature-selection, X_test needs to include only the same features as X_train\n",
        "    # cols = X_train.columns\n",
        "    # X_test = X_test[cols]\n",
        "\n",
        "    # Undersample and balance classes\n",
        "    X_train, y_train = RandomUnderSampler(sampling_strategy={1:sample_size,0:sample_size}).fit_resample(X_train,y_train)\n",
        "\n",
        "    print('\\nX_train shape after reduction:', X_train.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in y_train:')\n",
        "    print(np.unique(y_train, return_counts=True))\n",
        "\n",
        "    return X_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pl5ZoepSNPf4",
        "colab": {}
      },
      "source": [
        "def gen_train_test_set(dt, feature_matrix, file_prefix='data'): \n",
        "    \n",
        "    # Subset labels\n",
        "    test_labels = feature_matrix.loc[feature_matrix['time'] == dt, 'Label']\n",
        "    train_labels = feature_matrix.loc[feature_matrix['time'] < dt, 'Label']\n",
        "\n",
        "    # Features\n",
        "    X_train = feature_matrix[feature_matrix['time'] < dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    X_test = feature_matrix[feature_matrix['time'] == dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    feature_names = list(X_train.columns)\n",
        "    \n",
        "    # Labels\n",
        "    y_train = np.array(train_labels).reshape((-1, ))\n",
        "    y_test = np.array(test_labels).reshape((-1, ))\n",
        "    \n",
        "    print('Training on {} observations.'.format(len(X_train)))\n",
        "    print('Testing on {} observations.\\n'.format(len(X_test)))\n",
        "\n",
        "    # Join the train data\n",
        "    train = X_train.join(train_labels)\n",
        "\n",
        "    print('Data shape before balancing:', train.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in previous data:')\n",
        "    print(train.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # sklearn pipeline\n",
        "    pipeline = make_pipeline(\n",
        "        SimpleImputer(strategy = 'constant', fill_value=0),\n",
        "        StandardScaler())\n",
        "    \n",
        "    X_train = pipeline.fit_transform(X_train)\n",
        "    X_test = pipeline.transform(X_test)\n",
        "\n",
        "    # imblearn pipeline\n",
        "    imb_pipeline = make_pipeline_imb(\n",
        "          # NearMiss(version=3, n_neighbors_ver3=3, n_jobs=4)\n",
        "          # SMOTE(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # ADASYN(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # OneSidedSelection(n_neighbors=1, n_seeds_S=200, random_state=42, n_jobs=4)\n",
        "          SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "    )\n",
        "     \n",
        "    # Balance the data\n",
        "    to_balanced = False\n",
        "    if to_balanced:\n",
        "      print('\\nBalancing data')\n",
        "      X_bal, y_bal = imb_pipeline.fit_resample(X_train, y_train)\n",
        "      X_bal = pd.DataFrame(X_bal,columns=feature_names)\n",
        "      y_bal = pd.DataFrame(y_bal,columns=['Label'])\n",
        "    else:\n",
        "      print('\\nNO balancing')\n",
        "      X_bal = pd.DataFrame(X_train,columns=feature_names)\n",
        "      y_bal = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    balanced = X_bal.join(y_bal)\n",
        "\n",
        "    # print('-'*40)\n",
        "    print('Data shape after balancing:',balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "\n",
        "    # Remove high correlation outliers\n",
        "    no_outliers = remove_outliers(balanced, remove=False)\n",
        "   \n",
        "    # Remove features with low correlation\n",
        "    remove_features = True\n",
        "    if remove_features:\n",
        "      print('\\nFiltering features')\n",
        "      features_selected = filter_features(no_outliers)\n",
        "    else:\n",
        "      print('\\nNO filtering')\n",
        "      features_selected = no_outliers \n",
        "\n",
        "    columns_selected = features_selected.drop('Label',1).columns\n",
        "\n",
        "    # Under sampling\n",
        "    X_train, y_train = under_sampler(features_selected, sampling=False) \n",
        "    X_train = pd.DataFrame(X_train,columns=columns_selected)\n",
        "    y_train = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    # For X_test, now only use the selected features\n",
        "    X_test = pd.DataFrame(X_test,columns=feature_names)\n",
        "    X_test = X_test[columns_selected]\n",
        "    y_test = pd.DataFrame(y_test,columns=['Label'])\n",
        "\n",
        "    #print(X_train.describe())\n",
        "    #return\n",
        "\n",
        "    # Save data\n",
        "    # print(X_train.head(10))\n",
        "    # print(y_train.head(10)) \n",
        "\n",
        "    # print(X_test.head(10))\n",
        "    # print(y_test.head(10)) \n",
        "    X_train.to_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "    y_train.to_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "   \n",
        "    X_test.to_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "    y_test.to_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "\n",
        "    gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PcKlL67TP9UM",
        "colab": {}
      },
      "source": [
        "def model(dt, feature_matrix, file_prefix='data', csv=False, class_weight=1.0, estimators=300, depth=3):\n",
        "    \"\"\"Predict for a particular month.\n",
        "\n",
        "    - https://www.kaggle.com/miguelniblock/optimizing-imbalanced-classification-100-recall\n",
        "    - https://www.kaggle.com/saxinou/imbalanced-data-xgboost-tunning\n",
        "    - https://www.kaggle.com/andreanuzzo/balance-the-imbalanced-rf-and-xgboost-with-smote\n",
        "    - https://github.com/mengwangk/FraudDetection/blob/master/05_Sampling_techniques_for_extremely_imbalanced_data.ipynb\n",
        "    - https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
        "    - https://github.com/coding-maniacs/over-under-sampling/blob/master/src/main.py\n",
        "    - https://github.com/scikit-learn-contrib/imbalanced-learn/issues/552#issuecomment-466348310\n",
        "    - https://stackoverflow.com/questions/52499788/smotetomek-how-to-set-ratio-as-dictionary-for-fixed-balance\n",
        "    - https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.OneSidedSelection.html#imblearn.under_sampling.OneSidedSelection\n",
        "    - https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
        "    - https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/\n",
        "    - https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "    - https://stats.stackexchange.com/questions/243207/what-is-the-proper-usage-of-scale-pos-weight-in-xgboost-for-imbalanced-datasets\n",
        "    - https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py\n",
        "    - https://machinelearningmastery.com/cost-sensitive-logistic-regression/\n",
        "    \n",
        "    - https://datascience.stackexchange.com/questions/28285/what-is-the-best-way-to-deal-with-imbalanced-data-for-xgboost/28292\n",
        "    - https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
        "    - https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Read data\n",
        "    if not csv:\n",
        "      X_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "      y_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "    \n",
        "      X_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "      y_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "    else:\n",
        "      X_train = pd.read_csv(DATASET_PATH/f\"{file_prefix}_X_train.csv\", header=0, sep=',', quotechar='\"')\n",
        "      y_train = pd.read_csv(DATASET_PATH/f\"{file_prefix}_y_train.csv\", header=0, sep=',', quotechar='\"')\n",
        "    \n",
        "      X_test = pd.read_csv(DATASET_PATH/f\"{file_prefix}_X_test.csv\", header=0, sep=',', quotechar='\"')\n",
        "      y_test = pd.read_csv(DATASET_PATH/f\"{file_prefix}_y_test.csv\", header=0, sep=',', quotechar='\"')\n",
        "\n",
        "\n",
        "    print(f'\\n-----------{dt}-----------------\\n')\n",
        "\n",
        "    # Reshape\n",
        "    y_train = np.array(y_train).reshape((-1, ))\n",
        "    y_test = np.array(y_test).reshape((-1, ))\n",
        "    \n",
        "    print('Data shape')\n",
        "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "    # print(X_train.describe())\n",
        "    # return\n",
        "\n",
        "    # Calculate hit ratio for xgboost classifier\n",
        "    print(\"\\nCalculating scale pos weight\")\n",
        "    counter = Counter(y_train)\n",
        "    print(Counter(y_train))\n",
        "    #scale_pos_weight = float(counter[0] / counter[1])\n",
        "    scale_pos_weight = (float(counter[0] / counter[1])) * class_weight\n",
        "    print(f\"\\nscale_pos_weight - {scale_pos_weight}\\n\")\n",
        "    \n",
        "     # Modeling\n",
        "#     clf = xgb.XGBClassifier(\n",
        "#               n_jobs=4, \n",
        "#               random_state=42,\n",
        "#               #learning_rate=0.1,\n",
        "#               #n_estimators=500,\n",
        "#               #max_depth=6, \n",
        "#               #min_child_weight=3, \n",
        "#               #gamma=0,\n",
        "#               #subsample=0.8,\n",
        "#               #colsample_bytree=0.8,\n",
        "#               objective='binary:logistic', \n",
        "#               scale_pos_weight=scale_pos_weight,\n",
        "#               ##eval_metric=\"auc\",\n",
        "#               ##max_delta_step=1,\n",
        "#               seed=27)\n",
        "#     clf = xgb.XGBClassifier(n_jobs=4, \n",
        "#                             random_state=42,\n",
        "#                             objective='binary:logistic', \n",
        "#                             #scale_pos_weight=28)\n",
        "#                             scale_pos_weight=scale_pos_weight)\n",
        "    \n",
        "    clf = xgb.XGBClassifier(\n",
        "                    n_jobs=4, \n",
        "                    random_state=42,\n",
        "                    objective='binary:logistic',\n",
        "                    subsample=0.55, \n",
        "                    #n_estimators=300,\n",
        "                    n_estimators=estimators,\n",
        "                    #n_estimators=550,\n",
        "                    min_child_weight=1,\n",
        "                    #max_depth=3, \n",
        "                    max_depth=depth, \n",
        "                    learning_rate=0.007,\n",
        "                    gamma=0.1, \n",
        "                    colsample_bytree=0.95,\n",
        "                    tree_method='hist',\n",
        "                    booster='dart',\n",
        "                    scale_pos_weight=scale_pos_weight\n",
        "                    )\n",
        "\n",
        "    clf_params = clf.get_params()\n",
        "    print(clf_params)\n",
        "\n",
        "    # Set parameters\n",
        "    #clf_params['max_depth'] = 10\n",
        "    #clf.set_params(clf_params)\n",
        "\n",
        "    # Parameters to compare\n",
        "    weights = [i for i in range(1,36,1)]\n",
        "    weights.append(scale_pos_weight)\n",
        "    learn_params = {\n",
        "        'n_estimators': [100, 300, 500, 800, 1000], \n",
        "        'max_depth': range(3,10,2),\n",
        "        'min_child_weight': range(1,6,2),\n",
        "        #'gamma':[i/10.0 for i in range(0,5)],\n",
        "        'subsample':[i/100.0 for i in range(55,70,5)],\n",
        "        'colsample_bytree':[i/100.0 for i in range(85,100,5)],\n",
        "        #'learning_rate':[i/1000.0 for i in range(5,20,2)],\n",
        "        'scale_pos_weight': weights\n",
        "    }\n",
        "    print(f'Parameter distribution: {learn_params}')\n",
        "    \n",
        "    # Test and validate\n",
        "    ret_val = score_optimization(dt,\n",
        "                       feature_matrix,\n",
        "                       clf, \n",
        "                       learn_params,  \n",
        "                       X_train, \n",
        "                       y_train, \n",
        "                       X_test, \n",
        "                       y_test, \n",
        "                       skip_grid_search_cv=True,\n",
        "                       optimized_scorer=True)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return ret_val\n",
        "    \n",
        "    # clf.fit(X_train, y_train)\n",
        "    # y_pred = clf.predict(X_test)\n",
        "\n",
        "    # # ROC score\n",
        "    # auc = roc_auc_score(y_test, y_pred)\n",
        "    # print(\"ROC score: \", auc)\n",
        "\n",
        "    # # Print confusion matrix\n",
        "    # conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    # sns.heatmap(conf_matrix, annot=True,fmt='d', cmap=plt.cm.copper)\n",
        "    # plt.show()\n",
        "\n",
        "    # Parameters to compare\n",
        "    # params = {\n",
        "    #     'criterion':['entropy','gini'],\n",
        "    #     'class_weight':[{1:1,0:0.3},{1:1,0:0.4},{1:1,0:0.5},{1:1,0:0.6},{1:1,0:7}]\n",
        "    # }\n",
        "\n",
        "    # Implement the classifier\n",
        "    # clf = RandomForestClassifier(\n",
        "    #     n_estimators=100,\n",
        "    #     max_features=None,\n",
        "    #     n_jobs=4,\n",
        "    # )\n",
        "\n",
        "    # # Test and validate\n",
        "    # score_optimization(clf, params, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m9UobqUWMI9b",
        "jupyter": {
          "source_hidden": true
        },
        "colab": {}
      },
      "source": [
        "# Predict for a particular month\n",
        "\n",
        "# %time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection)\n",
        "\n",
        "#%time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='test')\n",
        "#%time model(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='orig')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ns3Puh7Gnxl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8ed55ab-5855-42cb-f400-81fb3efd7471"
      },
      "source": [
        "target_mt = pd.datetime(2020,2,1)\n",
        "%time gen_train_test_set(target_mt, feature_matrix_selection, file_prefix='test')\n",
        "\n",
        "for (estimators, depth) in ((300,3), (500,6), (550,6), (600,6)):\n",
        "  %time model(target_mt, feature_matrix_selection, file_prefix='test', estimators=estimators, depth=depth)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 969893 observations.\n",
            "Testing on 10000 observations.\n",
            "\n",
            "Data shape before balancing: (969893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes in previous data:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "NO balancing\n",
            "Data shape after balancing: (969893, 63)\n",
            "\n",
            "Counts of strikes VS non-strikes in new data:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Filtering features\n",
            "\n",
            "Data shape before feature selection: (969893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes before feature selection:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "\n",
            "Data shape after feature selection: (969893, 36)\n",
            "\n",
            "Counts of strikes vs non-strikes in new data:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "CPU times: user 14.6 s, sys: 177 ms, total: 14.7 s\n",
            "Wall time: 21.5 s\n",
            "\n",
            "-----------2020-02-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(969893, 35) (969893,) (10000, 35) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 937508, 1: 32385})\n",
            "\n",
            "scale_pos_weight - 28.94883433688436\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 3, 'min_child_weight': 1, 'missing': None, 'n_estimators': 300, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.94883433688436, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.94883433688436]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.471, F-Score=0.06217\n",
            "\n",
            "Recall: 0.050473186119873815\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.97      0.93      0.05      0.95      0.22      0.05      9683\n",
            "          1       0.02      0.05      0.93      0.03      0.22      0.04       317\n",
            "\n",
            "avg / total       0.94      0.90      0.08      0.92      0.22      0.05     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3hU1b3/8XdmAkHUMUwgYcioGK0w\nKMcLUdqe8kNBDbUBDNSGX6qmR6v1koi2CtFKAgiFBGrVBuq1nlhFq7WABiWoWG2tIoioOCgawzWT\nBBKSCbdgZvb5gzYVw0RmJjtDNp+Xz34es757Z9b2yTMf11r7EmcYhoGIiEiEbLHugIiIdG8KEhER\niYqCREREoqIgERGRqChIREQkKgoSERGJSnyXftq707v04+TY0H9MSay7IBZU07i3835ZuN993w1z\n/xjr2iARETkWWfx2PU1tiYhIVDQiERExm8VHJAoSERGzWTtHFCQiIqaz+IhEayQiIhIVjUhERMxm\n8RGJgkRExGzWzhEFiYiI6TQiERGRqFg7RxQkIiKm04hERESioiAREZGoWDtHFCQiIqaz+IhENySK\niEhUNCIRETGbxUckChIREbNZO0cUJCIiprP4iERrJCIiEhWNSEREzGbxEYmCRETEbEFrB4mmtkRE\nzGaEuYXhjTfe4IorrmD8+PGMGzeOFStWAFBVVUV2djYZGRlkZ2ezadOmtmMirYWiIBERMZ05SWIY\nBlOmTKGkpISlS5dSUlLC1KlTCQaDFBUVkZOTQ0VFBTk5ORQWFrYdF2ktFAWJiIjZTByR2Gw2mpub\nAWhubiY5OZldu3bh9XrJzMwEIDMzE6/XS0NDA/X19RHVOqI1EhERs4W52O73+/H7/e3aHQ4HDoej\n7ee4uDjuv/9+br75Znr37s2ePXt45JFH8Pl8pKSkYLfbAbDb7SQnJ+Pz+TAMI6Ka0+kM2V8FiYiI\n2cIcZZSVlVFaWtquPS8vj/z8/LafW1tbefjhh1m4cCHDhg3j/fff57bbbqOkpCTaHodFQSIiYrrw\nkiQ3N5esrKx27V8fjQBs2LCBuro6hg0bBsCwYcM47rjjSEhIoLa2lkAggN1uJxAIUFdXh8vlwjCM\niGod0RqJiIjZwlwjcTgcuN3udts3g6R///7U1NTw5ZdfAlBZWUl9fT2nnnoqHo+H8vJyAMrLy/F4\nPDidTpKSkiKqdSTOMLrwTpl3p3fZR8mxo/+Yrh3Gy7GhpnFv5/2yJbeFt/8V9x/xri+++CKPPvoo\ncXFxANx6661ccsklVFZWUlBQgN/vx+FwUFxcTFpaGkDEtVAUJNLtKUjEDJ0aJIvDDJKsIw+So4HW\nSEREzKZHpIiISFQUJCIiEhVr54iCRETEdBYfkejyXxERiYpGJCIiZrP4iERBIiJiNmvniIJERMR0\nGpGIiEhUrJ0jChIREfNZO0kUJCIiZrN2jihIRERMF7R2kihIRETMZvHFdt2QKCIiUdGIRETEZOG+\nrSPOpH6YRUEiImKycGe2FCQiInKIrnx/YCwoSERETGZWjGzbto1bbrml7efm5mZ2797Ne++9R1VV\nFQUFBTQ2NpKYmEhxcTEDBw4EiLgWihbbRURMZhhGWNuRcrvdLF26tG0bPXo0mZmZABQVFZGTk0NF\nRQU5OTkUFha2HRdpLRQFiYiIyYJGeFskDhw4wEsvvcTEiROpr6/H6/W2hUpmZiZer5eGhoaIax3R\n1JaIiMnCXSLx+/34/f527Q6HA4fDcdhjVq5cSUpKCmeddRbr168nJSUFu90OgN1uJzk5GZ/Ph2EY\nEdWcTmfI/ipIRERMFgwzScrKyigtLW3XnpeXR35+/mGPeeGFF5g4cWJE/YuWgsRk23bsZsaTa1j3\nxU569rCTkX4yd//0fOLtNlZ+sJ37nv+Q7Tv3MOjkRGZdeyFnpJ4EHJxTvf+Fj/nr379kb0srQ07t\nQ+HV6XzHfbBe8Oi7lL+zmR7x/5mdXPPQROw2zVYey8ZP+DG/mno3bvfJ1NXVMvnmG1j1zj/Jufpn\n5N/+K5KTU1j17jvcnncjtTU+AHr27Mm9c+dzeeZY4nv0YPWqd5ly+63U+KpjfDbWEW6Q5ObmkpWV\n1a491GiktraW1atXU1JSAoDL5aK2tpZAIIDdbicQCFBXV4fL5cIwjIhqHdG3jslmPLmGJEcv/vFA\nFktmjmH1Z3Usev1zNtU0c8dD/2T6zy5g9R8mcvG5A7jp/rdoDQQBeOW9rbzw9y9Z9OtLeG/hBM49\nPYkpj7xzyO++7nIPHzxyZdumEDm2/b+LRjFtxixuu+UXnO5O5orLL2Pzpk18/wcjuLtwOj/L+QmD\nT0tl6+ZNPPT4/7Ydd/2Nt5B+4YVc/N/DOXfw6TQ17mJ2yW9jdyIWZBjhbQ6HA7fb3W4LFSSLFy9m\n5MiR9OnTB4CkpCQ8Hg/l5eUAlJeX4/F4cDqdEdc6om8ek23bsYcfXngKCT3t9Es8jh8MdfHF9ib+\n8bGP9EHJpJ/Zj3i7jet/NITaXftY/WndweN27mbYd/pycvIJ2G02xn3/NL6oborx2cjR7M677uG3\nJXNYu2Y1hmFQ46umxlfNpRk/5KUli/ns0w189dVX3DdvLt/77xGcOvA0AE45dSB/e/01du6oo6Wl\nhaV/fYFBgz0xPhtrMeuqrX9bvHhxu2mt6dOn89RTT5GRkcFTTz3FjBkzoq6FckRTW7t27aKmpgaA\n/v37t6WefLvcjEEse3czFw5Oxr/nAH//yMfkCUOp3bXvkD8Y41//bNzexPfO6s+Php/K8lVbqKrx\n4+57Aovf/pIRQw8dXj6z8nOeWfk57r7H84uxZ5FxwcldfXpylLDZbJxz3vlUvLKMd9Z+TEJCL5Yv\ne4mZhXcDEBf3n3ul//3vg4ecxeZNVSz6Uxn3zp1HSn8X/qZGJlyZzcrXVsTkPKzK7NsRKyoq2rWd\nfvrpPP/884fdP9JaKB0GyZYtW5g2bRper5fk5GQA6urqGDJkCDNmzAh5k0qoKw7cYXXNGi4Y1I/n\n3viCYTf+hUDQIOsHp3HJMDdf+pqZ/9w6Vm2o5bzv9OXRZRv4qjXI/pZWAPol9uL8M/sxZuoy7LY4\n+jt7U1Ywqu33Xn3pmUz9/+dx4nE9eHt9DbctfJu+J/Vi2Jn9YnWqEkP9klPo2bMnY8dnMf6Hl9L6\n1Vf876LnuO2OAla+9ioP/bGMsiceo6ryC3455S6CwSDHHXccAF9++QXV27fx4aeVtLa2ssH7CXff\n+csYn5G1hLtG0t10OLU1ZcoUJk6cyKpVq1i2bBnLli1j1apVTJgwgalTp4Y8rqysjNGjR7fbjjXB\noMHP5/+NS9NPZt0jV/Luggk07TnAvOfWcfoAB3Nv+C73/ul9Rty6hF3NLZwx4CRSnL0BWLDkE9ZX\nNfDm78bz0WM/Ie+Ks8mdu5J9/wqaswY66XNCAvF2GyPPGcDY7w3k1fe3xvJ0JYb279sHwOOP/IG6\n2hoaGup5eOGDjL4sg7+/+Qbz58zm8ScXsfqjDWzdspndzc34qrcDMGfe7+iZkMDggamkDejLyy8t\nZdFflsTydCwn3DWS7qbDIGlsbGTcuHHYvraIa7PZGD9+PE1Noefrc3Nzef3119ttx5rGPQeort/L\nVZd8h5497PQ5IYGJI9J468ODV8uMueAUyn9zOasWTuTWrKFs37mHoaclAfDpll38cPgp9Hf2Jt5u\nY8KINPx7D4RcJ4mje/4BSudoampk+7Zth06Xfu3v4YnHHub7w/6LoWeexrIXlxAfH8+nXi8AZw/9\nL/686CkaG3dx4MABHn/kD5yffgFOZ1JXn4Zlmb1GEmsdBkliYiLl5eXf+OM0ePHFF0NePQChrzg4\n1jhPTMDd73ieWfkFrYEg/j0HWPyPKgadnAjA+qoGAsEgDf79THviPUadl8rpAw7+dx16mpPl721h\nZ9M+gkGDJW9X0doa5NTkEwFYvnoLe/Z/RTBo8I+Pfbz4ziZGnZcaq1OVo8CfF/2J6264ib59+3HS\nSYnccFMer1a8QkJCAoM9QwBIdbuZ/0Apjz60kKamRgDWfbCWKyflcKLDQXx8PD+77gZ81dU0NNTH\n8nQsxeojkg7XSObOnUtRUREzZ84kJSUFOHi98uDBg5k7d26XdLC7K80fwW8WreXRZV5stji+60nh\nrpzzAJj99Pt8urWRHnYbYy44mYKc89uOu/5HQ6hvbuGKacvZ29LKqSkn8mD+D3Ac3xOAJ1ds5NeP\nv4dhGLj7ncCs/7mQ4Z6UmJyjHB3uK5mD05nE2+9/SMv+Fl5c8gIPzC8moVcvFj72BAMHprF7926e\nffpJimf/50qcGffcxazi+bzz/kf06NmTT71err1qUgzPxHqCFn9pe5xxBOOohoYGfL6D0zEul+tb\nrykO6d3pkR0n0oH+Y0pi3QWxoJrGvZ33u35/TVj7989/stM+uysc0eW/Tqcz8vAQETnGdcfpqnDo\nESkiIiYzLD61pSARETFZpI+G7y4UJCIiJuuOl/SGQ0EiImIyi+eIgkRExGwakYiISFSCse6AyRQk\nIiIm04hERESiYvEcUZCIiJhNIxIREYlKwOJBolftioiYzMyn/7a0tFBUVMRll13G2LFjmTZtGgBV\nVVVkZ2eTkZFBdnY2mzZtajsm0looChIREZOZ+T6SefPmkZCQQEVFBS+99BKTJ08GoKioiJycHCoq\nKsjJyaGwsLDtmEhroShIRERMFjTC247Unj17WLJkCZMnTyYuLg6Avn37Ul9fj9frJTMzE4DMzEy8\nXi8NDQ0R1zqiNRIREZOF+9BGv9+P3+9v1+5wOA55qeDWrVtJTEyktLSUVatWcfzxxzN58mR69epF\nSkoKdrsdALvdTnJyMj6fD8MwIqp19AR4BYmIiMnCXfcoKyujtLS0XXteXh75+fltPwcCAbZu3cqQ\nIUOYOnUqH374ITfeeCMPPPBAtF0Oi4JERMRk4a575ObmkpWV1a79m684d7lcxMfHt01FnXPOOfTp\n04devXpRW1tLIBDAbrcTCASoq6vD5XJhGEZEtY5ojURExGThXrXlcDhwu93ttm8GidPpZPjw4bz9\n9tvAwSuu6uvrGThwIB6Ph/LycgDKy8vxeDw4nU6SkpIiqnXkiF6122n0ql0xgV61K2bozFftfnDv\nj8Pa/7xpfznifbdu3crdd99NY2Mj8fHx3HbbbYwcOZLKykoKCgrw+/04HA6Ki4tJS0sDiLgWioJE\nuj0FiZihM4NkbZhBcn4YQXI00BqJiIjJghZ/RaKCRETEZEGLPyJFQSIiYjJrx4iCRETEdHr6r4iI\nRMXiOaIgERExm9ZIREQkKhbPEQWJiIjZwn1oY3ejIBERMZlGJCIiEhWtkYiISFQUJCIiEhWL54iC\nRETEbLohUUREomLxZzYqSEREzKYRiYiIRMXaMaIgERExnUYkIiISFTPXSEaNGkXPnj1JSEgA4I47\n7mDEiBGsW7eOwsJCWlpaSE1NZd68eSQlJQFEXAvFZt7piYgIHByRhLOF68EHH2Tp0qUsXbqUESNG\nEAwGufPOOyksLKSiooL09HTmz58PEHGtIwoSERGTGUZ4W7TWr19PQkIC6enpAEyaNInly5dHVeuI\nprZEREwWCDMd/H4/fr+/XbvD4cDhcLRrv+OOOzAMg2HDhvHLX/4Sn8/HgAED2upOp5NgMEhjY2PE\ntcTExJD9VZCIiJgs3OmqsrIySktL27Xn5eWRn59/SNvTTz+Ny+XiwIEDzJ49m5kzZ3LppZdG1d9w\nKUhEREwW7nRVbm4uWVlZ7doPNxpxuVwA9OzZk5ycHG666SauueYaqqur2/ZpaGjAZrORmJiIy+WK\nqNYRrZGIiJjMCPMfh8OB2+1ut30zSPbu3Utzc/PBzzAMXn75ZTweD2effTb79+9nzZo1ADz77LOM\nGTMGIOJaRzQiERExmVmX/9bX15Ofn08gECAYDHL66adTVFSEzWajpKSEoqKiQy7jBSKudSTO6Mo7\nZd6d3mUfJceO/mNKYt0FsaCaxr2d9rueywtvzeInpa922md3BY1IRERMZvEb2xUkIiJm0yNSREQk\nKtaOEQWJiIjp9KpdERGJisVzREEiImI2jUhERCQqCpJOdPzI2V35cXKM2HugNdZdEOmQxXNEIxIR\nEbPp8l8REYmKxXNEQSIiYragxe8kUZCIiJhMIxIREYmK1khERCQqFs8RBYmIiNm0RiIiIlEJmvVm\nq6OEgkRExGRWn9rSO9tFRExmGEZYWyRKS0sZNGgQGzduBGDdunWMGzeOjIwMrr32Wurr69v2jbQW\nioJERMRkwTC3cH3yySesW7eO1NTUg58XDHLnnXdSWFhIRUUF6enpzJ8/P6paRxQkIiImM3NEcuDA\nAWbOnMn06dPb2tavX09CQgLp6ekATJo0ieXLl0dV64jWSERETBbubJXf78fv97drdzgcOByOQ9oe\neOABxo0bh9vtbmvz+XwMGDCg7Wen00kwGKSxsTHiWmJiYsj+KkhEREwW7iijrKyM0tLSdu15eXnk\n5+e3/fzBBx+wfv167rjjjqj7GA0FiYiIycK9+jc3N5esrKx27d8cjaxevZrKykpGjx4NQE1NDddd\ndx1XX3011dXVbfs1NDRgs9lITEzE5XJFVOuIgkRExGRGmDckHm4K63BuuOEGbrjhhrafR40axUMP\nPcQZZ5zBc889x5o1a0hPT+fZZ59lzJgxAJx99tns378/7FpHFCQiIibr6vtIbDYbJSUlFBUV0dLS\nQmpqKvPmzYuq1pE4owufJnZ8Qo+u+ig5hugNiWKGzvxq/M2Vw8Pa/+7nV3XaZ3cFjUhEREwW7tRW\nd6MgERExmdUfkaIgERExmd5HIiIiUbH4w38VJCIiZtOIREREomLtGFGQiIiYTiMSERGJisVzREEi\nImK2oMWTREEiImIyBYmIiETF4jmiIBERMZtGJCIiEhWL54iCRETEbHpoo4iIREUjEhERiYrWSERE\nJCoWzxEFiYiI2cxcI7n55pvZtm0bNpuN3r17M23aNDweD1VVVRQUFNDY2EhiYiLFxcUMHDgQIOJa\nKHrVrnR7etWumKEzvxpvvWxoWPs/uOLjI963ubmZE088EYDXXnuNBQsWsHjxYq655homTpzI+PHj\nWbp0KS+88AJPPvkkQMS1UGxhnZ2IiIQtaBhhbeH4d4gA7N69m7i4OOrr6/F6vWRmZgKQmZmJ1+ul\noaEh4lpHNLUlImKyYJhvtvL7/fj9/nbtDocDh8PRrv3Xv/41b7/9NoZh8Nhjj+Hz+UhJScFutwNg\nt9tJTk7G5/NhGEZENafTGbK/ChIREZOFO0lWVlZGaWlpu/a8vDzy8/Pbtc+ePRuAJUuWUFJSwuTJ\nkyPpZsQUJCIiJgt3vSU3N5esrKx27YcbjXzdFVdcQWFhIf3796e2tpZAIIDdbicQCFBXV4fL5cIw\njIhqHdEaiYiIyYJGeJvD4cDtdrfbvhkke/bswefztf28cuVKTjrpJJKSkvB4PJSXlwNQXl6Ox+PB\n6XRGXOuIrtqSbk9XbYkZOvOr8fqLPGHt/+jfNhzRfjt37uTmm29m37592Gw2TjrpJKZOncpZZ51F\nZWUlBQUF+P1+HA4HxcXFpKWlAURcC0VBIt2egkTM0JlfjT8fGV6QPPbmkQXJ0UJrJCIiJtNDG0VE\nJCphXv3b7ShIRERM1oUrCDGhIBERMZnFc0RBIiJitoDFk0RBIiJiMk1tiYhIVCyeIwoSERGzaUQi\nIiJRCca6AybTs7a62ONPlFG5aQu+HfWsW/8Juf9zbVvtoosvZu1HH7NjVxMvV7zKyaec0labMPHH\nvP63t9ixq4lXVrwWi65LN3LLLbewevVq9u/fzxNPPHFI7bjjjmPBggXs2LGDxsZG3nzzzRj18thh\nGEZYW3ejIOli80uK8Zx5Bq5+Sfxk4gSKps/g3PPOJykpiUV/fp57p0/H3T+ZD9a+z5NPLWo7bteu\nBhb8/kF+O68khr2X7qK6uppZs2bxxz/+sV3tkUcewel0tj2M7/bbb49BD48thhHe1t1oaquLbdjg\nbfv3f//fR1paGuedfz4bvF4W//UFAGbfO5Mt1TWcOWgQGz/7jDdWrgQ4ZAQjEsrixYsBSE9Px+12\nt7UPGjSIcePG4Xa7aW5uBmDt2rUx6eOxpDuOMsKhEUkM/O7B37NjVxPrPv6EmpoaKpa/wpAhQ/j4\n44/a9tm7dy9VX1bi8QyJYU/Fai688EI2b97MjBkz2LFjBx999BETJkyIdbcszwhz624iDpKxY8eG\nrPn9frZt29Zuk4NuvzWflKQ+XHLxRSxdspiWlhaOP/4E/E1Nh+zX1OQ/5H3MItFyu90MHTqUpqYm\nBgwYQF5eHmVlZQwePDjWXbM0M9/ZfjTocGrriy++CFnbtWtXyFqo10TKfwSDQd7559tMysnh+l/c\nyJ49uznxGy+tcTgcbdMPIp1h3759HDhwgFmzZhEIBHjrrbd44403uOyyy/j0009j3T3LCved7d1N\nh0GSmZlJamrqYef3GhsbQx4X6jWRg04/LYIuWlu8PZ60tDS8Xi8/verqtvbevXtzWlraIWsqItH6\n6KOP2rVZff7+aGD1/8QdBklqaiqLFi0iJSWlXW3kyJEhj3M4HN/6buFjUb9+/Rh50cW88vIy9u3b\nx6jRo7kyO5ufXXMV7737LrPnzGX8FVksf+Vl7vr1Paz/+GM2fvYZADabjR49ehAfH4/NZiMhIYFA\nIEBrq17qJO3Z7Xbi4+Ox2+3Y7XYSEhJobW3lrbfeYsuWLdx1113MmTOH4cOHc/HFFzNlypRYd9nS\ngt1y5ePIdbhGctlll7F9+/bD1i699FJTOmRlhmHw8xt+wcYvN7G9dge/mVvClDt+xcvl5ezcuZOf\nTvoJRTNnsr12B+kXXEju1T9tOzbnp1fR4N/Ng6UL+MGIETT4d7PgDw/H8GzkaHbPPfewf/9+7rrr\nLq6++mr279/PPffcQ2trK+PHj+fyyy+nqamJRx99lGuuuYbP/vU/LGIOsy7/3bVrF9dffz0ZGRmM\nHTuWvLw8GhoaAFi3bh3jxo0jIyODa6+9lvr6+rbjIq2FolftSrenV+2KGTrzq/Hyc0759p2+5uUP\ntxzRfo2NjXz22WcMHz4cgOLiYpqampg1axYZGRnMmTOH9PR0Fi5cyNatW5kzZw7BYDCiWkd0+a+I\niMnMGpEkJia2hQjAueeeS3V1NevXrychIYH09HQAJk2axPLlywEirnVENySKiJgs3DUSv9+P3+9v\n197R+nMwGOSZZ55h1KhR+Hw+BgwY0FZzOp0Eg0EaGxsjriUmJobsr4JERMRk4c6ShbqFIi8vj/z8\n/MMec++999K7d2+uuuoqXn311Ui6GTEFiYiIycJdbwl1C0Wo0UhxcTGbN2/moYcewmaz4XK5qK6u\nbqs3NDRgs9lITEyMuNYRrZGIiJgs3DUSh8OB2+1utx0uSO677z7Wr1/PggUL6NmzJwBnn302+/fv\nZ82aNQA8++yzjBkzJqpaR3TVlnR7umpLzNCZX42jhqSGtf9K7+Fvu/imzz//nMzMTAYOHEivXr2A\ng4/BWbBgAWvXrqWoqIiWlhZSU1OZN28effv2BYi4FoqCRLo9BYmYoTO/Gi/yDPj2nb7mbxuqv32n\no4jWSERETHZMPyJFRESiZ/XnmSlIRERMZvGH/ypIRETMZlj8oY0KEhERk1l8ZktBIiJiNq2RiIhI\nVLRGIiIiUdEaiYiIRMXiM1sKEhERswUsPrelIBERMZkW20VEJCrWjhEFiYiI6TQiERGRqFh8iURB\nIiJiNo1IREQkKtaOEQWJiIjprD4i0TvbRURMFu4728NRXFzMqFGjGDRoEBs3bmxrr6qqIjs7m4yM\nDLKzs9m0aVPUtVAUJCIiJgsaRlhbOEaPHs3TTz9Nauqh74UvKioiJyeHiooKcnJyKCwsjLoWioJE\nRMRkZgZJeno6LpfrkLb6+nq8Xi+ZmZkAZGZm4vV6aWhoiLjWEa2RiIiYLNzpKr/fj9/vb9fucDhw\nOBzferzP5yMlJQW73Q6A3W4nOTkZn8+HYRgR1ZxOZ8jPU5CIiJgs3FFGWVkZpaWl7drz8vLIz8/v\nrG51GgWJiIjJwh2R5ObmkpWV1a79SEYjAC6Xi9raWgKBAHa7nUAgQF1dHS6XC8MwIqp1RGskIiIm\nM8L8x+Fw4Ha7221HGiRJSUl4PB7Ky8sBKC8vx+Px4HQ6I651JM7owgucj0/o0VUfJceQvQdaY90F\nsaDO/Go8te+JYe2/eWfzEe87a9YsVqxYwc6dO+nTpw+JiYksW7aMyspKCgoK8Pv9OBwOiouLSUtL\nA4i4FoqCRLo9BYmYoTO/Gk9OOiGs/bfW7+60z+4KWiMRETGZxW9sV5CIiJjN6o9IUZCIiJjM2jGi\nIBERMZ3e2S4iIlHR1JaIiETF4jmiIBERMZth8VUSBYmIiMksvkSiIBERMZvWSEREJCoWzxEFiYiI\n2bRGIiIiUdEaiYiIREVrJCIiEhWL54iCRETEbAGLJ4mCRETEZJraEhGRqFg8RxQkIiJm04hERESi\nEox1B0ymIBERMZnVRyRxhtXPsBvy+/2UlZWRm5uLw+GIdXfEIvR3JWaxxboD0p7f76e0tBS/3x/r\nroiF6O9KzKIgERGRqChIREQkKgoSERGJioJERESioiA5CjkcDvLy8nRljXQq/V2JWXT5r4iIREUj\nEhERiYqCREREoqIgOcpUVVWRnZ1NRkYG2dnZbNq0KdZdEgsoLi5m1KhRDBo0iI0bN8a6O2IxCpKj\nTFFRETk5OVRUVJCTk0NhYWGsuyQWMHr0aJ5++mlSU1Nj3RWxIAXJUaS+vh6v10tmZiYAmZmZeL1e\nGhoaYtwz6e7S09NxuVyx7oZYlILkKOLz+UhJScFutwNgt9tJTk7G5/PFuGciIqEpSEREJCoKkqOI\ny+WitraWQCAAQCAQoK6uTlMSInJUU5AcRZKSkvB4PJSXlwNQXl6Ox+PB6XTGuGciIqHpzvajTGVl\nJQUFBfj9fhwOB8XFxaSlpcW6W9LNzZo1ixUrVrBz50769OlDYmIiy5Yti3W3xCIUJCIiEhVNbYmI\nSFQUJCIiEhUFiYiIREVBIprZfM8AAAAgSURBVCIiUVGQiIhIVBQkIiISFQWJiIhERUEiIiJR+T8W\nmg0rWpYidwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0504732</td>\n",
              "      <td>0.927915</td>\n",
              "      <td>0.0259615</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity   Optimize\n",
              "XGBClassifier_optimize            0   0.0504732         0.927915  0.0259615"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 714 \n",
            "\n",
            "First 23 matches\n",
            "23 [  0  13  17  19  28  29  33  35  42  45  52  54  72  92 111 139 151 163\n",
            " 191 193 197 199 201]\n",
            "\n",
            "[0.50685877 0.5033251  0.50277746 0.51575655 0.5039137  0.5026578\n",
            " 0.50536865 0.5088764  0.50419843 0.500126   0.50590575 0.50413275\n",
            " 0.51133925 0.50342876 0.5018266  0.5011236  0.50322425 0.50321686\n",
            " 0.500613   0.50859404 0.50753075 0.5055681  0.50231284]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[1933 4465 8981 9595  905 3111 1230 2479 7836   19 1271 4562 5311  950\n",
            " 6114  823 1047  592 8938 8690 1949 5301 7455]\n",
            "\n",
            "\n",
            "[0.5153783  0.5177647  0.51862836 0.5163709  0.5159055  0.51613307\n",
            " 0.51648396 0.5168489  0.5159804  0.51575655 0.5192052  0.52193505\n",
            " 0.53189355 0.52347255 0.527189   0.5305465  0.52050555 0.53662175\n",
            " 0.52050716 0.5227675  0.524571   0.537097   0.52512646]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "192 [ 13  17  19  28  29  33  35  42  45  52  54  72  92 111 139 151 163 191\n",
            " 193 197 199 201 210 216 219 226 230 239 253 260 262 270 271 272 278 279\n",
            " 282 283 291 298 314 316 317 325 328 330 336 344 352 353 354 363 365 382\n",
            " 384 392 421 422 431 437 446 451 468 479 490 495 496 498 500 507 510 520\n",
            " 535 537 538 540 546 553 557 559 560 567 568 569 573 575 580 581 591 592\n",
            " 594 598 599 604 607 608 613 614 616 618 621 622 623 626 628 630 634 637\n",
            " 641 643 644 649 657 661 666 667 674 691 697 706 708 709 716 720 728 731\n",
            " 736 743 744 749 752 757 760 764 765 767 769 771 776 782 787 792 793 799\n",
            " 800 802 803 804 806 809 810 816 817 818 823 831 839 847 848 854 855 859\n",
            " 866 868 874 876 877 882 885 886 887 895 900 903 905 921 922 923 925 929\n",
            " 942 944 947 950 952 954 957 963 966 971 974 987]\n",
            "\n",
            "[0.5033251  0.50277746 0.51575655 0.5039137  0.5026578  0.50536865\n",
            " 0.5088764  0.50419843 0.500126   0.50590575 0.50413275 0.51133925\n",
            " 0.50342876 0.5018266  0.5011236  0.50322425 0.50321686 0.500613\n",
            " 0.50859404 0.50753075 0.5055681  0.50231284 0.5108487  0.50420636\n",
            " 0.50732    0.5026786  0.5048685  0.50104076 0.50174785 0.5032388\n",
            " 0.50017667 0.5028019  0.50254035 0.50752324 0.5042445  0.5064163\n",
            " 0.503481   0.5101219  0.5046015  0.50686884 0.5058063  0.50102687\n",
            " 0.5039492  0.5016582  0.5022074  0.50125    0.5031476  0.5066384\n",
            " 0.501196   0.5010004  0.504877   0.5008732  0.5024251  0.50066584\n",
            " 0.50330245 0.510123   0.5050188  0.5029317  0.50214326 0.5029084\n",
            " 0.501368   0.50204676 0.50291646 0.50660056 0.5021567  0.50587136\n",
            " 0.50277746 0.50875187 0.50490963 0.5097872  0.50185204 0.50125813\n",
            " 0.5062684  0.50630015 0.50188786 0.5025088  0.5023292  0.5050775\n",
            " 0.51296335 0.503987   0.5018754  0.5094072  0.505909   0.5005597\n",
            " 0.50053674 0.5069004  0.5028312  0.5014504  0.5022013  0.53662175\n",
            " 0.5081985  0.50265336 0.5016347  0.50648105 0.50337845 0.5042921\n",
            " 0.5022363  0.50067043 0.5020813  0.51073295 0.50491035 0.5099787\n",
            " 0.5039105  0.5061329  0.50059634 0.5072162  0.504721   0.50441766\n",
            " 0.5086733  0.5003537  0.502569   0.5007634  0.5022341  0.50659233\n",
            " 0.5019185  0.50046206 0.5013717  0.5031962  0.5025548  0.5076831\n",
            " 0.50901306 0.5008765  0.5026399  0.503215   0.50264984 0.50148565\n",
            " 0.5035591  0.5027905  0.5001812  0.5001145  0.5009595  0.5048831\n",
            " 0.50091046 0.50365406 0.50309324 0.5001872  0.50136065 0.50007206\n",
            " 0.50318944 0.50235283 0.50213635 0.50034124 0.5021423  0.50690556\n",
            " 0.50085557 0.50816476 0.5043129  0.50326693 0.5047201  0.5022856\n",
            " 0.5031094  0.50286025 0.50074977 0.5014732  0.5305465  0.50425845\n",
            " 0.51260334 0.5006182  0.5019607  0.50713736 0.5046649  0.50132513\n",
            " 0.50065    0.5007896  0.5036353  0.50088996 0.5026527  0.5009667\n",
            " 0.5015936  0.50396305 0.50344634 0.5144131  0.5002391  0.51384616\n",
            " 0.5159055  0.50494707 0.5070739  0.50782514 0.51303136 0.50276804\n",
            " 0.50237834 0.5045125  0.50329775 0.52347255 0.50771123 0.5040152\n",
            " 0.5019433  0.5033317  0.5059402  0.50086385 0.50840855 0.5026856 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.5044637322425842\n",
            "\n",
            "175 [   0   19   33   35   52   72  193  197  199  210  219  230  272  279\n",
            "  283  291  298  314  344  354  392  421  479  495  498  500  507  535\n",
            "  537  553  557  567  568  575  592  594  604  618  621  622  626  630\n",
            "  634  641  661  706  708  757  799  802  806  823  839  854  855  895\n",
            "  903  905  921  922  923  925  944  950  952  966  974 1009 1047 1050\n",
            " 1115 1126 1156 1198 1216 1221 1230 1231 1255 1271 1304 1320 1378 1438\n",
            " 1445 1508 1519 1553 1563 1577 1597 1671 1686 1773 1807 1933 1949 1950\n",
            " 1962 1979 2011 2029 2030 2036 2071 2144 2185 2190 2213 2255 2260 2451\n",
            " 2466 2479 2532 2533 2536 2609 2722 2759 2769 2958 3029 3034 3111 3138\n",
            " 3163 3183 3269 3350 3411 3830 3927 4012 4016 4060 4116 4339 4465 4562\n",
            " 4851 4855 4859 4923 5261 5301 5311 5378 5437 5464 6045 6114 6117 6326\n",
            " 6367 6439 6772 6874 7166 7231 7302 7455 7730 7836 8260 8360 8499 8690\n",
            " 8938 8981 9028 9192 9258 9431 9595]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.503632128238678\n",
            "\n",
            "235 [   0   19   28   33   35   42   52   54   72  193  197  199  210  216\n",
            "  219  230  272  278  279  283  291  298  314  317  344  354  392  421\n",
            "  479  495  498  500  507  535  537  553  557  559  567  568  575  592\n",
            "  594  604  608  618  621  622  623  626  630  634  637  641  661  706\n",
            "  708  757  764  799  802  803  806  823  831  839  854  855  874  886\n",
            "  895  903  905  921  922  923  925  944  950  952  954  966  974 1009\n",
            " 1017 1018 1039 1047 1050 1115 1126 1142 1156 1198 1216 1221 1230 1231\n",
            " 1255 1271 1304 1320 1378 1438 1445 1450 1474 1489 1502 1508 1519 1550\n",
            " 1553 1555 1563 1577 1597 1671 1686 1702 1773 1807 1811 1853 1933 1949\n",
            " 1950 1962 1979 2011 2029 2030 2036 2071 2144 2153 2185 2190 2213 2255\n",
            " 2260 2269 2276 2451 2466 2479 2532 2533 2536 2550 2603 2609 2701 2722\n",
            " 2759 2769 2936 2958 3029 3034 3081 3090 3091 3111 3138 3163 3183 3269\n",
            " 3350 3400 3411 3526 3733 3811 3830 3927 3988 4012 4016 4060 4116 4339\n",
            " 4465 4562 4851 4855 4859 4923 5045 5084 5261 5301 5311 5378 5437 5464\n",
            " 5523 5725 5820 6045 6114 6117 6326 6352 6367 6439 6772 6874 6997 7081\n",
            " 7166 7231 7302 7318 7402 7405 7455 7730 7836 8009 8031 8216 8260 8360\n",
            " 8499 8690 8938 8981 9028 9192 9258 9431 9595 9610 9895]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "714 [   0   13   17   19   28   29   33   35   42   45   52   54   72   92\n",
            "  111  139  151  163  191  193  197  199  201  210  216  219  226  230\n",
            "  239  253  260  262  270  271  272  278  279  282  283  291  298  314\n",
            "  316  317  325  328  330  336  344  352  353  354  363  365  382  384\n",
            "  392  421  422  431  437  446  451  468  479  490  495  496  498  500\n",
            "  507  510  520  535  537  538  540  546  553  557  559  560  567  568\n",
            "  569  573  575  580  581  591  592  594  598  599  604  607  608  613\n",
            "  614  616  618  621  622  623  626  628  630  634  637  641  643  644\n",
            "  649  657  661  666  667  674  691  697  706  708  709  716  720  728\n",
            "  731  736  743  744  749  752  757  760  764  765  767  769  771  776\n",
            "  782  787  792  793  799  800  802  803  804  806  809  810  816  817\n",
            "  818  823  831  839  847  848  854  855  859  866  868  874  876  877\n",
            "  882  885  886  887  895  900  903  905  921  922  923  925  929  942\n",
            "  944  947  950  952  954  957  963  966  971  974  987 1009 1016 1017\n",
            " 1018 1022 1027 1029 1030 1031 1039 1044 1046 1047 1050 1055 1061 1064\n",
            " 1071 1083 1085 1088 1102 1103 1112 1115 1126 1127 1142 1152 1156 1174\n",
            " 1175 1178 1198 1207 1216 1221 1228 1230 1231 1255 1256 1260 1271 1279\n",
            " 1284 1296 1298 1304 1308 1315 1316 1320 1321 1334 1336 1348 1349 1353\n",
            " 1354 1360 1362 1378 1399 1412 1414 1423 1431 1435 1438 1445 1450 1454\n",
            " 1456 1463 1467 1474 1475 1489 1502 1506 1508 1516 1519 1531 1534 1550\n",
            " 1553 1554 1555 1557 1563 1571 1577 1580 1581 1585 1597 1631 1649 1651\n",
            " 1655 1671 1684 1686 1689 1695 1700 1702 1710 1723 1736 1747 1748 1754\n",
            " 1766 1773 1777 1789 1807 1810 1811 1828 1830 1832 1834 1853 1857 1862\n",
            " 1867 1879 1884 1888 1896 1905 1907 1914 1933 1942 1949 1950 1957 1962\n",
            " 1979 1984 1992 2011 2019 2029 2030 2036 2050 2053 2056 2064 2071 2075\n",
            " 2093 2097 2105 2107 2115 2144 2145 2153 2157 2158 2172 2178 2185 2188\n",
            " 2190 2196 2209 2213 2241 2243 2248 2254 2255 2260 2266 2268 2269 2275\n",
            " 2276 2278 2302 2303 2325 2326 2331 2346 2353 2376 2398 2399 2414 2415\n",
            " 2418 2425 2439 2441 2446 2449 2451 2461 2466 2474 2479 2485 2495 2497\n",
            " 2502 2509 2520 2532 2533 2536 2550 2560 2571 2603 2609 2628 2633 2679\n",
            " 2701 2704 2722 2723 2749 2759 2760 2765 2769 2786 2842 2860 2865 2869\n",
            " 2872 2879 2889 2897 2936 2952 2958 2989 3003 3009 3010 3018 3029 3030\n",
            " 3034 3043 3081 3086 3090 3091 3099 3111 3113 3138 3139 3152 3156 3163\n",
            " 3171 3183 3209 3251 3254 3266 3269 3283 3306 3319 3334 3350 3356 3387\n",
            " 3400 3411 3428 3436 3448 3468 3482 3492 3494 3521 3526 3545 3591 3600\n",
            " 3613 3618 3624 3632 3675 3710 3732 3733 3772 3781 3791 3805 3811 3816\n",
            " 3828 3830 3845 3879 3890 3920 3927 3931 3932 3935 3968 3988 4012 4016\n",
            " 4036 4043 4044 4053 4060 4080 4098 4116 4123 4166 4242 4253 4257 4282\n",
            " 4315 4321 4336 4339 4369 4373 4465 4472 4473 4479 4514 4562 4578 4592\n",
            " 4609 4661 4669 4725 4753 4763 4780 4789 4826 4849 4851 4855 4859 4923\n",
            " 4932 4974 5009 5032 5045 5067 5070 5084 5087 5092 5213 5237 5240 5261\n",
            " 5294 5301 5309 5311 5331 5338 5340 5365 5371 5378 5426 5430 5437 5440\n",
            " 5442 5464 5506 5521 5523 5587 5631 5648 5709 5725 5726 5792 5797 5820\n",
            " 5825 5899 5960 6019 6045 6055 6092 6094 6114 6117 6131 6156 6215 6241\n",
            " 6281 6326 6352 6367 6379 6408 6439 6447 6451 6453 6481 6510 6586 6603\n",
            " 6620 6742 6749 6772 6806 6874 6918 6926 6945 6960 6962 6992 6997 7058\n",
            " 7081 7097 7136 7139 7143 7144 7160 7166 7222 7227 7231 7249 7302 7318\n",
            " 7320 7339 7397 7400 7402 7405 7433 7455 7486 7495 7513 7532 7649 7682\n",
            " 7730 7836 7904 7936 7956 8009 8031 8051 8078 8186 8216 8260 8360 8490\n",
            " 8499 8524 8690 8697 8701 8919 8938 8981 8983 9028 9110 9122 9192 9254\n",
            " 9258 9282 9364 9431 9465 9595 9610 9671 9692 9702 9755 9766 9895 9963]\n",
            "\n",
            "714 [0.50685877 0.5033251  0.50277746 0.51575655 0.5039137  0.5026578\n",
            " 0.50536865 0.5088764  0.50419843 0.500126   0.50590575 0.50413275\n",
            " 0.51133925 0.50342876 0.5018266  0.5011236  0.50322425 0.50321686\n",
            " 0.500613   0.50859404 0.50753075 0.5055681  0.50231284 0.5108487\n",
            " 0.50420636 0.50732    0.5026786  0.5048685  0.50104076 0.50174785\n",
            " 0.5032388  0.50017667 0.5028019  0.50254035 0.50752324 0.5042445\n",
            " 0.5064163  0.503481   0.5101219  0.5046015  0.50686884 0.5058063\n",
            " 0.50102687 0.5039492  0.5016582  0.5022074  0.50125    0.5031476\n",
            " 0.5066384  0.501196   0.5010004  0.504877   0.5008732  0.5024251\n",
            " 0.50066584 0.50330245 0.510123   0.5050188  0.5029317  0.50214326\n",
            " 0.5029084  0.501368   0.50204676 0.50291646 0.50660056 0.5021567\n",
            " 0.50587136 0.50277746 0.50875187 0.50490963 0.5097872  0.50185204\n",
            " 0.50125813 0.5062684  0.50630015 0.50188786 0.5025088  0.5023292\n",
            " 0.5050775  0.51296335 0.503987   0.5018754  0.5094072  0.505909\n",
            " 0.5005597  0.50053674 0.5069004  0.5028312  0.5014504  0.5022013\n",
            " 0.53662175 0.5081985  0.50265336 0.5016347  0.50648105 0.50337845\n",
            " 0.5042921  0.5022363  0.50067043 0.5020813  0.51073295 0.50491035\n",
            " 0.5099787  0.5039105  0.5061329  0.50059634 0.5072162  0.504721\n",
            " 0.50441766 0.5086733  0.5003537  0.502569   0.5007634  0.5022341\n",
            " 0.50659233 0.5019185  0.50046206 0.5013717  0.5031962  0.5025548\n",
            " 0.5076831  0.50901306 0.5008765  0.5026399  0.503215   0.50264984\n",
            " 0.50148565 0.5035591  0.5027905  0.5001812  0.5001145  0.5009595\n",
            " 0.5048831  0.50091046 0.50365406 0.50309324 0.5001872  0.50136065\n",
            " 0.50007206 0.50318944 0.50235283 0.50213635 0.50034124 0.5021423\n",
            " 0.50690556 0.50085557 0.50816476 0.5043129  0.50326693 0.5047201\n",
            " 0.5022856  0.5031094  0.50286025 0.50074977 0.5014732  0.5305465\n",
            " 0.50425845 0.51260334 0.5006182  0.5019607  0.50713736 0.5046649\n",
            " 0.50132513 0.50065    0.5007896  0.5036353  0.50088996 0.5026527\n",
            " 0.5009667  0.5015936  0.50396305 0.50344634 0.5144131  0.5002391\n",
            " 0.51384616 0.5159055  0.50494707 0.5070739  0.50782514 0.51303136\n",
            " 0.50276804 0.50237834 0.5045125  0.50329775 0.52347255 0.50771123\n",
            " 0.5040152  0.5019433  0.5033317  0.5059402  0.50086385 0.50840855\n",
            " 0.5026856  0.50841856 0.50154334 0.503787   0.50427    0.50044906\n",
            " 0.5017425  0.5004651  0.5000313  0.5014536  0.5043593  0.50357246\n",
            " 0.50033087 0.52050555 0.5045967  0.50156444 0.5007373  0.5003508\n",
            " 0.5024047  0.5029101  0.5004017  0.5016965  0.500803   0.5023658\n",
            " 0.5025465  0.5057849  0.50451374 0.5013619  0.5038482  0.5002992\n",
            " 0.5046192  0.5028597  0.5029118  0.5014137  0.5055436  0.50127476\n",
            " 0.50495905 0.50791967 0.502195   0.51648396 0.5073899  0.5053183\n",
            " 0.50281    0.5028662  0.5192052  0.50049454 0.5009887  0.50339013\n",
            " 0.5012148  0.50483656 0.50359565 0.50169754 0.5015026  0.506712\n",
            " 0.50117624 0.50200665 0.5006097  0.50215465 0.50047284 0.50154465\n",
            " 0.50029176 0.50218636 0.5000992  0.5064659  0.50146514 0.5002631\n",
            " 0.5028232  0.50334513 0.50314957 0.5001669  0.5061792  0.50608784\n",
            " 0.5044343  0.501712   0.5011323  0.5003228  0.50013816 0.5036404\n",
            " 0.50299054 0.50380576 0.503767   0.50066376 0.50985557 0.50355405\n",
            " 0.50735563 0.502148   0.50264233 0.50403106 0.507069   0.501775\n",
            " 0.5038209  0.5021604  0.5057468  0.50157857 0.50446415 0.50064814\n",
            " 0.5001455  0.50045216 0.5082264  0.50105876 0.5013851  0.50204676\n",
            " 0.50139046 0.50732994 0.50336635 0.5068786  0.5021466  0.50268596\n",
            " 0.50288314 0.5038956  0.5024081  0.5020695  0.50113016 0.50269634\n",
            " 0.5018927  0.50085896 0.5008103  0.50577927 0.5002313  0.5010563\n",
            " 0.50624555 0.502441   0.50399    0.5005074  0.50244    0.5003694\n",
            " 0.50012034 0.5042389  0.5006541  0.5014113  0.50166655 0.502483\n",
            " 0.50292563 0.5003188  0.50232285 0.5002798  0.5035721  0.5020565\n",
            " 0.5153783  0.50211465 0.524571   0.5044686  0.5013608  0.50524765\n",
            " 0.5046865  0.50122195 0.5025193  0.5081406  0.501375   0.5085194\n",
            " 0.50964576 0.5049924  0.5010123  0.5003502  0.50237066 0.5029011\n",
            " 0.5048311  0.5024367  0.5003247  0.5017004  0.5021941  0.5032726\n",
            " 0.5012788  0.50811297 0.5018194  0.50390345 0.5018206  0.5024254\n",
            " 0.50308853 0.50214136 0.5098019  0.5002953  0.505823   0.50123954\n",
            " 0.5031589  0.50471187 0.5005304  0.50272954 0.5001359  0.5005849\n",
            " 0.50643766 0.5078951  0.5019198  0.5008998  0.50437343 0.5016336\n",
            " 0.5040752  0.50145745 0.5014729  0.50083345 0.50079805 0.5005628\n",
            " 0.5006692  0.50126743 0.50130713 0.5019525  0.50195044 0.5006651\n",
            " 0.5008894  0.5006941  0.5000487  0.5029745  0.5019373  0.5026598\n",
            " 0.50070626 0.50192106 0.5104835  0.50291026 0.50496185 0.50039285\n",
            " 0.5168489  0.5031465  0.50002986 0.50197697 0.50111264 0.50059783\n",
            " 0.5003783  0.5094527  0.5051587  0.50597143 0.50423205 0.50181186\n",
            " 0.502836   0.50372857 0.50792176 0.5010226  0.502797   0.50244325\n",
            " 0.5040057  0.5016804  0.5066852  0.5015866  0.5007022  0.50808614\n",
            " 0.50355387 0.50181586 0.5059085  0.50167674 0.50118715 0.5004774\n",
            " 0.50212264 0.50242746 0.50067425 0.50085735 0.50163484 0.50027454\n",
            " 0.504079   0.50220376 0.5057851  0.50101423 0.5014687  0.50012004\n",
            " 0.5035923  0.5013094  0.5050372  0.5000697  0.50481236 0.501347\n",
            " 0.5037575  0.500818   0.50428826 0.504439   0.5007044  0.51613307\n",
            " 0.5014867  0.5050195  0.5035706  0.50018656 0.50037724 0.50472975\n",
            " 0.50355995 0.5062454  0.5006811  0.5007373  0.5029009  0.5017483\n",
            " 0.50595    0.5000145  0.50360644 0.5023081  0.50213397 0.50800973\n",
            " 0.5019753  0.5002658  0.504204   0.50906575 0.50250447 0.5016806\n",
            " 0.5011264  0.50186807 0.5033243  0.5015433  0.5031763  0.50264204\n",
            " 0.50379616 0.50089395 0.50361997 0.50345963 0.5023287  0.50204176\n",
            " 0.5027799  0.50030375 0.5007931  0.50090325 0.5000773  0.504206\n",
            " 0.5011725  0.50072235 0.50258756 0.5025913  0.50363874 0.5012729\n",
            " 0.5001984  0.5046513  0.5004545  0.50011605 0.5001905  0.5000942\n",
            " 0.5053113  0.5029039  0.50082666 0.5024229  0.5027096  0.5041277\n",
            " 0.50489664 0.50509083 0.5024161  0.50038844 0.5019473  0.500238\n",
            " 0.50613594 0.5011014  0.5011082  0.5045728  0.5029831  0.5010353\n",
            " 0.50273365 0.50063616 0.50095147 0.50036114 0.50125617 0.50025463\n",
            " 0.5005453  0.5057319  0.50153106 0.5021447  0.5177647  0.5011362\n",
            " 0.50185204 0.5028946  0.5008979  0.52193505 0.5031183  0.50002927\n",
            " 0.50115204 0.50015664 0.5027212  0.5012342  0.50120145 0.5030948\n",
            " 0.5011489  0.5019517  0.5002654  0.5006439  0.5067328  0.5131891\n",
            " 0.50645703 0.50890857 0.50157475 0.5003626  0.5003599  0.5003415\n",
            " 0.5039153  0.50225407 0.5010567  0.50442874 0.5017869  0.5012431\n",
            " 0.50080997 0.5016814  0.50263244 0.5048933  0.5018826  0.537097\n",
            " 0.5013993  0.53189355 0.50147563 0.5021643  0.5007052  0.50057423\n",
            " 0.5028453  0.5125847  0.50006    0.50045    0.5052608  0.5008791\n",
            " 0.5025042  0.50677186 0.50256246 0.5019234  0.50363433 0.50318336\n",
            " 0.50022817 0.50260264 0.50048745 0.5042128  0.5014091  0.5007672\n",
            " 0.50056493 0.50372136 0.5014009  0.50022995 0.5017133  0.50162965\n",
            " 0.5069014  0.5010886  0.5008286  0.50050515 0.527189   0.5048404\n",
            " 0.5002506  0.50124586 0.5019663  0.50059927 0.500865   0.5110358\n",
            " 0.5044396  0.505342   0.50078917 0.5008489  0.50639915 0.50153804\n",
            " 0.50151837 0.5011308  0.50084245 0.5023073  0.502385   0.50181115\n",
            " 0.50348246 0.5002157  0.5018016  0.50518894 0.5034153  0.5083002\n",
            " 0.5016356  0.50174415 0.5027394  0.50305176 0.50291026 0.5007446\n",
            " 0.5037645  0.50078607 0.50380087 0.5026269  0.5002395  0.5034211\n",
            " 0.5025038  0.50007045 0.5003852  0.50641966 0.50224906 0.50030273\n",
            " 0.50652725 0.50165117 0.50765485 0.50393957 0.5020605  0.5026991\n",
            " 0.50079936 0.50023866 0.50416493 0.5038186  0.5018578  0.52512646\n",
            " 0.50347495 0.50130194 0.5003486  0.50230217 0.5005666  0.5010101\n",
            " 0.50471824 0.5159804  0.5022859  0.5005313  0.50036585 0.5044589\n",
            " 0.5037253  0.5007172  0.50290716 0.503269   0.50401413 0.5051937\n",
            " 0.5061917  0.5008115  0.51249105 0.5018159  0.5227675  0.50113094\n",
            " 0.50125545 0.50199115 0.52050716 0.51862836 0.50170404 0.5047022\n",
            " 0.5014479  0.5011994  0.50455844 0.50082594 0.5134311  0.50356054\n",
            " 0.5007379  0.5063558  0.5029683  0.5163709  0.5039889  0.50257957\n",
            " 0.5001333  0.502546   0.503238   0.5002408  0.5037342  0.50285476]\n",
            "\n",
            "Matched draws\n",
            "Count: 16, Index: (array([ 392,  422, 1228, 1284, 1467, 1475, 1519, 1789, 1884, 2030, 2145,\n",
            "       2889, 3400, 3927, 4116, 6439]),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "107279  507220 2020-02-08  ConsolationNo4      422\n",
            "107356  507520 2020-02-15      SpecialNo2      392\n",
            "\n",
            "\n",
            "All matched\n",
            "        DrawNo   DrawDate        PrizeType  LuckyNo\n",
            "107187  506820 2020-02-01   ConsolationNo4     1789\n",
            "107199  506820 2020-02-01       SpecialNo6     2030\n",
            "107206  506920 2020-02-02   ConsolationNo1     1519\n",
            "107216  506920 2020-02-02       SpecialNo1     1467\n",
            "107223  506920 2020-02-02       SpecialNo7     1284\n",
            "107243  507020 2020-02-04       SpecialNo4     1884\n",
            "107279  507220 2020-02-08   ConsolationNo4      422\n",
            "107289  507220 2020-02-08       SpecialNo4     4116\n",
            "107356  507520 2020-02-15       SpecialNo2      392\n",
            "107365  507620 2020-02-16       2ndPrizeNo     1228\n",
            "107385  507620 2020-02-16       SpecialNo8     3400\n",
            "107402  507720 2020-02-19       SpecialNo2     2145\n",
            "107414  507820 2020-02-22  ConsolationNo10     2889\n",
            "107456  508020 2020-02-26       1stPrizeNo     3927\n",
            "107466  508020 2020-02-26   ConsolationNo7     1475\n",
            "107475  508020 2020-02-26       SpecialNo6     6439\n",
            "CPU times: user 35min 19s, sys: 1.14 s, total: 35min 20s\n",
            "Wall time: 9min 5s\n",
            "\n",
            "-----------2020-02-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(969893, 35) (969893,) (10000, 35) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 937508, 1: 32385})\n",
            "\n",
            "scale_pos_weight - 28.94883433688436\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.94883433688436, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.94883433688436]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.444, F-Score=0.06231\n",
            "\n",
            "Recall: 0.0473186119873817\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.97      0.95      0.05      0.96      0.21      0.05      9683\n",
            "          1       0.03      0.05      0.95      0.04      0.21      0.04       317\n",
            "\n",
            "avg / total       0.94      0.92      0.08      0.93      0.21      0.05     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0473186</td>\n",
              "      <td>0.949189</td>\n",
              "      <td>0.0276927</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity   Optimize\n",
              "XGBClassifier_optimize            0   0.0473186         0.949189  0.0276927"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 507 \n",
            "\n",
            "First 23 matches\n",
            "23 [ 28  50  72 113 197 201 210 219 262 283 291 316 344 382 384 391 392 412\n",
            " 414 451 496 500 507]\n",
            "\n",
            "[0.51009935 0.50707376 0.52511775 0.5103161  0.5096816  0.50290906\n",
            " 0.52166325 0.5057999  0.50037026 0.52010703 0.50520575 0.5052009\n",
            " 0.5099394  0.50040394 0.5009632  0.5014484  0.5076987  0.5005305\n",
            " 0.5055942  0.50070333 0.50688237 0.5043319  0.5140048 ]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[ 567 7294 6772  592   72  950 5311  568 5194 6763 8499 1597  895 1671\n",
            " 8938 5464 7231 7320 2144  210 2466  708 6045]\n",
            "\n",
            "\n",
            "[0.5202629  0.5204476  0.5205282  0.5342018  0.52511775 0.5207474\n",
            " 0.53445846 0.5219588  0.52074546 0.5245611  0.5303273  0.52211416\n",
            " 0.53116584 0.52266955 0.5311582  0.5233817  0.5235255  0.52357334\n",
            " 0.52992904 0.52166325 0.52281314 0.52546114 0.5231969 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "84 [ 28  50  72 113 197 201 210 219 262 283 291 316 344 382 384 391 392 412\n",
            " 414 451 496 500 507 519 533 535 537 557 559 567 568 592 594 598 604 607\n",
            " 608 612 618 621 626 630 641 644 661 669 691 708 724 728 731 746 752 764\n",
            " 773 774 792 798 799 802 806 813 818 841 847 866 874 895 903 911 921 922\n",
            " 923 931 938 947 950 951 954 957 963 974 979 994]\n",
            "\n",
            "[0.51009935 0.50707376 0.52511775 0.5103161  0.5096816  0.50290906\n",
            " 0.52166325 0.5057999  0.50037026 0.52010703 0.50520575 0.5052009\n",
            " 0.5099394  0.50040394 0.5009632  0.5014484  0.5076987  0.5005305\n",
            " 0.5055942  0.50070333 0.50688237 0.5043319  0.5140048  0.511491\n",
            " 0.50723463 0.5014904  0.50340086 0.5097363  0.5010234  0.5202629\n",
            " 0.5219588  0.5342018  0.5101498  0.5074004  0.5146329  0.51666385\n",
            " 0.5091416  0.5044585  0.5069854  0.505089   0.50103325 0.500648\n",
            " 0.50996053 0.5057549  0.50171196 0.5132908  0.5069667  0.52546114\n",
            " 0.5007276  0.5176572  0.5005729  0.5038772  0.5050035  0.5155955\n",
            " 0.50239515 0.5078863  0.51402706 0.5095712  0.5102049  0.5145725\n",
            " 0.5066194  0.50706035 0.5016752  0.50051534 0.5053159  0.5014648\n",
            " 0.51346296 0.53116584 0.51070756 0.5022638  0.5010228  0.5099397\n",
            " 0.52013034 0.51329505 0.50386804 0.5040512  0.5207474  0.50279593\n",
            " 0.51369643 0.5069404  0.5011688  0.50191545 0.5139398  0.5079651 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.508690595626831\n",
            "\n",
            "133 [  28   72  113  197  210  283  344  507  519  557  567  568  592  594\n",
            "  604  607  608  641  669  708  728  764  792  798  799  802  874  895\n",
            "  903  922  923  931  950  954  979 1034 1044 1053 1071 1115 1156 1172\n",
            " 1176 1178 1187 1198 1247 1260 1308 1354 1377 1387 1457 1475 1508 1553\n",
            " 1571 1597 1671 1684 1754 1815 1884 2030 2071 2075 2097 2144 2173 2185\n",
            " 2414 2466 2500 2617 2641 2750 2936 2958 3091 3102 3266 3356 3416 3499\n",
            " 3545 3675 3692 4016 4071 4166 4197 4373 4479 4554 4694 4725 4859 4880\n",
            " 4882 4941 4976 5194 5311 5331 5371 5442 5464 5725 5797 5813 6045 6379\n",
            " 6763 6772 6960 6968 6997 7081 7100 7136 7231 7294 7320 7682 8009 8051\n",
            " 8186 8295 8499 8524 8938 9028 9915]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.5066543817520142\n",
            "\n",
            "189 [  28   50   72  113  197  210  283  344  392  496  507  519  533  557\n",
            "  567  568  592  594  598  604  607  608  618  641  669  691  708  728\n",
            "  764  774  792  798  799  802  813  874  895  903  922  923  931  950\n",
            "  954  957  979  994 1034 1044 1053 1071 1102 1115 1131 1156 1172 1176\n",
            " 1178 1187 1198 1247 1260 1298 1308 1316 1354 1377 1387 1457 1475 1508\n",
            " 1546 1553 1571 1597 1631 1671 1684 1702 1754 1807 1815 1879 1884 1888\n",
            " 2020 2030 2071 2075 2097 2144 2173 2185 2259 2269 2271 2399 2414 2420\n",
            " 2446 2466 2500 2532 2550 2607 2617 2641 2694 2750 2820 2892 2936 2958\n",
            " 3003 3035 3057 3091 3102 3266 3356 3416 3442 3499 3545 3675 3692 3733\n",
            " 3781 3811 3816 4016 4071 4166 4197 4373 4479 4492 4554 4587 4694 4725\n",
            " 4859 4880 4882 4941 4976 5193 5194 5311 5331 5371 5442 5464 5506 5545\n",
            " 5725 5796 5797 5813 5913 6045 6379 6763 6772 6779 6894 6918 6960 6968\n",
            " 6997 7081 7100 7136 7231 7294 7320 7682 8009 8051 8186 8295 8499 8524\n",
            " 8927 8938 9028 9223 9465 9755 9915]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "507 [  28   50   72  113  197  201  210  219  262  283  291  316  344  382\n",
            "  384  391  392  412  414  451  496  500  507  519  533  535  537  557\n",
            "  559  567  568  592  594  598  604  607  608  612  618  621  626  630\n",
            "  641  644  661  669  691  708  724  728  731  746  752  764  773  774\n",
            "  792  798  799  802  806  813  818  841  847  866  874  895  903  911\n",
            "  921  922  923  931  938  947  950  951  954  957  963  974  979  994\n",
            " 1005 1008 1027 1029 1034 1044 1050 1053 1058 1071 1074 1100 1101 1102\n",
            " 1115 1126 1131 1142 1148 1156 1163 1172 1176 1177 1178 1187 1198 1215\n",
            " 1240 1247 1256 1260 1271 1279 1294 1295 1298 1304 1308 1316 1343 1347\n",
            " 1354 1362 1374 1377 1387 1399 1409 1415 1441 1454 1457 1475 1508 1519\n",
            " 1546 1550 1553 1555 1557 1558 1571 1580 1581 1597 1622 1631 1635 1650\n",
            " 1655 1658 1661 1671 1684 1702 1718 1721 1723 1751 1754 1771 1784 1788\n",
            " 1807 1813 1815 1830 1854 1879 1880 1884 1888 1927 1930 1950 1979 1982\n",
            " 2020 2030 2033 2043 2056 2058 2071 2075 2097 2107 2133 2144 2158 2171\n",
            " 2173 2178 2185 2196 2203 2210 2241 2243 2244 2246 2259 2267 2269 2271\n",
            " 2276 2302 2325 2353 2376 2377 2389 2399 2414 2420 2442 2446 2451 2465\n",
            " 2466 2489 2500 2527 2529 2532 2550 2557 2562 2603 2607 2617 2633 2641\n",
            " 2694 2730 2749 2750 2760 2785 2794 2795 2803 2820 2842 2865 2892 2903\n",
            " 2920 2936 2958 2988 3003 3034 3035 3057 3090 3091 3094 3102 3111 3139\n",
            " 3171 3210 3216 3251 3266 3275 3299 3302 3312 3313 3346 3350 3356 3400\n",
            " 3415 3416 3436 3442 3470 3489 3499 3505 3526 3531 3545 3624 3640 3664\n",
            " 3675 3692 3710 3733 3781 3789 3811 3816 3830 3842 3891 3947 3953 3968\n",
            " 3976 3987 3999 4016 4024 4036 4044 4071 4086 4092 4123 4133 4143 4145\n",
            " 4156 4166 4197 4256 4266 4270 4271 4336 4373 4413 4430 4465 4473 4479\n",
            " 4492 4498 4533 4554 4587 4613 4615 4617 4667 4669 4694 4702 4725 4726\n",
            " 4728 4737 4740 4789 4795 4849 4859 4877 4880 4882 4941 4976 5012 5043\n",
            " 5047 5067 5154 5181 5193 5194 5210 5213 5227 5234 5240 5251 5261 5299\n",
            " 5309 5311 5331 5371 5426 5430 5440 5442 5451 5461 5464 5506 5521 5545\n",
            " 5575 5679 5688 5694 5709 5725 5750 5777 5796 5797 5813 5886 5913 5984\n",
            " 6029 6045 6048 6084 6092 6094 6103 6216 6241 6281 6312 6326 6379 6391\n",
            " 6397 6423 6451 6453 6474 6481 6529 6602 6620 6639 6710 6742 6763 6772\n",
            " 6779 6782 6894 6898 6913 6918 6945 6960 6962 6968 6992 6997 7013 7017\n",
            " 7081 7100 7136 7161 7166 7204 7222 7231 7243 7294 7304 7313 7320 7324\n",
            " 7355 7373 7397 7402 7538 7564 7682 7733 7757 7759 7783 7849 7890 7904\n",
            " 7927 7936 7956 7977 7985 8009 8051 8111 8186 8239 8295 8302 8499 8524\n",
            " 8543 8927 8938 9028 9054 9223 9282 9311 9368 9372 9422 9465 9595 9611\n",
            " 9671 9755 9915]\n",
            "\n",
            "507 [0.51009935 0.50707376 0.52511775 0.5103161  0.5096816  0.50290906\n",
            " 0.52166325 0.5057999  0.50037026 0.52010703 0.50520575 0.5052009\n",
            " 0.5099394  0.50040394 0.5009632  0.5014484  0.5076987  0.5005305\n",
            " 0.5055942  0.50070333 0.50688237 0.5043319  0.5140048  0.511491\n",
            " 0.50723463 0.5014904  0.50340086 0.5097363  0.5010234  0.5202629\n",
            " 0.5219588  0.5342018  0.5101498  0.5074004  0.5146329  0.51666385\n",
            " 0.5091416  0.5044585  0.5069854  0.505089   0.50103325 0.500648\n",
            " 0.50996053 0.5057549  0.50171196 0.5132908  0.5069667  0.52546114\n",
            " 0.5007276  0.5176572  0.5005729  0.5038772  0.5050035  0.5155955\n",
            " 0.50239515 0.5078863  0.51402706 0.5095712  0.5102049  0.5145725\n",
            " 0.5066194  0.50706035 0.5016752  0.50051534 0.5053159  0.5014648\n",
            " 0.51346296 0.53116584 0.51070756 0.5022638  0.5010228  0.5099397\n",
            " 0.52013034 0.51329505 0.50386804 0.5040512  0.5207474  0.50279593\n",
            " 0.51369643 0.5069404  0.5011688  0.50191545 0.5139398  0.5079651\n",
            " 0.5002433  0.5009967  0.5019336  0.5003982  0.50934434 0.51509935\n",
            " 0.5032788  0.5163783  0.5064758  0.5098119  0.5044078  0.5018805\n",
            " 0.503476   0.5070808  0.51247764 0.50528026 0.5066605  0.506177\n",
            " 0.501739   0.5098488  0.5025324  0.51104015 0.5179154  0.50528497\n",
            " 0.5159733  0.5117597  0.50998545 0.501896   0.50554144 0.5110943\n",
            " 0.50404847 0.5142149  0.5034864  0.50134957 0.5053827  0.5040837\n",
            " 0.50777656 0.5012163  0.512236   0.5070807  0.5006266  0.5048026\n",
            " 0.5124441  0.50236046 0.5053355  0.5139598  0.50876904 0.5019328\n",
            " 0.50309616 0.5012489  0.5006688  0.50507444 0.5090268  0.5141559\n",
            " 0.5139273  0.5033748  0.50695723 0.5056223  0.519216   0.50257\n",
            " 0.50264823 0.50454426 0.5091176  0.5032963  0.50106865 0.52211416\n",
            " 0.50635916 0.5066582  0.5059676  0.5039745  0.500547   0.5008066\n",
            " 0.5017985  0.52266955 0.5093796  0.50825226 0.501418   0.5057083\n",
            " 0.5064323  0.5043326  0.50961107 0.50065005 0.5033562  0.5013893\n",
            " 0.507554   0.5053991  0.5105825  0.50426275 0.50238425 0.50735635\n",
            " 0.5026572  0.51531637 0.5083783  0.5037885  0.50527877 0.5022553\n",
            " 0.5055057  0.5015068  0.5075769  0.50900793 0.5022104  0.5024068\n",
            " 0.5031395  0.5001537  0.5088958  0.51306236 0.5089916  0.5034696\n",
            " 0.5001756  0.52992904 0.5059565  0.5024862  0.5111725  0.50435984\n",
            " 0.5200662  0.50394505 0.50303996 0.5018106  0.50049853 0.50014704\n",
            " 0.5048293  0.5015402  0.50712585 0.5001391  0.50847656 0.5067729\n",
            " 0.5017594  0.50596696 0.50452876 0.50451064 0.50038826 0.50212836\n",
            " 0.50000775 0.5075437  0.5107127  0.507906   0.5011505  0.50709015\n",
            " 0.502379   0.5062666  0.52281314 0.5010585  0.51002413 0.50635815\n",
            " 0.50287354 0.50820166 0.50816035 0.50066185 0.5014218  0.5027508\n",
            " 0.5085781  0.50992316 0.50110286 0.51689893 0.50711787 0.50404376\n",
            " 0.5043758  0.5126623  0.50017637 0.50202835 0.50402707 0.5043098\n",
            " 0.503975   0.5078998  0.50466025 0.5035495  0.508331   0.5049523\n",
            " 0.5014386  0.51705    0.51439226 0.503411   0.50827956 0.50281906\n",
            " 0.50789165 0.5076044  0.50433826 0.5107523  0.5050514  0.51363724\n",
            " 0.50490636 0.5001556  0.50307256 0.5031806  0.50119406 0.50098675\n",
            " 0.5182045  0.50629836 0.5013184  0.50440925 0.50336856 0.50197923\n",
            " 0.50223714 0.5031611  0.514916   0.5047622  0.50620437 0.5141626\n",
            " 0.5043568  0.5075683  0.5037113  0.50422853 0.512625   0.500187\n",
            " 0.5053965  0.50184244 0.5106245  0.5023414  0.50372726 0.5028523\n",
            " 0.51207614 0.50891066 0.5015031  0.50852346 0.5086247  0.50209165\n",
            " 0.5085485  0.50868696 0.50472    0.504002   0.5040941  0.5012719\n",
            " 0.5025126  0.50526667 0.5014586  0.5033982  0.5038648  0.5197773\n",
            " 0.5009985  0.50329506 0.5044081  0.5108372  0.50061667 0.5010108\n",
            " 0.5057509  0.5065276  0.50157577 0.5043717  0.5019069  0.5136804\n",
            " 0.517919   0.5045771  0.5027331  0.50474983 0.50054854 0.50504404\n",
            " 0.51139337 0.5059108  0.500851   0.5047502  0.5017571  0.51541173\n",
            " 0.50714284 0.50271505 0.5020467  0.5089629  0.5067294  0.50450593\n",
            " 0.5033653  0.5011862  0.5007225  0.5035727  0.5111569  0.50477666\n",
            " 0.508755   0.5023482  0.5007149  0.5023669  0.50368625 0.50640714\n",
            " 0.50637233 0.50088406 0.5088043  0.50307465 0.513863   0.5178365\n",
            " 0.5124845  0.5153872  0.50236005 0.5039646  0.50308263 0.50477034\n",
            " 0.50095046 0.50047016 0.5082304  0.52074546 0.5044633  0.50629795\n",
            " 0.5065627  0.50310135 0.5039092  0.5041504  0.5033794  0.50527984\n",
            " 0.5004647  0.53445846 0.5131584  0.5128237  0.5018483  0.5051292\n",
            " 0.5029575  0.5145108  0.50527024 0.50471735 0.5233817  0.50790966\n",
            " 0.50606614 0.50856036 0.50551605 0.500121   0.50525856 0.50543493\n",
            " 0.50644493 0.5154903  0.5013195  0.50451696 0.50799    0.5179541\n",
            " 0.5104732  0.50123584 0.5076202  0.50350034 0.5057039  0.5231969\n",
            " 0.500565   0.5004557  0.50421417 0.5020368  0.5003327  0.50294423\n",
            " 0.5051203  0.50220525 0.50011027 0.5051561  0.5136152  0.50240827\n",
            " 0.5019169  0.5001636  0.5013607  0.5064545  0.50115997 0.5042579\n",
            " 0.5001225  0.50398874 0.5053571  0.5004859  0.5021251  0.5037823\n",
            " 0.5245611  0.5205282  0.5068804  0.50091875 0.50714135 0.50311184\n",
            " 0.5035517  0.5082097  0.5052171  0.5138578  0.5047964  0.51238024\n",
            " 0.50593543 0.5101713  0.50095356 0.50492966 0.5153865  0.5107322\n",
            " 0.5117191  0.50128853 0.50207734 0.50130093 0.50294155 0.5235255\n",
            " 0.50076777 0.5204476  0.5002709  0.5065372  0.52357334 0.50280935\n",
            " 0.5018399  0.50385076 0.50569123 0.505058   0.500535   0.50464267\n",
            " 0.51208997 0.5029738  0.50004476 0.5031076  0.5041393  0.5034109\n",
            " 0.5009007  0.50491667 0.5004485  0.50233024 0.50180566 0.50591695\n",
            " 0.5014694  0.512626   0.5117981  0.50571895 0.50886434 0.50213766\n",
            " 0.50932115 0.5057001  0.5303273  0.5138459  0.5032027  0.5070235\n",
            " 0.5311582  0.5158694  0.5004771  0.5069198  0.50488573 0.50527334\n",
            " 0.5010483  0.5035686  0.5005182  0.50740266 0.50342816 0.5033243\n",
            " 0.50286543 0.50713927 0.50901693]\n",
            "\n",
            "Matched draws\n",
            "Count: 15, Index: (array([ 392, 1475, 1519, 1884, 2030, 2785, 3057, 3400, 3692, 3891, 4271,\n",
            "       4728, 5012, 5227, 5796]),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "        DrawNo   DrawDate   PrizeType  LuckyNo\n",
            "107356  507520 2020-02-15  SpecialNo2      392\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "        DrawNo   DrawDate   PrizeType  LuckyNo\n",
            "107356  507520 2020-02-15  SpecialNo2      392\n",
            "\n",
            "\n",
            "All matched\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "107180  506820 2020-02-01      1stPrizeNo     5796\n",
            "107193  506820 2020-02-01      SpecialNo1     2785\n",
            "107199  506820 2020-02-01      SpecialNo6     2030\n",
            "107206  506920 2020-02-02  ConsolationNo1     1519\n",
            "107239  507020 2020-02-04      SpecialNo1     3057\n",
            "107243  507020 2020-02-04      SpecialNo4     1884\n",
            "107284  507220 2020-02-08  ConsolationNo9     4728\n",
            "107312  507320 2020-02-09      SpecialNo4     3692\n",
            "107329  507420 2020-02-12  ConsolationNo8     3891\n",
            "107334  507420 2020-02-12      SpecialNo3     5227\n",
            "107350  507520 2020-02-15  ConsolationNo6     5012\n",
            "107356  507520 2020-02-15      SpecialNo2      392\n",
            "107385  507620 2020-02-16      SpecialNo8     3400\n",
            "107466  508020 2020-02-26  ConsolationNo7     1475\n",
            "107469  508020 2020-02-26      SpecialNo1     4271\n",
            "CPU times: user 3h 32min 28s, sys: 3.67 s, total: 3h 32min 32s\n",
            "Wall time: 53min 52s\n",
            "\n",
            "-----------2020-02-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(969893, 35) (969893,) (10000, 35) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 937508, 1: 32385})\n",
            "\n",
            "scale_pos_weight - 28.94883433688436\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'missing': None, 'n_estimators': 550, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.94883433688436, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.94883433688436]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.438, F-Score=0.06312\n",
            "\n",
            "Recall: 0.03785488958990536\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.97      0.95      0.04      0.96      0.19      0.04      9683\n",
            "          1       0.02      0.04      0.95      0.03      0.19      0.03       317\n",
            "\n",
            "avg / total       0.94      0.92      0.07      0.93      0.19      0.04     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0378549</td>\n",
              "      <td>0.948879</td>\n",
              "      <td>0.0200598</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity   Optimize\n",
              "XGBClassifier_optimize            0   0.0378549         0.948879  0.0200598"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 507 \n",
            "\n",
            "First 23 matches\n",
            "23 [ 28  50  72 113 139 197 210 219 262 271 282 283 291 316 344 384 392 409\n",
            " 414 451 496 500 507]\n",
            "\n",
            "[0.50654733 0.5073871  0.52543694 0.50765383 0.5021818  0.50849915\n",
            " 0.516828   0.5028169  0.503307   0.5005377  0.50396496 0.5205427\n",
            " 0.5075602  0.5041397  0.50736785 0.5001961  0.5063832  0.5036734\n",
            " 0.50645554 0.50416076 0.5049058  0.50106895 0.512595  ]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[ 283 5194 1884 6045  568 4976 7294 7320  923 5797 1553 6772 4016 6763\n",
            " 2466  592  895 5311   72 2144 8938 8499 2185]\n",
            "\n",
            "\n",
            "[0.5205427  0.5223276  0.52123165 0.52326435 0.5209632  0.52069604\n",
            " 0.5213449  0.52381724 0.5206952  0.5218156  0.52223253 0.522679\n",
            " 0.5207005  0.5239036  0.5298235  0.5278731  0.5312932  0.5303758\n",
            " 0.52543694 0.52725595 0.53330934 0.53328884 0.5246686 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "80 [ 28  50  72 113 139 197 210 219 262 271 282 283 291 316 344 384 392 409\n",
            " 414 451 496 500 507 519 533 535 537 557 567 568 580 592 594 598 604 607\n",
            " 608 612 618 621 626 630 641 644 669 691 708 724 728 731 746 752 764 773\n",
            " 774 792 798 799 802 806 813 818 847 866 874 895 903 922 923 931 938 947\n",
            " 950 951 954 957 963 974 979 994]\n",
            "\n",
            "[0.50654733 0.5073871  0.52543694 0.50765383 0.5021818  0.50849915\n",
            " 0.516828   0.5028169  0.503307   0.5005377  0.50396496 0.5205427\n",
            " 0.5075602  0.5041397  0.50736785 0.5001961  0.5063832  0.5036734\n",
            " 0.50645554 0.50416076 0.5049058  0.50106895 0.512595   0.50510484\n",
            " 0.50336385 0.50373405 0.50478613 0.505469   0.5188266  0.5209632\n",
            " 0.50007415 0.5278731  0.50771576 0.5051011  0.5167177  0.5148486\n",
            " 0.5076603  0.5021486  0.5098522  0.5016606  0.5054426  0.50103396\n",
            " 0.5139775  0.508576   0.5131704  0.50576633 0.51369137 0.5009244\n",
            " 0.51836896 0.5023971  0.5003821  0.50482744 0.5162819  0.5020324\n",
            " 0.5093268  0.5157412  0.5036519  0.50715697 0.5134118  0.51083714\n",
            " 0.5088875  0.50273085 0.5029684  0.50329745 0.51541626 0.5312932\n",
            " 0.50541013 0.51087075 0.5206952  0.51089954 0.5091001  0.5054112\n",
            " 0.51503354 0.50075847 0.5152483  0.5079622  0.50215155 0.5038991\n",
            " 0.5107705  0.5016868 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.5083950161933899\n",
            "\n",
            "144 [  72  197  210  283  507  567  568  592  604  607  618  641  644  669\n",
            "  708  728  764  774  792  802  806  813  874  895  922  923  931  938\n",
            "  950  954  979 1034 1044 1053 1071 1115 1172 1176 1178 1187 1198 1247\n",
            " 1260 1298 1308 1316 1354 1377 1387 1457 1475 1508 1553 1571 1597 1671\n",
            " 1702 1754 1815 1884 2071 2075 2144 2173 2185 2259 2269 2399 2414 2446\n",
            " 2466 2500 2550 2617 2641 2750 2892 2936 2958 3057 3102 3111 3266 3356\n",
            " 3415 3416 3499 3545 3675 3811 3816 4016 4044 4166 4197 4336 4373 4479\n",
            " 4554 4725 4880 4882 4941 4976 5193 5194 5311 5331 5371 5442 5464 5506\n",
            " 5725 5796 5797 5813 5913 6045 6216 6379 6763 6772 6779 6918 6960 6968\n",
            " 6997 7081 7100 7136 7231 7294 7320 7373 7397 7682 8051 8186 8499 8524\n",
            " 8938 9028 9311 9465]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.5067496299743652\n",
            "\n",
            "189 [  50   72  113  197  210  283  291  344  507  567  568  592  594  604\n",
            "  607  608  618  641  644  669  708  728  764  774  792  799  802  806\n",
            "  813  874  895  922  923  931  938  950  954  957  979 1034 1044 1053\n",
            " 1058 1071 1115 1131 1156 1172 1176 1178 1187 1198 1221 1247 1260 1294\n",
            " 1298 1308 1316 1354 1377 1387 1454 1457 1475 1508 1546 1550 1553 1571\n",
            " 1597 1671 1684 1702 1754 1815 1879 1884 1888 1979 2071 2075 2144 2173\n",
            " 2185 2259 2269 2271 2399 2414 2446 2451 2466 2500 2550 2607 2617 2641\n",
            " 2750 2892 2936 2958 3057 3102 3111 3266 3356 3400 3415 3416 3499 3545\n",
            " 3675 3692 3733 3781 3811 3816 3830 3968 4016 4044 4166 4197 4336 4373\n",
            " 4413 4479 4492 4554 4613 4725 4795 4859 4880 4882 4941 4976 5193 5194\n",
            " 5299 5311 5331 5371 5442 5464 5506 5709 5725 5777 5796 5797 5813 5913\n",
            " 5984 6045 6216 6326 6379 6453 6763 6772 6779 6918 6960 6968 6992 6997\n",
            " 7081 7100 7136 7231 7294 7313 7320 7373 7397 7682 7977 8009 8051 8186\n",
            " 8499 8524 8938 9028 9282 9311 9465]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "507 [  28   50   72  113  139  197  210  219  262  271  282  283  291  316\n",
            "  344  384  392  409  414  451  496  500  507  519  533  535  537  557\n",
            "  567  568  580  592  594  598  604  607  608  612  618  621  626  630\n",
            "  641  644  669  691  708  724  728  731  746  752  764  773  774  792\n",
            "  798  799  802  806  813  818  847  866  874  895  903  922  923  931\n",
            "  938  947  950  951  954  957  963  974  979  994 1005 1008 1009 1029\n",
            " 1034 1044 1050 1053 1058 1071 1074 1100 1101 1102 1115 1126 1131 1142\n",
            " 1148 1156 1163 1172 1176 1177 1178 1187 1198 1215 1221 1240 1247 1256\n",
            " 1260 1271 1279 1294 1295 1298 1304 1308 1316 1347 1354 1374 1377 1387\n",
            " 1399 1409 1413 1415 1441 1454 1457 1475 1495 1508 1516 1519 1546 1550\n",
            " 1553 1555 1558 1571 1573 1580 1581 1585 1597 1622 1631 1635 1654 1655\n",
            " 1658 1661 1671 1684 1702 1718 1721 1723 1748 1751 1754 1771 1784 1788\n",
            " 1807 1811 1813 1815 1830 1854 1879 1880 1884 1888 1927 1930 1979 1982\n",
            " 2020 2030 2033 2043 2056 2057 2071 2075 2097 2107 2144 2161 2173 2178\n",
            " 2185 2196 2203 2243 2244 2246 2259 2267 2269 2271 2276 2302 2325 2350\n",
            " 2353 2376 2377 2389 2399 2414 2420 2442 2446 2451 2464 2465 2466 2489\n",
            " 2500 2527 2529 2544 2550 2557 2562 2603 2607 2617 2641 2694 2730 2749\n",
            " 2750 2760 2785 2786 2794 2795 2803 2820 2865 2871 2892 2903 2936 2958\n",
            " 2988 3003 3034 3035 3057 3083 3090 3091 3094 3102 3111 3113 3171 3210\n",
            " 3251 3266 3269 3275 3299 3312 3334 3346 3350 3356 3400 3415 3416 3436\n",
            " 3442 3470 3487 3489 3499 3505 3526 3545 3577 3624 3664 3675 3685 3692\n",
            " 3710 3733 3747 3781 3789 3811 3816 3830 3842 3891 3920 3947 3953 3968\n",
            " 3976 3987 3999 4009 4012 4016 4024 4036 4044 4071 4086 4092 4123 4133\n",
            " 4143 4145 4156 4166 4197 4256 4266 4336 4373 4413 4465 4473 4479 4492\n",
            " 4498 4533 4554 4587 4613 4615 4667 4669 4692 4694 4695 4702 4725 4726\n",
            " 4737 4740 4789 4795 4832 4849 4859 4877 4880 4882 4941 4976 5047 5096\n",
            " 5193 5194 5213 5227 5234 5251 5261 5299 5311 5331 5338 5371 5422 5426\n",
            " 5430 5440 5442 5451 5461 5464 5506 5521 5545 5575 5679 5694 5709 5725\n",
            " 5726 5750 5777 5796 5797 5813 5886 5913 5946 5984 6019 6029 6045 6084\n",
            " 6092 6094 6103 6216 6241 6281 6312 6326 6379 6384 6391 6397 6423 6451\n",
            " 6453 6474 6481 6602 6620 6674 6710 6742 6763 6772 6779 6782 6894 6913\n",
            " 6918 6945 6960 6962 6968 6992 6997 7013 7017 7081 7100 7113 7136 7204\n",
            " 7214 7222 7231 7294 7304 7313 7320 7324 7347 7355 7373 7397 7402 7538\n",
            " 7682 7733 7757 7759 7849 7855 7891 7927 7936 7956 7977 8009 8051 8111\n",
            " 8186 8225 8239 8295 8302 8476 8499 8524 8533 8543 8556 8624 8643 8747\n",
            " 8881 8927 8938 8956 9028 9054 9223 9282 9311 9368 9372 9422 9465 9611\n",
            " 9692 9755 9915]\n",
            "\n",
            "507 [0.50654733 0.5073871  0.52543694 0.50765383 0.5021818  0.50849915\n",
            " 0.516828   0.5028169  0.503307   0.5005377  0.50396496 0.5205427\n",
            " 0.5075602  0.5041397  0.50736785 0.5001961  0.5063832  0.5036734\n",
            " 0.50645554 0.50416076 0.5049058  0.50106895 0.512595   0.50510484\n",
            " 0.50336385 0.50373405 0.50478613 0.505469   0.5188266  0.5209632\n",
            " 0.50007415 0.5278731  0.50771576 0.5051011  0.5167177  0.5148486\n",
            " 0.5076603  0.5021486  0.5098522  0.5016606  0.5054426  0.50103396\n",
            " 0.5139775  0.508576   0.5131704  0.50576633 0.51369137 0.5009244\n",
            " 0.51836896 0.5023971  0.5003821  0.50482744 0.5162819  0.5020324\n",
            " 0.5093268  0.5157412  0.5036519  0.50715697 0.5134118  0.51083714\n",
            " 0.5088875  0.50273085 0.5029684  0.50329745 0.51541626 0.5312932\n",
            " 0.50541013 0.51087075 0.5206952  0.51089954 0.5091001  0.5054112\n",
            " 0.51503354 0.50075847 0.5152483  0.5079622  0.50215155 0.5038991\n",
            " 0.5107705  0.5016868  0.5014262  0.5011142  0.50169265 0.50173545\n",
            " 0.508753   0.5142988  0.5060133  0.51692086 0.5079073  0.50901693\n",
            " 0.50519747 0.5017069  0.5005615  0.50206316 0.5124068  0.5060349\n",
            " 0.5068609  0.5060316  0.50446856 0.5073288  0.5035706  0.512909\n",
            " 0.51928014 0.5057192  0.51473516 0.51064014 0.5091344  0.501832\n",
            " 0.5067946  0.506386   0.5119468  0.5008271  0.51557946 0.5001818\n",
            " 0.5029368  0.50787395 0.50549304 0.51096576 0.5018392  0.5120983\n",
            " 0.5092539  0.5055012  0.5110054  0.50564444 0.51132846 0.5094256\n",
            " 0.501706   0.5059265  0.5032267  0.50217164 0.5002134  0.5083697\n",
            " 0.5167558  0.5128701  0.50102574 0.5144955  0.50109637 0.50351125\n",
            " 0.50703543 0.5077746  0.52223253 0.5062157  0.5040162  0.5111289\n",
            " 0.5054743  0.5025117  0.50260764 0.50148296 0.5189182  0.5065976\n",
            " 0.5049452  0.5019959  0.501005   0.50354517 0.50003284 0.50225633\n",
            " 0.52042717 0.5081786  0.5090168  0.5042648  0.50661737 0.5009921\n",
            " 0.5005435  0.50422466 0.51774925 0.5019223  0.5014648  0.5029198\n",
            " 0.5052186  0.50032365 0.5026469  0.51296335 0.50667185 0.50251406\n",
            " 0.5079884  0.502516   0.52123165 0.5082123  0.50375485 0.5042065\n",
            " 0.50683814 0.5009334  0.5043998  0.50610256 0.50184375 0.50435716\n",
            " 0.5023952  0.50211066 0.51087177 0.513771   0.50342774 0.50416714\n",
            " 0.52725595 0.50005925 0.5147717  0.5053595  0.5246686  0.5007369\n",
            " 0.5065414  0.5028278  0.50338775 0.50236404 0.5093942  0.50060886\n",
            " 0.509088   0.5079434  0.5019154  0.5008739  0.50526893 0.5003451\n",
            " 0.5034068  0.50120056 0.5039371  0.50028545 0.5087421  0.5129004\n",
            " 0.5056076  0.50174737 0.5093855  0.5073785  0.5017801  0.50611526\n",
            " 0.5298235  0.5032813  0.511766   0.5066819  0.5065764  0.5000705\n",
            " 0.5084079  0.50388575 0.5003141  0.50063515 0.5068772  0.5095751\n",
            " 0.5180508  0.5047437  0.50302845 0.5030914  0.5178337  0.5032414\n",
            " 0.50242925 0.50081944 0.5041919  0.50625867 0.5035395  0.50334305\n",
            " 0.5054569  0.5005351  0.5098089  0.5014246  0.51652145 0.51656616\n",
            " 0.5053235  0.50577986 0.50168324 0.504325   0.5091011  0.5006014\n",
            " 0.50033796 0.50454295 0.5033895  0.5138201  0.5090973  0.5034207\n",
            " 0.5046614  0.5030785  0.5017739  0.52011305 0.50289965 0.50505817\n",
            " 0.5015796  0.5041571  0.50494564 0.5020995  0.506558   0.51222444\n",
            " 0.50794685 0.5107251  0.51599056 0.50266135 0.50543    0.5043897\n",
            " 0.50567055 0.5015636  0.51193833 0.5004631  0.50570446 0.51185745\n",
            " 0.5020087  0.50349385 0.50566864 0.5144921  0.500571   0.5071844\n",
            " 0.50486666 0.50680476 0.50033176 0.50704736 0.5034179  0.5113357\n",
            " 0.5091452  0.5072096  0.504498   0.50600445 0.50257456 0.5012228\n",
            " 0.503742   0.5067638  0.50329816 0.5008776  0.5056504  0.5001411\n",
            " 0.50068724 0.5207005  0.5038616  0.505802   0.50922155 0.50555456\n",
            " 0.50143105 0.50150806 0.501012   0.50560594 0.505299   0.50425535\n",
            " 0.50359553 0.5092588  0.5178674  0.50141776 0.5043102  0.50861335\n",
            " 0.5121459  0.50810724 0.5057873  0.50638086 0.5156124  0.5077884\n",
            " 0.503512   0.502567   0.51127946 0.50384754 0.50818413 0.5010636\n",
            " 0.5026009  0.5018579  0.50145185 0.50506777 0.50296366 0.50554234\n",
            " 0.5094857  0.50484526 0.50083596 0.50292027 0.50490505 0.5074689\n",
            " 0.5001246  0.5031049  0.5080042  0.505152   0.513379   0.5114728\n",
            " 0.5130619  0.52069604 0.50085443 0.5024813  0.51022625 0.5223276\n",
            " 0.5065892  0.5034629  0.50337577 0.50542545 0.50016624 0.50806934\n",
            " 0.5303758  0.5158031  0.5033513  0.516894   0.5011918  0.5024962\n",
            " 0.5039175  0.5066356  0.5161342  0.5050471  0.5043458  0.5148351\n",
            " 0.509517   0.5011963  0.5048886  0.5028983  0.50188303 0.5050573\n",
            " 0.5070203  0.51260805 0.5001747  0.5005435  0.5070468  0.50854385\n",
            " 0.5218156  0.5125459  0.5003935  0.5092256  0.50092137 0.5083614\n",
            " 0.500212   0.505559   0.52326435 0.50205517 0.50572664 0.50368446\n",
            " 0.5023173  0.50907665 0.5031694  0.5028099  0.5015391  0.5072834\n",
            " 0.5104279  0.50123763 0.5001044  0.5013271  0.5008314  0.5033837\n",
            " 0.5078999  0.50141704 0.5052567  0.5030347  0.50494754 0.5010389\n",
            " 0.5025293  0.50274086 0.5239036  0.522679   0.5084773  0.501214\n",
            " 0.5058546  0.5046513  0.510497   0.50536114 0.5174072  0.5022195\n",
            " 0.514547   0.5076136  0.5091342  0.5034598  0.5045352  0.51351446\n",
            " 0.514832   0.50111055 0.51647943 0.5040271  0.50286144 0.5002345\n",
            " 0.52000177 0.5213449  0.5004012  0.5072286  0.52381724 0.50443625\n",
            " 0.5020528  0.50178695 0.5091819  0.50918823 0.50606394 0.50260127\n",
            " 0.5124255  0.50596434 0.50137836 0.50321907 0.5025251  0.50038\n",
            " 0.50089025 0.5002516  0.5008312  0.5037765  0.50677943 0.5072484\n",
            " 0.5149434  0.503683   0.51307416 0.5031674  0.5017251  0.50560486\n",
            " 0.50362164 0.5008723  0.53328884 0.5134102  0.5001049  0.50566703\n",
            " 0.50132847 0.5000007  0.5000706  0.5017798  0.5030826  0.50262624\n",
            " 0.53330934 0.5024441  0.51281524 0.5022145  0.5045689  0.50768137\n",
            " 0.50875247 0.50109273 0.50241834 0.50129855 0.5098668  0.5023401\n",
            " 0.5012281  0.5055945  0.5043385 ]\n",
            "\n",
            "Matched draws\n",
            "Count: 12, Index: (array([ 392, 1475, 1519, 1884, 2030, 2785, 3057, 3400, 3692, 3891, 5227,\n",
            "       5796]),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "        DrawNo   DrawDate   PrizeType  LuckyNo\n",
            "107243  507020 2020-02-04  SpecialNo4     1884\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "        DrawNo   DrawDate   PrizeType  LuckyNo\n",
            "107356  507520 2020-02-15  SpecialNo2      392\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "        DrawNo   DrawDate   PrizeType  LuckyNo\n",
            "107356  507520 2020-02-15  SpecialNo2      392\n",
            "\n",
            "\n",
            "All matched\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "107180  506820 2020-02-01      1stPrizeNo     5796\n",
            "107193  506820 2020-02-01      SpecialNo1     2785\n",
            "107199  506820 2020-02-01      SpecialNo6     2030\n",
            "107206  506920 2020-02-02  ConsolationNo1     1519\n",
            "107239  507020 2020-02-04      SpecialNo1     3057\n",
            "107243  507020 2020-02-04      SpecialNo4     1884\n",
            "107312  507320 2020-02-09      SpecialNo4     3692\n",
            "107329  507420 2020-02-12  ConsolationNo8     3891\n",
            "107334  507420 2020-02-12      SpecialNo3     5227\n",
            "107356  507520 2020-02-15      SpecialNo2      392\n",
            "107385  507620 2020-02-16      SpecialNo8     3400\n",
            "107466  508020 2020-02-26  ConsolationNo7     1475\n",
            "CPU times: user 4h 20min 32s, sys: 4.24 s, total: 4h 20min 36s\n",
            "Wall time: 1h 6min 3s\n",
            "\n",
            "-----------2020-02-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(969893, 35) (969893,) (10000, 35) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 937508, 1: 32385})\n",
            "\n",
            "scale_pos_weight - 28.94883433688436\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'missing': None, 'n_estimators': 600, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.94883433688436, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.94883433688436]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.436, F-Score=0.06320\n",
            "\n",
            "Recall: 0.0473186119873817\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       0.97      0.95      0.05      0.96      0.21      0.05      9683\n",
            "          1       0.03      0.05      0.95      0.04      0.21      0.04       317\n",
            "\n",
            "avg / total       0.94      0.92      0.08      0.93      0.21      0.05     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIAtBAkAwJb/AQW0PR66XhdpAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0473186</td>\n",
              "      <td>0.946194</td>\n",
              "      <td>0.0270938</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity   Optimize\n",
              "XGBClassifier_optimize            0   0.0473186         0.946194  0.0270938"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 536 \n",
            "\n",
            "First 23 matches\n",
            "23 [ 28  50  72 113 197 210 219 262 271 282 283 291 316 344 365 392 409 412\n",
            " 414 451 489 496 500]\n",
            "\n",
            "[0.50740576 0.5064516  0.5251435  0.50756836 0.50447315 0.5194693\n",
            " 0.5023858  0.5032877  0.50009245 0.50722694 0.52067995 0.5079118\n",
            " 0.50470537 0.5081521  0.50036293 0.50751716 0.51123214 0.50044715\n",
            " 0.50516975 0.50480855 0.5013409  0.5038257  0.505206  ]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[3499  923 7294 6772 5797  950  728 7231  895 6763   72 8938 6045 5311\n",
            "  592 2185 3266 8499 2144 2466 5194 7136  568]\n",
            "\n",
            "\n",
            "[0.5218379  0.52223647 0.52284896 0.5228046  0.52220404 0.5227682\n",
            " 0.52286106 0.52491903 0.5336285  0.5265676  0.5251435  0.53673226\n",
            " 0.52439183 0.53017014 0.5254308  0.5270579  0.52453315 0.529583\n",
            " 0.5252656  0.53208816 0.52723503 0.52390736 0.52351415]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "78 [ 28  50  72 113 197 210 219 262 271 282 283 291 316 344 365 392 409 412\n",
            " 414 451 489 496 500 507 519 535 537 557 559 567 568 592 594 598 604 607\n",
            " 608 612 618 621 626 641 644 661 669 670 691 708 728 731 752 764 774 792\n",
            " 798 802 806 813 818 847 866 874 895 903 911 921 922 923 931 932 938 947\n",
            " 950 954 957 974 979 994]\n",
            "\n",
            "[0.50740576 0.5064516  0.5251435  0.50756836 0.50447315 0.5194693\n",
            " 0.5023858  0.5032877  0.50009245 0.50722694 0.52067995 0.5079118\n",
            " 0.50470537 0.5081521  0.50036293 0.50751716 0.51123214 0.50044715\n",
            " 0.50516975 0.50480855 0.5013409  0.5038257  0.505206   0.51113\n",
            " 0.50306946 0.50508857 0.50653064 0.50496787 0.5010506  0.52133125\n",
            " 0.52351415 0.5254308  0.5028815  0.50927234 0.5147736  0.51497173\n",
            " 0.5093973  0.5038664  0.5129829  0.50462675 0.5063897  0.51563776\n",
            " 0.50591415 0.5010626  0.512755   0.5023308  0.5059096  0.5098491\n",
            " 0.52286106 0.5052361  0.50361186 0.51792485 0.5159527  0.51688665\n",
            " 0.5065662  0.50929695 0.50853163 0.5097517  0.5037843  0.50525266\n",
            " 0.50686455 0.5137923  0.5336285  0.5113622  0.50221455 0.5003564\n",
            " 0.5042129  0.52223647 0.5120786  0.5034012  0.50633734 0.5058255\n",
            " 0.5227682  0.5161398  0.5080832  0.505727   0.51164764 0.501479  ]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.5091462731361389\n",
            "\n",
            "148 [  72  210  283  409  507  567  568  592  598  604  607  608  618  641\n",
            "  669  708  728  764  774  792  802  813  874  895  903  923  931  950\n",
            "  954  979 1044 1053 1071 1115 1148 1172 1176 1178 1187 1198 1247 1260\n",
            " 1298 1308 1354 1377 1387 1409 1457 1475 1508 1546 1553 1571 1573 1597\n",
            " 1622 1671 1702 1754 1815 1830 1884 1888 2071 2144 2173 2185 2259 2271\n",
            " 2414 2446 2466 2500 2529 2550 2641 2750 2936 2958 3102 3266 3356 3400\n",
            " 3415 3416 3499 3526 3545 3675 3811 4016 4036 4044 4166 4197 4479 4554\n",
            " 4702 4725 4880 4882 4941 4976 5193 5194 5311 5331 5371 5440 5442 5451\n",
            " 5464 5506 5725 5777 5797 5813 5913 6045 6216 6326 6379 6763 6772 6779\n",
            " 6960 6968 7081 7100 7136 7204 7231 7294 7320 7373 7402 7682 7977 8051\n",
            " 8186 8295 8499 8524 8938 9028 9311 9465]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.5072968602180481\n",
            "\n",
            "199 [  28   72  113  210  283  291  344  392  409  507  567  568  592  598\n",
            "  604  607  608  618  641  669  708  728  764  774  792  802  806  813\n",
            "  874  895  903  923  931  950  954  957  979 1044 1053 1058 1071 1115\n",
            " 1131 1142 1148 1172 1176 1178 1187 1198 1247 1260 1294 1295 1298 1308\n",
            " 1354 1377 1387 1409 1454 1457 1475 1508 1546 1553 1571 1573 1597 1622\n",
            " 1671 1702 1754 1815 1830 1884 1888 2043 2071 2144 2173 2185 2203 2259\n",
            " 2269 2271 2414 2446 2451 2465 2466 2500 2527 2529 2550 2617 2641 2694\n",
            " 2750 2936 2958 2988 3003 3057 3102 3266 3350 3356 3400 3415 3416 3487\n",
            " 3499 3526 3545 3675 3710 3811 4016 4036 4044 4143 4166 4197 4373 4413\n",
            " 4473 4479 4492 4554 4613 4694 4702 4725 4795 4859 4880 4882 4941 4976\n",
            " 5193 5194 5213 5299 5311 5331 5371 5440 5442 5451 5464 5506 5521 5709\n",
            " 5725 5777 5796 5797 5813 5913 5984 6045 6216 6326 6379 6620 6763 6772\n",
            " 6779 6894 6918 6945 6960 6968 6997 7017 7081 7100 7136 7204 7231 7294\n",
            " 7313 7320 7373 7402 7682 7733 7977 8051 8186 8295 8499 8524 8938 9028\n",
            " 9311 9465 9755]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "536 [  28   50   72  113  197  210  219  262  271  282  283  291  316  344\n",
            "  365  392  409  412  414  451  489  496  500  507  519  535  537  557\n",
            "  559  567  568  592  594  598  604  607  608  612  618  621  626  641\n",
            "  644  661  669  670  691  708  728  731  752  764  774  792  798  802\n",
            "  806  813  818  847  866  874  895  903  911  921  922  923  931  932\n",
            "  938  947  950  954  957  974  979  994 1005 1008 1009 1027 1029 1034\n",
            " 1044 1050 1053 1058 1071 1074 1083 1100 1115 1126 1131 1142 1148 1156\n",
            " 1163 1172 1176 1177 1178 1187 1198 1215 1221 1240 1247 1260 1271 1279\n",
            " 1294 1295 1298 1308 1316 1343 1347 1354 1362 1374 1377 1387 1399 1409\n",
            " 1413 1415 1441 1454 1457 1475 1508 1516 1519 1546 1550 1553 1555 1558\n",
            " 1571 1573 1580 1581 1585 1597 1622 1631 1635 1654 1655 1661 1671 1684\n",
            " 1702 1718 1721 1723 1748 1751 1754 1771 1776 1784 1788 1807 1811 1813\n",
            " 1815 1822 1830 1854 1867 1879 1880 1884 1888 1904 1927 1930 1949 1972\n",
            " 1979 1982 2019 2030 2033 2043 2057 2058 2071 2075 2097 2101 2107 2144\n",
            " 2161 2173 2178 2185 2196 2198 2203 2244 2246 2259 2267 2269 2271 2276\n",
            " 2323 2325 2353 2377 2389 2399 2414 2420 2442 2446 2451 2464 2465 2466\n",
            " 2479 2489 2500 2527 2529 2544 2550 2557 2568 2603 2607 2617 2633 2641\n",
            " 2694 2730 2749 2750 2760 2785 2786 2794 2795 2803 2820 2865 2892 2903\n",
            " 2927 2936 2958 2988 3003 3034 3035 3057 3083 3091 3094 3102 3111 3113\n",
            " 3154 3171 3210 3251 3266 3269 3299 3302 3312 3313 3334 3344 3346 3350\n",
            " 3356 3400 3415 3416 3421 3430 3435 3436 3442 3470 3487 3494 3499 3505\n",
            " 3516 3526 3545 3577 3624 3640 3664 3675 3685 3692 3710 3733 3781 3789\n",
            " 3811 3816 3830 3842 3891 3920 3947 3953 3968 3976 3987 3999 4012 4016\n",
            " 4024 4036 4044 4071 4086 4092 4115 4133 4143 4145 4156 4166 4172 4197\n",
            " 4205 4231 4256 4266 4271 4336 4373 4413 4465 4473 4477 4479 4492 4498\n",
            " 4514 4533 4554 4587 4613 4615 4667 4669 4692 4694 4695 4702 4725 4726\n",
            " 4728 4737 4740 4789 4795 4849 4859 4860 4877 4880 4882 4941 4976 5043\n",
            " 5067 5181 5193 5194 5211 5213 5227 5234 5251 5299 5311 5331 5338 5371\n",
            " 5422 5426 5430 5440 5442 5451 5461 5464 5472 5506 5521 5523 5545 5575\n",
            " 5679 5694 5709 5725 5726 5777 5789 5796 5797 5813 5886 5913 5946 5984\n",
            " 6019 6029 6045 6084 6092 6094 6216 6241 6281 6312 6326 6379 6397 6423\n",
            " 6451 6453 6481 6487 6510 6557 6598 6602 6620 6639 6674 6710 6742 6763\n",
            " 6772 6779 6782 6894 6913 6918 6945 6960 6962 6968 6992 6997 7017 7081\n",
            " 7100 7113 7136 7160 7166 7204 7214 7222 7231 7264 7294 7313 7320 7324\n",
            " 7347 7355 7373 7397 7402 7538 7564 7603 7664 7682 7733 7757 7759 7783\n",
            " 7849 7855 7891 7927 7936 7943 7946 7956 7977 8009 8051 8078 8111 8186\n",
            " 8225 8295 8302 8361 8476 8499 8524 8533 8543 8556 8624 8747 8784 8881\n",
            " 8908 8927 8938 8956 9028 9054 9223 9282 9311 9368 9372 9465 9595 9611\n",
            " 9671 9692 9755 9915]\n",
            "\n",
            "536 [0.50740576 0.5064516  0.5251435  0.50756836 0.50447315 0.5194693\n",
            " 0.5023858  0.5032877  0.50009245 0.50722694 0.52067995 0.5079118\n",
            " 0.50470537 0.5081521  0.50036293 0.50751716 0.51123214 0.50044715\n",
            " 0.50516975 0.50480855 0.5013409  0.5038257  0.505206   0.51113\n",
            " 0.50306946 0.50508857 0.50653064 0.50496787 0.5010506  0.52133125\n",
            " 0.52351415 0.5254308  0.5028815  0.50927234 0.5147736  0.51497173\n",
            " 0.5093973  0.5038664  0.5129829  0.50462675 0.5063897  0.51563776\n",
            " 0.50591415 0.5010626  0.512755   0.5023308  0.5059096  0.5098491\n",
            " 0.52286106 0.5052361  0.50361186 0.51792485 0.5159527  0.51688665\n",
            " 0.5065662  0.50929695 0.50853163 0.5097517  0.5037843  0.50525266\n",
            " 0.50686455 0.5137923  0.5336285  0.5113622  0.50221455 0.5003564\n",
            " 0.5042129  0.52223647 0.5120786  0.5034012  0.50633734 0.5058255\n",
            " 0.5227682  0.5161398  0.5080832  0.505727   0.51164764 0.501479\n",
            " 0.5007107  0.50340194 0.5024669  0.5003802  0.5029301  0.50674814\n",
            " 0.51699084 0.50404316 0.5184414  0.5075565  0.51146823 0.5061289\n",
            " 0.50310826 0.5012816  0.5143571  0.50513184 0.5078251  0.5076037\n",
            " 0.50966424 0.5006458  0.5016664  0.5155703  0.518247   0.5035376\n",
            " 0.51822704 0.5106809  0.5109309  0.5028516  0.50620353 0.5032329\n",
            " 0.5119956  0.513637   0.5035334  0.5048855  0.5087167  0.5083713\n",
            " 0.51174617 0.5153959  0.50479007 0.50058097 0.50547063 0.5164833\n",
            " 0.5027933  0.5035104  0.5142539  0.5097786  0.5010445  0.5144035\n",
            " 0.50676286 0.50120425 0.50138587 0.5076686  0.5189394  0.5110595\n",
            " 0.51351774 0.50123376 0.50727296 0.50927806 0.5071502  0.5177618\n",
            " 0.5048718  0.5004829  0.51228195 0.50923413 0.50257045 0.5015138\n",
            " 0.502807   0.51907134 0.51045185 0.50245035 0.50081307 0.5015234\n",
            " 0.5053961  0.5017302  0.5217945  0.50663006 0.5104855  0.505848\n",
            " 0.5055119  0.5019659  0.50320053 0.5034625  0.5178335  0.50133806\n",
            " 0.50124955 0.50017726 0.5032268  0.5062134  0.5022803  0.50176996\n",
            " 0.5211421  0.5016115  0.5095528  0.50461775 0.5002654  0.5054595\n",
            " 0.50348854 0.5215071  0.51219106 0.50213623 0.5030385  0.50066197\n",
            " 0.50188994 0.5028977  0.5061978  0.50402766 0.50106627 0.50222903\n",
            " 0.50249594 0.5079077  0.50364494 0.50070536 0.51324826 0.50259036\n",
            " 0.5061613  0.50115913 0.50595415 0.5252656  0.50333685 0.5178721\n",
            " 0.5016649  0.5270579  0.50330216 0.5015599  0.5090852  0.50250095\n",
            " 0.5002595  0.5102044  0.5038965  0.50901324 0.50981355 0.50392354\n",
            " 0.50146943 0.50490683 0.5066237  0.5041835  0.50153494 0.50398064\n",
            " 0.51404405 0.506256   0.5037863  0.5124623  0.50810117 0.5000162\n",
            " 0.50795287 0.53208816 0.50526047 0.5022976  0.5144162  0.50906754\n",
            " 0.50933033 0.500562   0.5129021  0.50567394 0.5013524  0.50132227\n",
            " 0.5067327  0.5091377  0.5033894  0.517494   0.50758505 0.5030116\n",
            " 0.5036627  0.5211154  0.5028325  0.5020448  0.5028176  0.50472707\n",
            " 0.50711894 0.5028207  0.50502783 0.5059851  0.5058181  0.5018731\n",
            " 0.50166476 0.51993054 0.5186706  0.5082603  0.5074977  0.50380695\n",
            " 0.5048737  0.5090212  0.50129235 0.50432885 0.5066943  0.5166648\n",
            " 0.5018865  0.5043546  0.5002043  0.5052563  0.5036695  0.5011643\n",
            " 0.52453315 0.50406027 0.5000859  0.50042224 0.50267655 0.5010065\n",
            " 0.5041979  0.50099117 0.5030403  0.5083787  0.5128482  0.5091936\n",
            " 0.5133198  0.517986   0.5013223  0.5027669  0.50299054 0.5006219\n",
            " 0.50633454 0.5035602  0.5074571  0.5010249  0.5218379  0.50136447\n",
            " 0.50201297 0.5097207  0.510045   0.5056706  0.5026125  0.5007106\n",
            " 0.5046483  0.5141088  0.50297284 0.5070892  0.5091013  0.506803\n",
            " 0.50633526 0.50584966 0.51221704 0.50447893 0.50590485 0.50526834\n",
            " 0.50613725 0.50354284 0.5010677  0.50302297 0.5063969  0.50635463\n",
            " 0.50179225 0.50577    0.50291204 0.5211773  0.5027831  0.50981086\n",
            " 0.5134728  0.50684893 0.50231266 0.5019769  0.50004476 0.5025624\n",
            " 0.5084541  0.50607044 0.5049676  0.51231736 0.50104564 0.5196102\n",
            " 0.5023675  0.50031304 0.5056758  0.5071635  0.50272685 0.5015712\n",
            " 0.5089578  0.50814885 0.50553405 0.50761133 0.5008816  0.51865363\n",
            " 0.5090312  0.5025015  0.5001165  0.503235   0.51539564 0.50506467\n",
            " 0.5087653  0.5007103  0.50055915 0.50340265 0.50377136 0.5091449\n",
            " 0.50475687 0.51497567 0.5102607  0.5063569  0.5056762  0.5051369\n",
            " 0.5046756  0.50299996 0.5075985  0.50295293 0.50839573 0.500929\n",
            " 0.5040736  0.5139661  0.5135638  0.5146985  0.52177405 0.50223607\n",
            " 0.5026615  0.5016387  0.51049423 0.52723503 0.501148   0.50800705\n",
            " 0.5041659  0.50170815 0.5022067  0.50872725 0.53017014 0.5138598\n",
            " 0.5009473  0.51774156 0.5030305  0.5051112  0.5022373  0.5106848\n",
            " 0.516116   0.51043165 0.5041387  0.51909363 0.5001739  0.5130648\n",
            " 0.5085712  0.5041165  0.5054109  0.5062576  0.50367504 0.50715363\n",
            " 0.5078662  0.5149659  0.503044   0.51083326 0.504711   0.50837004\n",
            " 0.52220404 0.51819193 0.50212747 0.5141047  0.5017817  0.50790507\n",
            " 0.50296104 0.50357115 0.52439183 0.50692564 0.5037049  0.5059935\n",
            " 0.510908   0.50157416 0.5053107  0.505224   0.5125222  0.5117955\n",
            " 0.5011943  0.50386506 0.50125015 0.50688416 0.506654   0.5017295\n",
            " 0.5000777  0.5018267  0.5023599  0.5049364  0.50749475 0.50401294\n",
            " 0.50173146 0.50050926 0.50365597 0.5265676  0.5228046  0.51200783\n",
            " 0.5003841  0.5077964  0.5058656  0.50802916 0.5087256  0.51917595\n",
            " 0.50084126 0.5186798  0.50430757 0.5088568  0.50740093 0.5126349\n",
            " 0.51294726 0.5000491  0.52390736 0.5011089  0.50422025 0.5101315\n",
            " 0.50420636 0.5042289  0.52491903 0.5013901  0.52284896 0.5080869\n",
            " 0.51950586 0.50602144 0.50623804 0.5029797  0.51214653 0.50686336\n",
            " 0.5132776  0.5012557  0.50195765 0.50355506 0.50069153 0.51707625\n",
            " 0.5075685  0.50235754 0.5059704  0.5002502  0.50101507 0.5019987\n",
            " 0.5067777  0.502722   0.5039628  0.50262046 0.50004673 0.50452906\n",
            " 0.50997514 0.50608534 0.51494837 0.50133586 0.5016565  0.51756483\n",
            " 0.5030912  0.5114606  0.5032206  0.50060004 0.5006092  0.529583\n",
            " 0.5191792  0.50323564 0.5053469  0.50084573 0.5008867  0.50571615\n",
            " 0.50112695 0.50595856 0.5006478  0.5046607  0.53673226 0.5038889\n",
            " 0.5143576  0.5022299  0.50380623 0.5072644  0.510369   0.5002702\n",
            " 0.50190365 0.5112646  0.5019383  0.50099313 0.5003257  0.5045886\n",
            " 0.5082756  0.5012811 ]\n",
            "\n",
            "Matched draws\n",
            "Count: 15, Index: (array([ 392, 1475, 1519, 1884, 2030, 2785, 3057, 3400, 3692, 3891, 4271,\n",
            "       4728, 5227, 5796, 7603]),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "        DrawNo   DrawDate   PrizeType  LuckyNo\n",
            "107356  507520 2020-02-15  SpecialNo2      392\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "        DrawNo   DrawDate   PrizeType  LuckyNo\n",
            "107356  507520 2020-02-15  SpecialNo2      392\n",
            "\n",
            "\n",
            "All matched\n",
            "        DrawNo   DrawDate       PrizeType  LuckyNo\n",
            "107180  506820 2020-02-01      1stPrizeNo     5796\n",
            "107193  506820 2020-02-01      SpecialNo1     2785\n",
            "107199  506820 2020-02-01      SpecialNo6     2030\n",
            "107206  506920 2020-02-02  ConsolationNo1     1519\n",
            "107239  507020 2020-02-04      SpecialNo1     3057\n",
            "107243  507020 2020-02-04      SpecialNo4     1884\n",
            "107284  507220 2020-02-08  ConsolationNo9     4728\n",
            "107285  507220 2020-02-08      SpecialNo1     7603\n",
            "107312  507320 2020-02-09      SpecialNo4     3692\n",
            "107329  507420 2020-02-12  ConsolationNo8     3891\n",
            "107334  507420 2020-02-12      SpecialNo3     5227\n",
            "107356  507520 2020-02-15      SpecialNo2      392\n",
            "107385  507620 2020-02-16      SpecialNo8     3400\n",
            "107466  508020 2020-02-26  ConsolationNo7     1475\n",
            "107469  508020 2020-02-26      SpecialNo1     4271\n",
            "CPU times: user 5h 9min 10s, sys: 5.32 s, total: 5h 9min 15s\n",
            "Wall time: 1h 18min 23s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRtzlp_pcBGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VE6Xbz-IyvLj",
        "colab": {}
      },
      "source": [
        "# weight=1.0\n",
        "# decrement = 0.000\n",
        "# to_stop=False\n",
        "\n",
        "# dt = pd.datetime(2020,1,1)\n",
        "# %time gen_train_test_set(dt, feature_matrix_selection, file_prefix='test')\n",
        "# while not to_stop:\n",
        "#   to_stop = model(dt, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#   decrement = decrement + 0.005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qktZbi7OGqP3",
        "colab": {}
      },
      "source": [
        "# start_mt = pd.datetime(2019,7,1)\n",
        "# how_many_mt = 6 \n",
        "# for i in range(how_many_mt):\n",
        "#   month_to_predict = start_mt + relativedelta(months=i)\n",
        "#   print(f\"\\n{month_to_predict}\\n-------------------\\n\")\n",
        "\n",
        "#   weight=1.0\n",
        "#   decrement = 0.000\n",
        "#   to_stop=False\n",
        "\n",
        "#   gen_train_test_set(month_to_predict, feature_matrix_selection, file_prefix='test')\n",
        "#   while not to_stop:\n",
        "#     to_stop = model(month_to_predict, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#     decrement = decrement + 0.001\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N8tcqn4yIl21",
        "colab": {}
      },
      "source": [
        "# weight=1.0\n",
        "# decrement = 0.000\n",
        "# to_stop=False\n",
        "\n",
        "# dt = pd.datetime(2020,2,1)\n",
        "# %time gen_train_test_set(dt, feature_matrix_selection, file_prefix='test')\n",
        "# while not to_stop:\n",
        "#   to_stop = model(dt, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#   decrement = decrement + 0.005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1SC8YG8P_ljs",
        "colab": {}
      },
      "source": [
        "# weight=1.0\n",
        "# decrement = 0.000\n",
        "# to_stop=False\n",
        "\n",
        "# dt = pd.datetime(2020,3,1)\n",
        "# %time gen_train_test_set(dt, feature_matrix_selection, file_prefix='test')\n",
        "# while not to_stop:\n",
        "#   to_stop = model(dt, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#   decrement = decrement + 0.00"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yASd8nZFR7Qb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "153K-Fpie_vd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target_mt = pd.datetime(2020,3,1)\n",
        "%time gen_train_test_set(target_mt, feature_matrix_selection, file_prefix='test')\n",
        "\n",
        "for (estimators, depth) in ((300,3), (500,6), (550,6), (600,6)):\n",
        "  %time model(target_mt, feature_matrix_selection, file_prefix='test', estimators=estimators, depth=depth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4FzHxatfCd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}