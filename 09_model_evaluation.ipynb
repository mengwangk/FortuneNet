{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Accuracy = \\frac{(TP + TN)} {(TP + TN + FP + FN)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Precision = \\frac {(TP)} {(TP + FP)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Recall = \\frac {(TP)} {(TP + FN)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ F1 = 2 * \\frac{precision * recall} {precision + recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_true = [0,1,1,0,1,1]\n",
    "y_pred = [0,0,1,0,0,1]\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Beta Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "harmonic mean - https://deepai.org/machine-learning-glossary-and-terms/harmonic-mean\n",
    "\n",
    "$$ F1_{\\beta} = (1 + \\beta^2)* \\frac{precision * recall} { \\beta^2 * precision + recall}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333333333333334"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "fbeta_score(y_true, y_pred, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fbeta_score?\n",
    "# f1_score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Loss/Binary Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ -(y log(p) + (1-y)log(1-p) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.512925464970229"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "log_loss(y_true, y_pred, eps=1e-15)\n",
    "#log_loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Log loss = \\frac {-1} {N}\\sum_{i=1}^{n} \\sum_{i=1}^{m} y_{ij} * log(p_{ij}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ sensitivity = TPR = Recall = \\frac {TP} {TP+FP} $$\n",
    "\n",
    "$$ 1 - specificity = FPR = \\frac {FP} {TN +FP} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m[\u001b[0m\u001b[0;34m'y_true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y_score'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pos_label=None'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sample_weight=None'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'drop_intermediate=True'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute Receiver operating characteristic (ROC)\n",
       "\n",
       "Note: this implementation is restricted to the binary classification task.\n",
       "\n",
       "Read more in the :ref:`User Guide <roc_metrics>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "\n",
       "y_true : array, shape = [n_samples]\n",
       "    True binary labels. If labels are not either {-1, 1} or {0, 1}, then\n",
       "    pos_label should be explicitly given.\n",
       "\n",
       "y_score : array, shape = [n_samples]\n",
       "    Target scores, can either be probability estimates of the positive\n",
       "    class, confidence values, or non-thresholded measure of decisions\n",
       "    (as returned by \"decision_function\" on some classifiers).\n",
       "\n",
       "pos_label : int or str, default=None\n",
       "    Label considered as positive and others are considered negative.\n",
       "\n",
       "sample_weight : array-like of shape = [n_samples], optional\n",
       "    Sample weights.\n",
       "\n",
       "drop_intermediate : boolean, optional (default=True)\n",
       "    Whether to drop some suboptimal thresholds which would not appear\n",
       "    on a plotted ROC curve. This is useful in order to create lighter\n",
       "    ROC curves.\n",
       "\n",
       "    .. versionadded:: 0.17\n",
       "       parameter *drop_intermediate*.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "fpr : array, shape = [>2]\n",
       "    Increasing false positive rates such that element i is the false\n",
       "    positive rate of predictions with score >= thresholds[i].\n",
       "\n",
       "tpr : array, shape = [>2]\n",
       "    Increasing true positive rates such that element i is the true\n",
       "    positive rate of predictions with score >= thresholds[i].\n",
       "\n",
       "thresholds : array, shape = [n_thresholds]\n",
       "    Decreasing thresholds on the decision function used to compute\n",
       "    fpr and tpr. `thresholds[0]` represents no instances being predicted\n",
       "    and is arbitrarily set to `max(y_score) + 1`.\n",
       "\n",
       "See also\n",
       "--------\n",
       "roc_auc_score : Compute the area under the ROC curve\n",
       "\n",
       "Notes\n",
       "-----\n",
       "Since the thresholds are sorted from low to high values, they\n",
       "are reversed upon returning them to ensure they correspond to both ``fpr``\n",
       "and ``tpr``, which are sorted in reversed order during their calculation.\n",
       "\n",
       "References\n",
       "----------\n",
       ".. [1] `Wikipedia entry for the Receiver operating characteristic\n",
       "        <https://en.wikipedia.org/wiki/Receiver_operating_characteristic>`_\n",
       "\n",
       ".. [2] Fawcett T. An introduction to ROC analysis[J]. Pattern Recognition\n",
       "       Letters, 2006, 27(8):861-874.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> import numpy as np\n",
       ">>> from sklearn import metrics\n",
       ">>> y = np.array([1, 1, 2, 2])\n",
       ">>> scores = np.array([0.1, 0.4, 0.35, 0.8])\n",
       ">>> fpr, tpr, thresholds = metrics.roc_curve(y, scores, pos_label=2)\n",
       ">>> fpr\n",
       "array([0. , 0. , 0.5, 0.5, 1. ])\n",
       ">>> tpr\n",
       "array([0. , 0.5, 0.5, 1. , 1. ])\n",
       ">>> thresholds\n",
       "array([1.8 , 0.8 , 0.4 , 0.35, 0.1 ])\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/ranking.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "y_true = np.array([0,0,1,1])\n",
    "y_scores = np.array([0.1,0.4,0.35,0.8])\n",
    "print(roc_auc_score(y_true, y_scores))\n",
    "# roc_auc_score?\n",
    "roc_curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hinge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kullback-Leibler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
