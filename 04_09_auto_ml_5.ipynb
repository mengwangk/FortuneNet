{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "04_02_automated_machine_learning.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mengwangk/dl-projects/blob/master/04_09_auto_ml_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4hyoPGdjpqa_"
      },
      "source": [
        "# Automated ML - Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SLxr2k_ue8yq",
        "colab": {}
      },
      "source": [
        "COLAB = True\n",
        "\n",
        "DATASET_NAME = '4D.zip'\n",
        "\n",
        "FEATURE_DATASET_PREFIX = 'feature_matrix_d2_v3'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wwYshXtLt7b7",
        "colab": {}
      },
      "source": [
        "#!pip install -U imblearn\n",
        "#!pip install -U xgboost\n",
        "# !pip install -U featuretools\n",
        "\n",
        "# https://towardsdatascience.com/handling-imbalanced-datasets-in-machine-learning-7a0e84220f28\n",
        "# https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/\n",
        "# https://machinelearningmastery.com/imbalanced-classification-model-to-detect-oil-spills/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oy5ww2zRfFGG",
        "outputId": "edb89f73-896e-4ad2-f1f3-788bb9532375",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "if COLAB:\n",
        "  !rm -rf dl-projects\n",
        "  !git clone https://github.com/mengwangk/dl-projects"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dl-projects'...\n",
            "remote: Enumerating objects: 145, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (138/138), done.\u001b[K\n",
            "remote: Total 1947 (delta 90), reused 17 (delta 7), pack-reused 1802\u001b[K\n",
            "Receiving objects: 100% (1947/1947), 78.24 MiB | 27.18 MiB/s, done.\n",
            "Resolving deltas: 100% (1201/1201), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G2xin10SfozR",
        "colab": {}
      },
      "source": [
        "if COLAB:\n",
        "  !cp dl-projects/utils* .\n",
        "  !cp dl-projects/preprocess* .\n",
        "  !cp dl-projects/plot* ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fC2-l3JBpqbE",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "# %reload_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TP7V_IzepqbK",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import math \n",
        "import matplotlib\n",
        "import sys\n",
        "import gc\n",
        "\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import featuretools as ft\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, precision_recall_curve, make_scorer, recall_score, roc_curve, mean_squared_error, accuracy_score, average_precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit, RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier\n",
        "from imblearn.under_sampling import (RandomUnderSampler, \n",
        "                                     ClusterCentroids,\n",
        "                                     TomekLinks,\n",
        "                                     NeighbourhoodCleaningRule,\n",
        "                                     AllKNN,\n",
        "                                     NearMiss,\n",
        "                                     OneSidedSelection,\n",
        "                                     EditedNearestNeighbours)\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
        "from imblearn.metrics import classification_report_imbalanced, geometric_mean_score\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC, ADASYN \n",
        "import pylab as pl\n",
        "import xgboost as xgb\n",
        "from collections import Counter\n",
        "from dateutil.relativedelta import relativedelta\n",
        "\n",
        "\n",
        "# from skopt import BayesSearchCV\n",
        "# from skopt.space import Real, Categorical, Integer\n",
        "# from scikitplot.plotters import plot_precision_recall_curve\n",
        "\n",
        "from utils import feature_selection, plot_feature_importances\n",
        "from preprocess import *\n",
        "from plot import plot_correlation_matrix, plot_labeled_scatter\n",
        "\n",
        "from IPython.display import display\n",
        "\n",
        "np.set_printoptions(threshold=sys.maxsize)\n",
        "\n",
        "plt.style.use('fivethirtyeight')\n",
        "\n",
        "sns.set(style=\"ticks\")\n",
        "\n",
        "# The Answer to the Ultimate Question of Life, the Universe, and Everything.\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3bFT5CoxpqbP",
        "outputId": "e3a7c961-8816-4141-cd6e-4868b438b0a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%aimport"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Modules to reload:\n",
            "all-except-skipped\n",
            "\n",
            "Modules to skip:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3E16jPVPpqbV"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U421BuhtfYS7",
        "outputId": "d4939e2d-d19b-4d46-b621-3b9e786fb8b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "if COLAB:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  GDRIVE_DATASET_FOLDER = Path('gdrive/My Drive/datasets/')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9IgnETKkpqbX",
        "outputId": "82a4e739-3e1c-45ab-de0d-b991b161fa5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "if COLAB:\n",
        "  DATASET_PATH = GDRIVE_DATASET_FOLDER\n",
        "  ORIGIN_DATASET_PATH = Path('dl-projects/datasets')\n",
        "else:\n",
        "  DATASET_PATH = Path(\"../datasets\")\n",
        "  ORIGIN_DATASET_PATH = Path('datasets')\n",
        "\n",
        "DATASET = DATASET_PATH/f\"{FEATURE_DATASET_PREFIX}.ft\"\n",
        "ORIGIN_DATASET = ORIGIN_DATASET_PATH/DATASET_NAME\n",
        "\n",
        "if COLAB:\n",
        "  !ls -l gdrive/\"My Drive\"/datasets/ --block-size=M\n",
        "  !ls -l dl-projects/datasets --block-size=M"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4800M\n",
            "-rw------- 1 root root   17M Mar  1 05:47 feature_matrix_2020_feb.ft\n",
            "-rw------- 1 root root   17M Mar  1 05:39 feature_matrix_2020_jan.ft\n",
            "-rw------- 1 root root   17M Mar  1 05:56 feature_matrix_2020_mar.ft\n",
            "-rw------- 1 root root 2454M Jan 12 01:24 feature_matrix_d2_v2.ft\n",
            "-rw------- 1 root root 1585M Jan 12 23:39 feature_matrix_d2_v3.ft\n",
            "-rw------- 1 root root   17M Feb 21 13:07 feature_matrix_snapshot.ft\n",
            "-rw------- 1 root root    5M Jan 30 04:33 orig_X_test.ft\n",
            "-rw------- 1 root root  415M Jan 30 04:33 orig_X_train.ft\n",
            "-rw------- 1 root root    1M Jan 30 04:33 orig_y_test.ft\n",
            "-rw------- 1 root root    7M Jan 30 04:33 orig_y_train.ft\n",
            "-rw------- 1 root root    3M Mar  1 12:56 test_X_test.ft\n",
            "-rw------- 1 root root  259M Mar  1 12:56 test_X_train.ft\n",
            "-rw------- 1 root root    1M Mar  1 12:56 test_y_test.ft\n",
            "-rw------- 1 root root    8M Mar  1 12:56 test_y_train.ft\n",
            "total 25M\n",
            "-rw-r--r-- 1 root root  1M Mar  2 12:55 4D.zip\n",
            "-rw-r--r-- 1 root root 25M Mar  2 12:55 labels.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "urQTD6DQNutw",
        "colab": {}
      },
      "source": [
        "# Read the data\n",
        "data = pd.read_feather(DATASET)\n",
        "origin_data = format_tabular(ORIGIN_DATASET)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Zov05QHZxxiS"
      },
      "source": [
        "## Add new data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "foPB8T1vx2tp",
        "outputId": "92857eeb-8f8d-48b4-f20f-f69f32575268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(959893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "43sc1Eaux25j",
        "colab": {}
      },
      "source": [
        "jan_2020 = pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_jan.ft\")\n",
        "feb_2020= pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_feb.ft\")\n",
        "mar_2020= pd.read_feather(DATASET_PATH/f\"feature_matrix_2020_mar.ft\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2vISxEbsyQG1",
        "outputId": "0c35a4c1-9a66-4e38-86b7-fe4dff49e858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_data = data.append(jan_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(feb_2020[data.columns],ignore_index=True)\n",
        "new_data = new_data.append(mar_2020[data.columns],ignore_index=True)\n",
        "new_data.shape "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FVVMXCj-zyaW",
        "outputId": "306ba4d6-6195-4895-d014-707af22bc64a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = new_data\n",
        "data.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 217)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vOYlp-8Br61r"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kHiN1VVlG9Kh"
      },
      "source": [
        "### View data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JnQXyVqng5Cm",
        "colab": {}
      },
      "source": [
        "# Feature matrix\n",
        "feature_matrix = data.drop(columns=['NumberId', 'month', 'year'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "apMYVNz9HK9e",
        "outputId": "0f5aa619-a16d-4eb5-a2e5-4f2d4e113a41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Sort data\n",
        "feature_matrix.sort_values(by=['time', 'MAX(Results.LuckyNo)'], inplace=True)\n",
        "feature_matrix.info()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 989893 entries, 7020 to 986394\n",
            "Columns: 214 entries, time to LAST(Results.PrizeType)_Prize\n",
            "dtypes: datetime64[ns](1), float64(155), int64(56), uint8(2)\n",
            "memory usage: 1.6 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZKTbWRFJNUq",
        "outputId": "45c66db4-64b9-42d0-8c01-6cfdf6a07b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "print('Distribution')\n",
        "print(feature_matrix['Label'].value_counts())\n",
        "print()\n",
        "print('Positive: ' + str(feature_matrix['Label'].value_counts()[0]) + ' which is ', round(feature_matrix['Label'].value_counts()[0]/len(feature_matrix) * 100,2), '% of the dataset')\n",
        "print('Negative: ' + str(feature_matrix['Label'].value_counts()[1]) + ' which is ', round(feature_matrix['Label'].value_counts()[1]/len(feature_matrix) * 100,2), '% of the dataset')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Distribution\n",
            "0    957191\n",
            "1     32702\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Positive: 957191 which is  96.7 % of the dataset\n",
            "Negative: 32702 which is  3.3 % of the dataset\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "plplpAQ6JrKb",
        "outputId": "7097a1c4-9907-4bf7-e82c-f1c54625ec0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "feature_matrix.isna().sum().sort_values(ascending=False)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))               7657\n",
              "CUM_MEAN(TREND(Results.LuckyNo, DrawDate))                7657\n",
              "TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)    7657\n",
              "TREND(Results.CUM_MEAN(TotalStrike), DrawDate)            7657\n",
              "TREND(Results.CUM_SUM(LuckyNo), DrawDate)                 7657\n",
              "                                                          ... \n",
              "CUM_SUM(MIN(Results.DrawNo))                                 0\n",
              "NUM_UNIQUE(Results.DAY(DrawDate))                            0\n",
              "NUM_UNIQUE(Results.MONTH(DrawDate))                          0\n",
              "SUM(Results.PERCENTILE(LuckyNo))                             0\n",
              "time                                                         0\n",
              "Length: 214, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zF_zCRksL1Ls"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S1aLGsXSOa9K",
        "colab": {}
      },
      "source": [
        "# Fill all NaN with 0\n",
        "feature_matrix = feature_matrix.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5A8LZ805MqjP",
        "outputId": "a315417d-47cd-40c7-a5b6-c33a018e0d8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "feature_matrix.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(989893, 214)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rPFOkiGjhuKj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "0fb6e1bf-3e47-43fc-f46b-eaf0020ee560"
      },
      "source": [
        "feature_matrix_selection = feature_selection(feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label']))\n",
        "# feature_matrix_selection = feature_matrix.drop(columns = ['time', 'TotalStrike', 'Label'])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original shape:  (989893, 211)\n",
            "0 missing columns with threshold: 90.\n",
            "41 zero variance columns.\n",
            "108 collinear columns removed with threshold: 0.95.\n",
            "Total columns removed:  149\n",
            "Shape after feature selection: (989893, 62).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vT2K0WeJhugH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3adaf7e9-c96a-4302-caa4-fe4fb93f3d06"
      },
      "source": [
        "feature_matrix_selection.shape, feature_matrix_selection.columns"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((989893, 62),\n",
              " Index(['STD(Results.DrawNo)', 'MAX(Results.DrawNo)', 'MAX(Results.LuckyNo)',\n",
              "        'MIN(Results.DrawNo)', 'MEAN(Results.DrawNo)', 'SKEW(Results.DrawNo)',\n",
              "        'AVG_TIME_BETWEEN(Results.DrawDate)', 'COUNT(Results)',\n",
              "        'SUM(Results.DrawNo)', 'SUM(Results.LuckyNo)',\n",
              "        'TREND(Results.DrawNo, DrawDate)', 'MONTH(first_Results_time)',\n",
              "        'DAY(first_Results_time)', 'TIME_SINCE(first_Results_time)',\n",
              "        'TIME_SINCE_PREVIOUS(first_Results_time)',\n",
              "        'STD(Results.PERCENTILE(DrawNo))',\n",
              "        'STD(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'STD(Results.CUM_SUM(DrawNo))', 'STD(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MAX(Results.PERCENTILE(DrawNo))',\n",
              "        'MAX(Results.PERCENTILE(TotalStrike))',\n",
              "        'MAX(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MIN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MIN(Results.CUM_SUM(DrawNo))', 'MIN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'MODE(Results.MONTH(DrawDate))', 'MODE(Results.DAY(DrawDate))',\n",
              "        'MEAN(Results.TIME_SINCE(DrawDate))',\n",
              "        'MEAN(Results.PERCENTILE(DrawNo))',\n",
              "        'MEAN(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'MEAN(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SKEW(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'SKEW(Results.CUM_SUM(DrawNo))', 'SKEW(Results.CUM_MEAN(LuckyNo))',\n",
              "        'LAST(Results.DAY(DrawDate))',\n",
              "        'LAST(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'LAST(Results.MONTH(DrawDate))', 'LAST(Results.CUM_MEAN(LuckyNo))',\n",
              "        'SUM(Results.TIME_SINCE(DrawDate))',\n",
              "        'SUM(Results.TIME_SINCE_PREVIOUS(DrawDate))',\n",
              "        'TREND(Results.CUM_MEAN(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(LuckyNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(DrawNo), DrawDate)',\n",
              "        'TREND(Results.PERCENTILE(TotalStrike), DrawDate)',\n",
              "        'TREND(Results.TIME_SINCE_PREVIOUS(DrawDate), DrawDate)',\n",
              "        'TREND(Results.CUM_SUM(DrawNo), DrawDate)',\n",
              "        'NUM_UNIQUE(Results.MONTH(DrawDate))',\n",
              "        'NUM_UNIQUE(Results.DAY(DrawDate))', 'CUM_SUM(MIN(Results.DrawNo))',\n",
              "        'CUM_SUM(SKEW(Results.DrawNo))',\n",
              "        'CUM_MEAN(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'CUM_MEAN(SUM(Results.LuckyNo))', 'CUM_MEAN(SKEW(Results.DrawNo))',\n",
              "        'PERCENTILE(STD(Results.LuckyNo))', 'PERCENTILE(LAST(Results.DrawNo))',\n",
              "        'PERCENTILE(MAX(Results.TotalStrike))',\n",
              "        'PERCENTILE(AVG_TIME_BETWEEN(Results.DrawDate))',\n",
              "        'PERCENTILE(COUNT(Results))', 'PERCENTILE(STD(Results.DrawNo))',\n",
              "        'PERCENTILE(SKEW(Results.DrawNo))', 'PERCENTILE(SUM(Results.DrawNo))',\n",
              "        'PERCENTILE(TREND(Results.DrawNo, DrawDate))'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yZUhYrWFiRod",
        "colab": {}
      },
      "source": [
        "feature_matrix_selection['time'] = feature_matrix['time']\n",
        "feature_matrix_selection['TotalStrike'] = feature_matrix['TotalStrike']\n",
        "feature_matrix_selection['Label'] = feature_matrix['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hugygOqSiR6K"
      },
      "source": [
        "### Feature Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "loagcqTEKOkO",
        "colab": {}
      },
      "source": [
        "#feature_matrix.isnull().sum().sort_values(ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u7Ha8Zlkhuoe",
        "colab": {}
      },
      "source": [
        "# Check with feature selection\n",
        "#corrs = feature_matrix_selection.corr().sort_values('Label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EWRODfAdPk6j",
        "colab": {}
      },
      "source": [
        "#corrs['Label'].head(60)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "waeD1ED_kqDB"
      },
      "source": [
        "## Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9yrJyIVLh5So",
        "colab": {}
      },
      "source": [
        "def recall_optim(y_true, y_pred):\n",
        "    \"\"\"Make a scoring function that improves specificity while identifying all strikes\n",
        "    \"\"\"\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    # Recall will be worth a greater value than specificity\n",
        "    rec = recall_score(y_true, y_pred) * 0.8 \n",
        "    spe = conf_matrix[0,0]/conf_matrix[0,:].sum() * 0.2 \n",
        "    \n",
        "    # Imperfect recalls will lose a penalty\n",
        "    # This means the best results will have perfect recalls and compete for specificity\n",
        "    if rec < 0.8:\n",
        "        rec -= 0.2\n",
        "    return rec + spe \n",
        "\n",
        "\n",
        "# Make a scoring callable from recall_score\n",
        "recall = make_scorer(recall_score)\n",
        "\n",
        "# Create a scoring callable based on the scoring function\n",
        "optimize = make_scorer(recall_optim)\n",
        "\n",
        "# Geometric mean scorer\n",
        "geo_mean_scorer = make_scorer(geometric_mean_score)\n",
        "\n",
        "# DataFrame to store classifier performance\n",
        "performance = pd.DataFrame(columns=['Train_Recall','Test_Recall','Test_Specificity', 'Optimize'])\n",
        "\n",
        "def to_labels(pos_probs, threshold):\n",
        "    \"\"\"Apply threshold to positive probabilities to create labels.\n",
        "    \"\"\"\n",
        "    return (pos_probs >= threshold).astype('int')\n",
        " \n",
        "\n",
        "def score_optimization(dt, feature_matrix, clf, params, X_train, y_train, X_test, y_test, skip_grid_search_cv=False, optimized_scorer=False):\n",
        "    \"\"\"Find the optimized classifier.\n",
        "    \"\"\"\n",
        "    if not skip_grid_search_cv:\n",
        "      print(\"\\nFinding the optimized classifier...\")\n",
        "\n",
        "      # Load GridSearchCV\n",
        "      # search = GridSearchCV(\n",
        "      search = RandomizedSearchCV(\n",
        "            estimator=clf,\n",
        "            #param_grid=params,\n",
        "            param_distributions=params,\n",
        "            n_jobs=4,\n",
        "            scoring=optimize  # Use custom scorer\n",
        "      )\n",
        "\n",
        "      # Train search object\n",
        "      search.fit(X_train, y_train)\n",
        "\n",
        "      # Heading\n",
        "      print('\\n','-'*40,'\\n',clf.__class__.__name__,'\\n','-'*40)\n",
        "\n",
        "      # Extract best estimator\n",
        "      best = search.best_estimator_\n",
        "      print('Best parameters: \\n\\n',search.best_params_,'\\n')\n",
        "    \n",
        "    else:\n",
        "      print(\"\\nUse the passed in classifier...\\n\")\n",
        "      best = clf\n",
        "\n",
        "    # Cross-validate on the train data\n",
        "    if not skip_grid_search_cv: \n",
        "      print(\"TRAIN GROUP\")\n",
        "      #cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)\n",
        "      cv = 3\n",
        "      if not optimized_scorer:\n",
        "        print('\\nUse default scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  scoring=recall,\n",
        "                                  cv=cv)\n",
        "      else:\n",
        "        print('\\nUse optimized scorer')\n",
        "        train_cv = cross_val_score(\n",
        "                                  n_jobs=4,\n",
        "                                  X=X_train, \n",
        "                                  y=y_train, \n",
        "                                  estimator=best, \n",
        "                                  #scoring=optimize,\n",
        "                                  scoring='roc_auc',\n",
        "                                  #scoring=geo_mean_scorer,\n",
        "                                  cv=cv)\n",
        "\n",
        "      print(\"\\nCross-validation recall scores:\",train_cv)\n",
        "      print(\"Mean recall score:\",train_cv.mean())\n",
        "      print('Mean G-Mean: %.3f (%.3f)' % (np.mean(train_cv), np.std(train_cv)))\n",
        "    else:\n",
        "      train_cv = np.zeros(3)\n",
        "\n",
        "    # Now predict on the test group\n",
        "    print(\"\\nTEST GROUP\")\n",
        "    y_pred = best.fit(X_train, y_train).predict(X_test)\n",
        "    # y_pred = best.fit(X_train, y_train,\n",
        "    #                   eval_set=[(X_test, y_test)],\n",
        "    #                   eval_metric='auc',\n",
        "    #                   early_stopping_rounds=10,\n",
        "    #                   verbose=True\n",
        "    #                   ).predict(X_test)\n",
        "\n",
        "    # keep probabilities for the positive outcome only\n",
        "    probas = best.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # define thresholds\n",
        "    thresholds = np.arange(0, 1, 0.001)\n",
        "\n",
        "    # evaluate each threshold\n",
        "    scores = [f1_score(y_test, to_labels(probas, t)) for t in thresholds]\n",
        "\n",
        "    # get best threshold\n",
        "    ix = np.argmax(scores)\n",
        "    print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix], scores[ix]))\n",
        "\n",
        "    # print recall\n",
        "    print(\"\\nRecall:\",recall_score(y_test,y_pred))\n",
        "\n",
        "    # Get imbalanced classification report\n",
        "    print(classification_report_imbalanced(y_test, y_pred))\n",
        "\n",
        "    # Print confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_test,y_pred)\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=plt.cm.copper)\n",
        "    plt.show()\n",
        "\n",
        "    # Store results\n",
        "    performance.loc[clf.__class__.__name__+'_optimize',:] = [\n",
        "        train_cv.mean(),\n",
        "        recall_score(y_test,y_pred),\n",
        "        conf_matrix[0,0]/conf_matrix[0,:].sum(),\n",
        "        recall_optim(y_test,y_pred)\n",
        "    ]\n",
        "    # Look at the parameters for the top best scores\n",
        "    if not skip_grid_search_cv:\n",
        "      display(pd.DataFrame(search.cv_results_).iloc[:,4:].sort_values(by='rank_test_score').head())\n",
        "    display(performance)\n",
        "\n",
        "    # Additionl info\n",
        "    print('\\n\\nAdditional Info')\n",
        "    print('='*40)\n",
        "    positive = np.where((y_pred==1))\n",
        "    print(f'Total predicted to be positive: {len(positive[0])} \\n')\n",
        "\n",
        "    pred = np.where((y_pred==1))\n",
        "    all_preds = pred[0]\n",
        "\n",
        "    # Total predicted matches\n",
        "    print('First 23 matches')\n",
        "    print(23, all_preds[0:23])\n",
        "    print(f'\\n{probas[all_preds[0:23]]}\\n') \n",
        "\n",
        "    print(\"\\nTop 23 Probable Matches\")\n",
        "    #print('probas', probas)\n",
        "    topN = np.argpartition(probas, -23)[-23:]\n",
        "    print(f'\\n{topN}\\n')          # Top N most high probability numbers\n",
        "    print(f'\\n{probas[topN]}\\n')  # Top N probability\n",
        "\n",
        "    # Check for 2 to 3 digits range \n",
        "    print('\\n2 To 3 Digits\\n')\n",
        "    idx_range = np.where((all_preds < 1000) & (all_preds >= 10))\n",
        "    #print(idx_range)\n",
        "    range_numbers = all_preds[idx_range]\n",
        "    print(len(range_numbers), range_numbers)\n",
        "    print(f'\\n{probas[range_numbers]}\\n') \n",
        "\n",
        "\n",
        "    # 2 to 3 Digits > Average Probas\n",
        "    print('\\n2 To 3 Digits Average Proba\\n')\n",
        "    avg_proba = np.average(probas[range_numbers])\n",
        "    print(f'Average proba {avg_proba}\\n')\n",
        "    idx_avg_proba = np.where(probas > avg_proba) \n",
        "    print(len(idx_avg_proba[0]), idx_avg_proba[0])\n",
        "\n",
        "    # 2 to 3 Digits > All Average Probas\n",
        "    print('\\n\\nAll Average Proba\\n')\n",
        "    all_avg_proba = np.average(probas[all_preds])\n",
        "    print(f'All average probas {all_avg_proba}\\n')\n",
        "    idx_all_avg_proba = np.where(probas > all_avg_proba) \n",
        "    print(len(idx_all_avg_proba[0]), idx_all_avg_proba[0])\n",
        "\n",
        "\n",
        "    # All predicted matches\n",
        "    print('\\n\\nAll Predictions\\n')\n",
        "    print(len(all_preds), all_preds)\n",
        "    print()\n",
        "    print(len(probas[all_preds]), probas[all_preds])\n",
        "    \n",
        "    #print('Debug')\n",
        "    #print(pred)\n",
        "    \n",
        "    if len(positive[0]) > 0:\n",
        "    \n",
        "      # Matching draws\n",
        "      print('\\nMatched draws')\n",
        "      md = np.where((y_pred==1) & (y_test==1))\n",
        "      print(f\"Count: {len(md[0])}, Index: {md}\")\n",
        "      month_data = feature_matrix.loc[feature_matrix['time'] == dt]\n",
        "      numbers = month_data.iloc[md[0]][['MAX(Results.LuckyNo)']]\n",
        "\n",
        "      print('\\n\\nTop 23 Possibility')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(topN))].head(23))  \n",
        "      \n",
        "      print('\\n\\nFirst 23 Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(pred[0][0:23]))].head(23))    \n",
        "             \n",
        "\n",
        "      print('\\n\\n2 To 3 Digits Numbers')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(range_numbers))].head(23))    \n",
        "     \n",
        "\n",
        "      print('\\n\\nAll matched')\n",
        "      print(origin_data[(origin_data['DrawDate'].dt.year == dt.year) & \n",
        "                          (origin_data['DrawDate'].dt.month == dt.month) &  \n",
        "                          (origin_data['LuckyNo'].isin(numbers['MAX(Results.LuckyNo)']))].head(100))    \n",
        "                                                  \n",
        "    else:\n",
        "      print('No luck this month')  \n",
        "\n",
        "    if len(range_numbers) >= 50:\n",
        "      return False\n",
        "\n",
        "    return True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VrL8gYwjc-hd",
        "colab": {}
      },
      "source": [
        "def remove_outliers(balanced, threshold=0.001, remove=True):\n",
        "    \"\"\"Removing Outliers from high-correlation features.\n",
        "    \"\"\"\n",
        "\n",
        "    if not remove:\n",
        "      return balanced\n",
        "\n",
        "    bal_corr = balanced.corr()\n",
        "    no_outliers=pd.DataFrame(balanced.copy())\n",
        "\n",
        "    cols = bal_corr.Label.index[:-1]\n",
        "\n",
        "    # For each feature correlated with Class...\n",
        "    for col in cols:\n",
        "        # If absolute correlation value is more than X percent...\n",
        "        correlation = bal_corr.loc['Label',col]\n",
        "\n",
        "        if np.absolute(correlation) > threshold:\n",
        "          # Separate the classes of the high-correlation column\n",
        "          nonstrikes = no_outliers.loc[no_outliers.Label==0,col]\n",
        "          strikes = no_outliers.loc[no_outliers.Label==1,col]\n",
        "\n",
        "          # Identify the 25th and 75th quartiles\n",
        "          all_values = no_outliers.loc[:,col]\n",
        "          q25, q75 = np.percentile(all_values, 25), np.percentile(all_values, 75)\n",
        "          # Get the inter quartile range\n",
        "          iqr = q75 - q25\n",
        "          # Smaller cutoffs will remove more outliers\n",
        "          cutoff = iqr * 7\n",
        "          # Set the bounds of the desired portion to keep\n",
        "          lower, upper = q25 - cutoff, q75 + cutoff\n",
        "          \n",
        "          # If positively correlated...\n",
        "          # Drop nonstrikes above upper bound, and strikes below lower bound\n",
        "          if correlation > 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes>upper].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes<lower].index,inplace=True)\n",
        "          \n",
        "          # If negatively correlated...\n",
        "          # Drop non strikes below lower bound, and strikes above upper bound\n",
        "          elif correlation < 0: \n",
        "              no_outliers.drop(index=nonstrikes[nonstrikes<lower].index,inplace=True)\n",
        "              no_outliers.drop(index=strikes[strikes>upper].index,inplace=True)\n",
        "        \n",
        "    print('\\nData shape before removing outliers:', balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in previous data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "    print('-'*40)\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after removing outliers:', no_outliers.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(no_outliers.Label.value_counts())\n",
        "\n",
        "    # no_outliers.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distributions with Less Outliers', fontsize=17)\n",
        "    # plt.show()\n",
        "    \n",
        "    no_outliers.reset_index(drop=True, inplace=True)\n",
        "    return no_outliers\n",
        "\n",
        "\n",
        "def filter_features(no_outliers, threshold=0.001):\n",
        "    \"\"\"Feature selection.\n",
        "    \"\"\"\n",
        "    feat_sel = pd.DataFrame(no_outliers.copy())\n",
        "\n",
        "    # Make a dataframe with the label-correlations before removing outliers\n",
        "    # corr_change = pd.DataFrame()\n",
        "    # corr_change['correlation']= bal_corr.Label\n",
        "    # corr_change['origin']= 'w/outliers'\n",
        "\n",
        "    # Make a dataframe with label-correlations after removing outliers \n",
        "    # corr_other = pd.DataFrame()\n",
        "    # corr_other['correlation']= feat_sel.corr().Label\n",
        "    # corr_other['origin']= 'no_outliers'\n",
        "\n",
        "    # Join them\n",
        "    # corr_change = corr_change.append(corr_other)\n",
        "\n",
        "    # plt.figure(figsize=(14,6))\n",
        "    # plt.xticks(rotation=90)\n",
        "\n",
        "    # Plot them\n",
        "    # sns.set_style('darkgrid')\n",
        "    # plt.title('Label correlation per feature. With vs without outliers', fontsize=17)\n",
        "    # sns.barplot(data=corr_change,x=corr_change.index,y='correlation',hue='origin')\n",
        "    # plt.show()\n",
        "\n",
        "    # Feature Selection based on correlation with label\n",
        "\n",
        "    print('\\nData shape before feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes before feature selection:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # Correlation matrix after removing outliers\n",
        "    new_corr = feat_sel.corr()\n",
        "\n",
        "    for col in new_corr.Label.index[:-1]:\n",
        "        # Pick desired cutoff for dropping features. In absolute-value terms.\n",
        "        if np.absolute(new_corr.loc['Label',col]) < threshold:\n",
        "            # Drop the feature if correlation is below cutoff\n",
        "            feat_sel.drop(columns=col,inplace=True)\n",
        "\n",
        "    print('-'*40)\n",
        "    print('\\nData shape after feature selection:', feat_sel.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in new data:')\n",
        "    print(feat_sel.Label.value_counts())\n",
        "\n",
        "    return feat_sel\n",
        "\n",
        "    # feat_sel.iloc[:,:-1].boxplot(rot=90,figsize=(16,4))\n",
        "    # plt.title('Distribution of Features Selected', fontsize=17)\n",
        "    # plt.show()\n",
        "\n",
        "def under_sampler(data, sample_size=20000, sampling=False):\n",
        "    # Undersample model for efficiency and balance classes.\n",
        "\n",
        "    X_train = data.drop('Label',1)\n",
        "    y_train = data.Label\n",
        "\n",
        "    if not sampling:\n",
        "      return X_train, y_train\n",
        "\n",
        "    # After feature-selection, X_test needs to include only the same features as X_train\n",
        "    # cols = X_train.columns\n",
        "    # X_test = X_test[cols]\n",
        "\n",
        "    # Undersample and balance classes\n",
        "    X_train, y_train = RandomUnderSampler(sampling_strategy={1:sample_size,0:sample_size}).fit_resample(X_train,y_train)\n",
        "\n",
        "    print('\\nX_train shape after reduction:', X_train.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in y_train:')\n",
        "    print(np.unique(y_train, return_counts=True))\n",
        "\n",
        "    return X_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pl5ZoepSNPf4",
        "colab": {}
      },
      "source": [
        "def gen_train_test_set(dt, feature_matrix, file_prefix='data'): \n",
        "    \n",
        "    # Subset labels\n",
        "    test_labels = feature_matrix.loc[feature_matrix['time'] == dt, 'Label']\n",
        "    train_labels = feature_matrix.loc[feature_matrix['time'] < dt, 'Label']\n",
        "\n",
        "    # Features\n",
        "    X_train = feature_matrix[feature_matrix['time'] < dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    X_test = feature_matrix[feature_matrix['time'] == dt].drop(columns = ['NumberId', 'time', 'Label', 'TotalStrike', 'month', 'year', 'index'], errors='ignore')\n",
        "    feature_names = list(X_train.columns)\n",
        "    \n",
        "    # Labels\n",
        "    y_train = np.array(train_labels).reshape((-1, ))\n",
        "    y_test = np.array(test_labels).reshape((-1, ))\n",
        "    \n",
        "    print('Training on {} observations.'.format(len(X_train)))\n",
        "    print('Testing on {} observations.\\n'.format(len(X_test)))\n",
        "\n",
        "    # Join the train data\n",
        "    train = X_train.join(train_labels)\n",
        "\n",
        "    print('Data shape before balancing:', train.shape)\n",
        "    print('\\nCounts of strikes vs non-strikes in previous data:')\n",
        "    print(train.Label.value_counts())\n",
        "    print('-'*40)\n",
        "\n",
        "    # sklearn pipeline\n",
        "    pipeline = make_pipeline(\n",
        "        SimpleImputer(strategy = 'constant', fill_value=0),\n",
        "        StandardScaler())\n",
        "    \n",
        "    X_train = pipeline.fit_transform(X_train)\n",
        "    X_test = pipeline.transform(X_test)\n",
        "\n",
        "    # imblearn pipeline\n",
        "    imb_pipeline = make_pipeline_imb(\n",
        "          # NearMiss(version=3, n_neighbors_ver3=3, n_jobs=4)\n",
        "          # SMOTE(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # ADASYN(sampling_strategy='minority',random_state=42, n_jobs=4)\n",
        "          # OneSidedSelection(n_neighbors=1, n_seeds_S=200, random_state=42, n_jobs=4)\n",
        "          SMOTEENN(enn=EditedNearestNeighbours(sampling_strategy='majority'))\n",
        "    )\n",
        "     \n",
        "    # Balance the data\n",
        "    to_balanced = False\n",
        "    if to_balanced:\n",
        "      print('\\nBalancing data')\n",
        "      X_bal, y_bal = imb_pipeline.fit_resample(X_train, y_train)\n",
        "      X_bal = pd.DataFrame(X_bal,columns=feature_names)\n",
        "      y_bal = pd.DataFrame(y_bal,columns=['Label'])\n",
        "    else:\n",
        "      print('\\nNO balancing')\n",
        "      X_bal = pd.DataFrame(X_train,columns=feature_names)\n",
        "      y_bal = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    balanced = X_bal.join(y_bal)\n",
        "\n",
        "    # print('-'*40)\n",
        "    print('Data shape after balancing:',balanced.shape)\n",
        "    print('\\nCounts of strikes VS non-strikes in new data:')\n",
        "    print(balanced.Label.value_counts())\n",
        "\n",
        "    # Remove high correlation outliers\n",
        "    no_outliers = remove_outliers(balanced, remove=False)\n",
        "   \n",
        "    # Remove features with low correlation\n",
        "    remove_features = True\n",
        "    if remove_features:\n",
        "      print('\\nFiltering features')\n",
        "      features_selected = filter_features(no_outliers)\n",
        "    else:\n",
        "      print('\\nNO filtering')\n",
        "      features_selected = no_outliers \n",
        "\n",
        "    columns_selected = features_selected.drop('Label',1).columns\n",
        "\n",
        "    # Under sampling\n",
        "    X_train, y_train = under_sampler(features_selected, sampling=False) \n",
        "    X_train = pd.DataFrame(X_train,columns=columns_selected)\n",
        "    y_train = pd.DataFrame(y_train,columns=['Label'])\n",
        "\n",
        "    # For X_test, now only use the selected features\n",
        "    X_test = pd.DataFrame(X_test,columns=feature_names)\n",
        "    X_test = X_test[columns_selected]\n",
        "    y_test = pd.DataFrame(y_test,columns=['Label'])\n",
        "\n",
        "    #print(X_train.describe())\n",
        "    #return\n",
        "\n",
        "    # Save data\n",
        "    # print(X_train.head(10))\n",
        "    # print(y_train.head(10)) \n",
        "\n",
        "    # print(X_test.head(10))\n",
        "    # print(y_test.head(10)) \n",
        "    X_train.to_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "    y_train.to_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "   \n",
        "    X_test.to_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "    y_test.to_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "\n",
        "    gc.collect()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PcKlL67TP9UM",
        "colab": {}
      },
      "source": [
        "def model(dt, feature_matrix, file_prefix='data', csv=False, class_weight=1.0):\n",
        "    \"\"\"Predict for a particular month.\n",
        "\n",
        "    - https://www.kaggle.com/miguelniblock/optimizing-imbalanced-classification-100-recall\n",
        "    - https://www.kaggle.com/saxinou/imbalanced-data-xgboost-tunning\n",
        "    - https://www.kaggle.com/andreanuzzo/balance-the-imbalanced-rf-and-xgboost-with-smote\n",
        "    - https://github.com/mengwangk/FraudDetection/blob/master/05_Sampling_techniques_for_extremely_imbalanced_data.ipynb\n",
        "    - https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets\n",
        "    - https://github.com/coding-maniacs/over-under-sampling/blob/master/src/main.py\n",
        "    - https://github.com/scikit-learn-contrib/imbalanced-learn/issues/552#issuecomment-466348310\n",
        "    - https://stackoverflow.com/questions/52499788/smotetomek-how-to-set-ratio-as-dictionary-for-fixed-balance\n",
        "    - https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.OneSidedSelection.html#imblearn.under_sampling.OneSidedSelection\n",
        "    - https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
        "    - https://machinelearningmastery.com/undersampling-algorithms-for-imbalanced-classification/\n",
        "    - https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "    - https://stats.stackexchange.com/questions/243207/what-is-the-proper-usage-of-scale-pos-weight-in-xgboost-for-imbalanced-datasets\n",
        "    - https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html#sphx-glr-auto-examples-svm-plot-oneclass-py\n",
        "    - https://machinelearningmastery.com/cost-sensitive-logistic-regression/\n",
        "    \n",
        "    - https://datascience.stackexchange.com/questions/28285/what-is-the-best-way-to-deal-with-imbalanced-data-for-xgboost/28292\n",
        "    - https://machinelearningmastery.com/xgboost-for-imbalanced-classification/\n",
        "    - https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Read data\n",
        "    if not csv:\n",
        "      X_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_train.ft\")\n",
        "      y_train = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_train.ft\")\n",
        "    \n",
        "      X_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_X_test.ft\")\n",
        "      y_test = pd.read_feather(DATASET_PATH/f\"{file_prefix}_y_test.ft\")\n",
        "    else:\n",
        "      X_train = pd.read_csv(DATASET_PATH/f\"{file_prefix}_X_train.csv\", header=0, sep=',', quotechar='\"')\n",
        "      y_train = pd.read_csv(DATASET_PATH/f\"{file_prefix}_y_train.csv\", header=0, sep=',', quotechar='\"')\n",
        "    \n",
        "      X_test = pd.read_csv(DATASET_PATH/f\"{file_prefix}_X_test.csv\", header=0, sep=',', quotechar='\"')\n",
        "      y_test = pd.read_csv(DATASET_PATH/f\"{file_prefix}_y_test.csv\", header=0, sep=',', quotechar='\"')\n",
        "\n",
        "\n",
        "    print(f'\\n-----------{dt}-----------------\\n')\n",
        "\n",
        "    # Reshape\n",
        "    y_train = np.array(y_train).reshape((-1, ))\n",
        "    y_test = np.array(y_test).reshape((-1, ))\n",
        "    \n",
        "    print('Data shape')\n",
        "    print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
        "\n",
        "    # print(X_train.describe())\n",
        "    # return\n",
        "\n",
        "    # Calculate hit ratio for xgboost classifier\n",
        "    print(\"\\nCalculating scale pos weight\")\n",
        "    counter = Counter(y_train)\n",
        "    print(Counter(y_train))\n",
        "    #scale_pos_weight = float(counter[0] / counter[1])\n",
        "    scale_pos_weight = (float(counter[0] / counter[1])) * class_weight\n",
        "    print(f\"\\nscale_pos_weight - {scale_pos_weight}\\n\")\n",
        "    \n",
        "     # Modeling\n",
        "#     clf = xgb.XGBClassifier(\n",
        "#               n_jobs=4, \n",
        "#               random_state=42,\n",
        "#               #learning_rate=0.1,\n",
        "#               #n_estimators=500,\n",
        "#               #max_depth=6, \n",
        "#               #min_child_weight=3, \n",
        "#               #gamma=0,\n",
        "#               #subsample=0.8,\n",
        "#               #colsample_bytree=0.8,\n",
        "#               objective='binary:logistic', \n",
        "#               scale_pos_weight=scale_pos_weight,\n",
        "#               ##eval_metric=\"auc\",\n",
        "#               ##max_delta_step=1,\n",
        "#               seed=27)\n",
        "#     clf = xgb.XGBClassifier(n_jobs=4, \n",
        "#                             random_state=42,\n",
        "#                             objective='binary:logistic', \n",
        "#                             #scale_pos_weight=28)\n",
        "#                             scale_pos_weight=scale_pos_weight)\n",
        "    \n",
        "    clf = xgb.XGBClassifier(\n",
        "                    n_jobs=4, \n",
        "                    random_state=42,\n",
        "                    objective='binary:logistic',\n",
        "                    subsample=0.55, \n",
        "                    #n_estimators=300,\n",
        "                    n_estimators=500,\n",
        "                    #n_estimators=550,\n",
        "                    min_child_weight=1,\n",
        "                    #max_depth=3, \n",
        "                    max_depth=6, \n",
        "                    learning_rate=0.007,\n",
        "                    gamma=0.1, \n",
        "                    colsample_bytree=0.95,\n",
        "                    tree_method='hist',\n",
        "                    booster='dart',\n",
        "                    scale_pos_weight=scale_pos_weight\n",
        "                    )\n",
        "\n",
        "    clf_params = clf.get_params()\n",
        "    print(clf_params)\n",
        "\n",
        "    # Set parameters\n",
        "    #clf_params['max_depth'] = 10\n",
        "    #clf.set_params(clf_params)\n",
        "\n",
        "    # Parameters to compare\n",
        "    weights = [i for i in range(1,36,1)]\n",
        "    weights.append(scale_pos_weight)\n",
        "    learn_params = {\n",
        "        'n_estimators': [100, 300, 500, 800, 1000], \n",
        "        'max_depth': range(3,10,2),\n",
        "        'min_child_weight': range(1,6,2),\n",
        "        #'gamma':[i/10.0 for i in range(0,5)],\n",
        "        'subsample':[i/100.0 for i in range(55,70,5)],\n",
        "        'colsample_bytree':[i/100.0 for i in range(85,100,5)],\n",
        "        #'learning_rate':[i/1000.0 for i in range(5,20,2)],\n",
        "        'scale_pos_weight': weights\n",
        "    }\n",
        "    print(f'Parameter distribution: {learn_params}')\n",
        "    \n",
        "    # Test and validate\n",
        "    ret_val = score_optimization(dt,\n",
        "                       feature_matrix,\n",
        "                       clf, \n",
        "                       learn_params,  \n",
        "                       X_train, \n",
        "                       y_train, \n",
        "                       X_test, \n",
        "                       y_test, \n",
        "                       skip_grid_search_cv=True,\n",
        "                       optimized_scorer=True)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    return ret_val\n",
        "    \n",
        "    # clf.fit(X_train, y_train)\n",
        "    # y_pred = clf.predict(X_test)\n",
        "\n",
        "    # # ROC score\n",
        "    # auc = roc_auc_score(y_test, y_pred)\n",
        "    # print(\"ROC score: \", auc)\n",
        "\n",
        "    # # Print confusion matrix\n",
        "    # conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    # sns.heatmap(conf_matrix, annot=True,fmt='d', cmap=plt.cm.copper)\n",
        "    # plt.show()\n",
        "\n",
        "    # Parameters to compare\n",
        "    # params = {\n",
        "    #     'criterion':['entropy','gini'],\n",
        "    #     'class_weight':[{1:1,0:0.3},{1:1,0:0.4},{1:1,0:0.5},{1:1,0:0.6},{1:1,0:7}]\n",
        "    # }\n",
        "\n",
        "    # Implement the classifier\n",
        "    # clf = RandomForestClassifier(\n",
        "    #     n_estimators=100,\n",
        "    #     max_features=None,\n",
        "    #     n_jobs=4,\n",
        "    # )\n",
        "\n",
        "    # # Test and validate\n",
        "    # score_optimization(clf, params, X_train, y_train, X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m9UobqUWMI9b",
        "jupyter": {
          "source_hidden": true
        },
        "colab": {}
      },
      "source": [
        "# Predict for a particular month\n",
        "\n",
        "# %time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection)\n",
        "\n",
        "#%time gen_train_test_set(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='test')\n",
        "#%time model(pd.datetime(2019,6,1), feature_matrix_selection, file_prefix='orig')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ns3Puh7Gnxl5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e0e55dd-77c5-47a3-b025-b384d8c2ae16"
      },
      "source": [
        "%time gen_train_test_set(pd.datetime(2020,3,1), feature_matrix_selection, file_prefix='test')\n",
        "%time model(pd.datetime(2020,3,1), feature_matrix_selection, file_prefix='test')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 979893 observations.\n",
            "Testing on 10000 observations.\n",
            "\n",
            "Data shape before balancing: (979893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes in previous data:\n",
            "0    947191\n",
            "1     32702\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "NO balancing\n",
            "Data shape after balancing: (979893, 63)\n",
            "\n",
            "Counts of strikes VS non-strikes in new data:\n",
            "0    947191\n",
            "1     32702\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Filtering features\n",
            "\n",
            "Data shape before feature selection: (979893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes before feature selection:\n",
            "0    947191\n",
            "1     32702\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "\n",
            "Data shape after feature selection: (979893, 35)\n",
            "\n",
            "Counts of strikes vs non-strikes in new data:\n",
            "0    947191\n",
            "1     32702\n",
            "Name: Label, dtype: int64\n",
            "CPU times: user 14.1 s, sys: 230 ms, total: 14.3 s\n",
            "Wall time: 14.6 s\n",
            "\n",
            "-----------2020-03-01 00:00:00-----------------\n",
            "\n",
            "Data shape\n",
            "(979893, 34) (979893,) (10000, 34) (10000,)\n",
            "\n",
            "Calculating scale pos weight\n",
            "Counter({0: 947191, 1: 32702})\n",
            "\n",
            "scale_pos_weight - 28.964314109228795\n",
            "\n",
            "{'base_score': 0.5, 'booster': 'dart', 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.95, 'gamma': 0.1, 'learning_rate': 0.007, 'max_delta_step': 0, 'max_depth': 6, 'min_child_weight': 1, 'missing': None, 'n_estimators': 500, 'n_jobs': 4, 'nthread': None, 'objective': 'binary:logistic', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'scale_pos_weight': 28.964314109228795, 'seed': None, 'silent': None, 'subsample': 0.55, 'verbosity': 1, 'tree_method': 'hist'}\n",
            "Parameter distribution: {'n_estimators': [100, 300, 500, 800, 1000], 'max_depth': range(3, 10, 2), 'min_child_weight': range(1, 6, 2), 'subsample': [0.55, 0.6, 0.65], 'colsample_bytree': [0.85, 0.9, 0.95], 'scale_pos_weight': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 28.964314109228795]}\n",
            "\n",
            "Use the passed in classifier...\n",
            "\n",
            "\n",
            "TEST GROUP\n",
            "Threshold=0.000, F-Score=0.00000\n",
            "\n",
            "Recall: 0.0\n",
            "                   pre       rec       spe        f1       geo       iba       sup\n",
            "\n",
            "          0       1.00      0.95      0.00      0.97      0.00      0.00     10000\n",
            "          1       0.00      0.00      0.95      0.00      0.00      0.00         0\n",
            "\n",
            "avg / total       1.00      0.95      0.00      0.97      0.00      0.00     10000\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAELCAYAAADz6wBxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train_Recall</th>\n",
              "      <th>Test_Recall</th>\n",
              "      <th>Test_Specificity</th>\n",
              "      <th>Optimize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>XGBClassifier_optimize</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.9506</td>\n",
              "      <td>-0.00988</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Train_Recall Test_Recall Test_Specificity Optimize\n",
              "XGBClassifier_optimize            0           0           0.9506 -0.00988"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Additional Info\n",
            "========================================\n",
            "Total predicted to be positive: 494 \n",
            "\n",
            "First 23 matches\n",
            "23 [ 72 210 283 315 372 391 446 496 502 509 538 552 553 587 590 625 630 642\n",
            " 652 666 708 728 729]\n",
            "\n",
            "[0.50707763 0.52533025 0.5127591  0.50393075 0.5087107  0.5093273\n",
            " 0.5006425  0.5086148  0.5150424  0.5146147  0.5073808  0.5182772\n",
            " 0.5002867  0.50506973 0.50012517 0.5022093  0.5011633  0.50241727\n",
            " 0.50120413 0.5047745  0.5008757  0.5105778  0.5021068 ]\n",
            "\n",
            "\n",
            "Top 23 Probable Matches\n",
            "\n",
            "[ 903 4089 6520 2377 2871 7587 2723 7402 3545 8499 5575 2335 4208 4880\n",
            " 9028 1754 3845 6326 2376  210 3111 2243 9595]\n",
            "\n",
            "\n",
            "[0.5235907  0.5245056  0.5238756  0.52501655 0.5265844  0.5250443\n",
            " 0.5255518  0.53344923 0.5332482  0.53903604 0.53103733 0.52513516\n",
            " 0.5289617  0.537929   0.52640605 0.5318129  0.5288399  0.54621804\n",
            " 0.5303178  0.52533025 0.54852355 0.53091156 0.5424325 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits\n",
            "\n",
            "39 [ 72 210 283 315 372 391 446 496 502 509 538 552 553 587 590 625 630 642\n",
            " 652 666 708 728 729 746 757 789 804 806 810 859 895 900 903 906 913 923\n",
            " 942 947 994]\n",
            "\n",
            "[0.50707763 0.52533025 0.5127591  0.50393075 0.5087107  0.5093273\n",
            " 0.5006425  0.5086148  0.5150424  0.5146147  0.5073808  0.5182772\n",
            " 0.5002867  0.50506973 0.50012517 0.5022093  0.5011633  0.50241727\n",
            " 0.50120413 0.5047745  0.5008757  0.5105778  0.5021068  0.5084943\n",
            " 0.50560254 0.50370556 0.5107835  0.5046049  0.50066316 0.51096714\n",
            " 0.50432944 0.50773966 0.5235907  0.5066677  0.50200325 0.5108607\n",
            " 0.5041389  0.5022995  0.5132455 ]\n",
            "\n",
            "\n",
            "2 To 3 Digits Average Proba\n",
            "\n",
            "Average proba 0.5072362422943115\n",
            "\n",
            "197 [ 210  283  372  391  496  502  509  538  552  728  746  804  859  900\n",
            "  903  923  994 1038 1083 1148 1232 1277 1282 1549 1597 1650 1751 1754\n",
            " 1811 1904 1905 1970 1972 1979 2071 2143 2144 2185 2209 2243 2266 2267\n",
            " 2335 2338 2350 2376 2377 2405 2415 2466 2532 2536 2572 2578 2609 2611\n",
            " 2617 2641 2692 2704 2723 2750 2871 2892 2895 2896 2899 2936 2958 3090\n",
            " 3102 3110 3111 3225 3334 3411 3416 3436 3438 3442 3472 3486 3494 3526\n",
            " 3545 3621 3664 3673 3692 3744 3812 3816 3830 3845 3928 3952 3953 3964\n",
            " 3968 4012 4016 4060 4089 4166 4180 4208 4256 4339 4349 4465 4473 4479\n",
            " 4498 4530 4562 4575 4609 4655 4657 4694 4707 4783 4880 5031 5050 5067\n",
            " 5154 5192 5213 5261 5401 5508 5575 5615 5725 5813 5950 6209 6210 6326\n",
            " 6451 6520 6620 6673 6683 6698 6710 6742 6744 6746 6772 6818 6874 6895\n",
            " 6919 6968 6978 6995 7057 7119 7136 7165 7231 7313 7371 7381 7402 7418\n",
            " 7434 7532 7554 7557 7587 7590 7690 7722 7730 7783 7890 7904 8078 8243\n",
            " 8361 8499 8747 8881 8930 9028 9064 9211 9400 9595 9610 9692 9755 9915\n",
            " 9926]\n",
            "\n",
            "\n",
            "All Average Proba\n",
            "\n",
            "All average probas 0.5075458884239197\n",
            "\n",
            "187 [ 210  283  372  391  496  502  509  552  728  746  804  859  900  903\n",
            "  923  994 1038 1083 1148 1232 1277 1282 1549 1597 1650 1751 1754 1811\n",
            " 1904 1905 1970 1972 1979 2071 2143 2144 2185 2209 2243 2266 2267 2335\n",
            " 2338 2350 2376 2377 2405 2466 2532 2536 2572 2578 2609 2611 2617 2641\n",
            " 2692 2704 2723 2750 2871 2892 2895 2899 2936 2958 3090 3102 3110 3111\n",
            " 3225 3334 3416 3436 3438 3442 3472 3486 3494 3526 3545 3621 3664 3673\n",
            " 3692 3744 3812 3816 3830 3845 3928 3952 3953 3964 3968 4012 4016 4060\n",
            " 4089 4166 4180 4208 4339 4349 4465 4473 4479 4498 4530 4562 4575 4609\n",
            " 4655 4694 4707 4783 4880 5031 5050 5067 5154 5192 5213 5261 5401 5508\n",
            " 5575 5615 5725 5813 5950 6209 6210 6326 6451 6520 6620 6673 6683 6698\n",
            " 6710 6742 6744 6746 6772 6818 6874 6895 6919 6968 6978 6995 7057 7119\n",
            " 7136 7165 7231 7313 7371 7402 7418 7532 7554 7587 7590 7690 7722 7730\n",
            " 7783 7890 7904 8078 8243 8361 8499 8747 8881 8930 9028 9064 9400 9595\n",
            " 9610 9692 9755 9915 9926]\n",
            "\n",
            "\n",
            "All Predictions\n",
            "\n",
            "494 [  72  210  283  315  372  391  446  496  502  509  538  552  553  587\n",
            "  590  625  630  642  652  666  708  728  729  746  757  789  804  806\n",
            "  810  859  895  900  903  906  913  923  942  947  994 1038 1040 1047\n",
            " 1058 1062 1083 1138 1148 1163 1164 1231 1232 1249 1252 1255 1270 1277\n",
            " 1282 1308 1343 1354 1445 1448 1475 1489 1500 1508 1526 1549 1553 1563\n",
            " 1593 1597 1631 1650 1671 1702 1746 1751 1754 1775 1788 1811 1820 1867\n",
            " 1879 1904 1905 1927 1950 1970 1972 1976 1979 1980 2019 2032 2071 2075\n",
            " 2107 2143 2144 2173 2185 2195 2209 2243 2246 2255 2256 2266 2267 2320\n",
            " 2335 2338 2341 2346 2350 2353 2376 2377 2399 2403 2405 2415 2446 2466\n",
            " 2479 2529 2532 2536 2562 2572 2578 2594 2609 2611 2617 2641 2652 2670\n",
            " 2679 2692 2704 2723 2750 2769 2779 2790 2791 2794 2795 2812 2842 2871\n",
            " 2892 2895 2896 2899 2930 2935 2936 2958 2997 3009 3030 3033 3057 3090\n",
            " 3102 3110 3111 3125 3140 3149 3165 3184 3206 3225 3302 3333 3334 3340\n",
            " 3346 3375 3381 3411 3416 3424 3436 3438 3442 3472 3486 3494 3526 3545\n",
            " 3548 3569 3584 3616 3621 3629 3649 3664 3673 3685 3686 3692 3732 3744\n",
            " 3789 3790 3812 3816 3830 3837 3845 3883 3886 3898 3925 3928 3952 3953\n",
            " 3958 3964 3968 3991 4001 4012 4016 4035 4060 4077 4087 4089 4096 4098\n",
            " 4123 4145 4151 4166 4180 4197 4201 4205 4208 4216 4223 4256 4275 4316\n",
            " 4336 4339 4349 4365 4373 4413 4448 4465 4473 4479 4488 4498 4514 4529\n",
            " 4530 4554 4562 4575 4609 4645 4655 4657 4689 4694 4707 4725 4726 4756\n",
            " 4777 4783 4789 4795 4834 4848 4859 4879 4880 5031 5036 5045 5047 5050\n",
            " 5067 5074 5084 5154 5192 5213 5227 5237 5251 5256 5261 5272 5280 5309\n",
            " 5385 5401 5430 5438 5440 5442 5508 5553 5575 5581 5615 5623 5624 5651\n",
            " 5673 5725 5746 5773 5777 5797 5813 5881 5884 5887 5913 5937 5950 5970\n",
            " 5989 5991 6004 6015 6019 6022 6051 6066 6084 6088 6090 6091 6129 6209\n",
            " 6210 6219 6255 6271 6275 6286 6313 6326 6379 6391 6413 6416 6430 6446\n",
            " 6451 6481 6490 6510 6520 6545 6620 6648 6673 6683 6697 6698 6706 6710\n",
            " 6742 6744 6746 6772 6818 6833 6834 6860 6874 6895 6913 6919 6960 6962\n",
            " 6968 6978 6982 6995 7000 7013 7014 7057 7072 7081 7100 7119 7136 7165\n",
            " 7166 7231 7260 7307 7313 7320 7371 7373 7381 7397 7400 7402 7418 7421\n",
            " 7428 7434 7486 7532 7554 7557 7568 7587 7590 7621 7678 7682 7690 7703\n",
            " 7722 7730 7775 7783 7855 7890 7904 7991 7992 8002 8029 8064 8078 8177\n",
            " 8184 8186 8239 8243 8248 8260 8307 8353 8361 8370 8499 8545 8666 8747\n",
            " 8773 8785 8825 8881 8930 9028 9064 9076 9187 9211 9281 9282 9364 9400\n",
            " 9423 9465 9482 9497 9521 9522 9562 9595 9610 9692 9747 9755 9862 9887\n",
            " 9908 9915 9926 9956]\n",
            "\n",
            "494 [0.50707763 0.52533025 0.5127591  0.50393075 0.5087107  0.5093273\n",
            " 0.5006425  0.5086148  0.5150424  0.5146147  0.5073808  0.5182772\n",
            " 0.5002867  0.50506973 0.50012517 0.5022093  0.5011633  0.50241727\n",
            " 0.50120413 0.5047745  0.5008757  0.5105778  0.5021068  0.5084943\n",
            " 0.50560254 0.50370556 0.5107835  0.5046049  0.50066316 0.51096714\n",
            " 0.50432944 0.50773966 0.5235907  0.5066677  0.50200325 0.5108607\n",
            " 0.5041389  0.5022995  0.5132455  0.50833213 0.50469637 0.5046996\n",
            " 0.5061595  0.5017888  0.50912255 0.50044316 0.5121134  0.50381\n",
            " 0.5007258  0.50198567 0.51219165 0.5037684  0.50323546 0.5007648\n",
            " 0.5023416  0.508122   0.508358   0.50044954 0.5072145  0.5023122\n",
            " 0.5001218  0.5006942  0.5003188  0.50012714 0.50636905 0.5029877\n",
            " 0.5001154  0.5085217  0.501162   0.50636953 0.50070876 0.51075906\n",
            " 0.5043989  0.5097689  0.50235265 0.5068093  0.5066963  0.5088728\n",
            " 0.5318129  0.5039132  0.5070233  0.51729757 0.5033545  0.50000197\n",
            " 0.5061216  0.51353437 0.516626   0.50051904 0.5018306  0.50842166\n",
            " 0.50894135 0.50520253 0.5136394  0.50441384 0.5002001  0.5051231\n",
            " 0.5145276  0.50197035 0.5071635  0.5102738  0.510169   0.50114036\n",
            " 0.5091459  0.5041689  0.516946   0.53091156 0.50534236 0.50224656\n",
            " 0.50598186 0.51089185 0.50796664 0.5043131  0.52513516 0.5156687\n",
            " 0.50038445 0.5053669  0.5177501  0.5050665  0.5303178  0.52501655\n",
            " 0.5013523  0.5024482  0.5082659  0.50731605 0.5039526  0.51272774\n",
            " 0.5055102  0.50437784 0.5154051  0.513157   0.50418794 0.5124691\n",
            " 0.50878686 0.50299394 0.50843954 0.5118238  0.51400536 0.50942975\n",
            " 0.5068394  0.5022483  0.5039849  0.50826174 0.5088326  0.5255518\n",
            " 0.5198802  0.5045545  0.5013804  0.50678265 0.5002708  0.5039768\n",
            " 0.5061815  0.50508523 0.5050681  0.5265844  0.51771444 0.50971985\n",
            " 0.5075049  0.51145995 0.50283337 0.5006472  0.5205757  0.5104091\n",
            " 0.5016165  0.5007941  0.5052848  0.50053227 0.5039928  0.5079227\n",
            " 0.50928766 0.51563317 0.54852355 0.5071143  0.5014436  0.5005224\n",
            " 0.5017581  0.5008857  0.5029749  0.50781953 0.5010428  0.50379544\n",
            " 0.5181805  0.50251234 0.50026774 0.503641   0.50443363 0.5072879\n",
            " 0.51897025 0.5016399  0.51776385 0.5129437  0.508133   0.51183975\n",
            " 0.5086402  0.51940084 0.5100568  0.5332482  0.50512403 0.5003299\n",
            " 0.50074935 0.5053728  0.5122607  0.50167036 0.5027918  0.5126682\n",
            " 0.5170982  0.5018046  0.5028311  0.5100083  0.5055111  0.5080711\n",
            " 0.5036625  0.5001961  0.512396   0.5176581  0.5080591  0.50472325\n",
            " 0.5288399  0.5022578  0.50415355 0.50030446 0.50148284 0.5171599\n",
            " 0.5127955  0.5093059  0.504144   0.509742   0.5084567  0.502365\n",
            " 0.5030172  0.5134427  0.5078035  0.50292987 0.5176441  0.5022071\n",
            " 0.5052273  0.5245056  0.50050235 0.50466084 0.5001944  0.5012627\n",
            " 0.5025073  0.5106292  0.5121595  0.5067426  0.50336456 0.50199026\n",
            " 0.5289617  0.50260884 0.50189036 0.5073876  0.5013546  0.50151885\n",
            " 0.5060815  0.50969017 0.5104509  0.50082576 0.50518745 0.5009071\n",
            " 0.5043662  0.51256007 0.5176845  0.51244974 0.506628   0.5079083\n",
            " 0.50106835 0.50116706 0.50785726 0.5021779  0.5122542  0.50774556\n",
            " 0.51686    0.5016856  0.51713485 0.50746065 0.50395226 0.50965387\n",
            " 0.52110606 0.50544345 0.50156385 0.5051319  0.5015577  0.511578\n",
            " 0.50533    0.5011685  0.5001941  0.5037093  0.5005038  0.50313544\n",
            " 0.537929   0.512473   0.5063926  0.50258934 0.5021655  0.5096269\n",
            " 0.51016176 0.5021337  0.5017881  0.5146631  0.51184565 0.508121\n",
            " 0.5069504  0.50024354 0.50245816 0.50382006 0.51108974 0.50350624\n",
            " 0.50128293 0.50669795 0.5006827  0.513495   0.50263965 0.5012195\n",
            " 0.5009577  0.5032266  0.5103717  0.5042047  0.53103733 0.5019524\n",
            " 0.5179427  0.5023423  0.503327   0.50361395 0.50248146 0.5101379\n",
            " 0.5031774  0.50239664 0.5020683  0.5027374  0.5218362  0.504273\n",
            " 0.5071728  0.50550365 0.5066903  0.50518465 0.5137073  0.5029377\n",
            " 0.5022227  0.50461435 0.50110775 0.5042653  0.50214356 0.5006534\n",
            " 0.50565    0.5019569  0.50108784 0.5004834  0.50080127 0.5013112\n",
            " 0.50513774 0.51337105 0.51286197 0.50323904 0.50283223 0.5038378\n",
            " 0.5031234  0.50191706 0.50450236 0.54621804 0.5035651  0.5008678\n",
            " 0.50601953 0.5008646  0.50153553 0.5029603  0.5099215  0.50554085\n",
            " 0.50523156 0.5024337  0.5238756  0.50544715 0.5085905  0.504683\n",
            " 0.5098372  0.51065886 0.5005505  0.51391524 0.50697356 0.50856084\n",
            " 0.5112618  0.52239776 0.51490545 0.51415724 0.51885045 0.503015\n",
            " 0.5018244  0.5045265  0.5159269  0.5138101  0.5009671  0.51438296\n",
            " 0.5046235  0.50473076 0.51265967 0.5160776  0.50081205 0.50836295\n",
            " 0.5030011  0.50409555 0.5057531  0.5118605  0.5007664  0.5029677\n",
            " 0.5027328  0.5101767  0.5134576  0.51159114 0.5022464  0.51412874\n",
            " 0.50485635 0.50247383 0.50810623 0.5010735  0.51014966 0.50684154\n",
            " 0.5075193  0.5025619  0.5009064  0.53344923 0.52041125 0.50358033\n",
            " 0.5000052  0.50741696 0.5001678  0.5205345  0.50866115 0.50749284\n",
            " 0.501695   0.5250443  0.51075953 0.5071206  0.5008669  0.50146693\n",
            " 0.512813   0.5010223  0.5080661  0.5088223  0.506577   0.51979625\n",
            " 0.5011163  0.5136299  0.50917786 0.50497323 0.5011638  0.5036154\n",
            " 0.50281954 0.5015441  0.5194509  0.5035872  0.50650656 0.501853\n",
            " 0.50501233 0.5124901  0.5016618  0.50136256 0.5017526  0.50262225\n",
            " 0.5141119  0.50192004 0.53903604 0.5007559  0.50232095 0.5165161\n",
            " 0.50223565 0.50093144 0.5019069  0.5126942  0.51453686 0.52640605\n",
            " 0.5117337  0.50673646 0.5015007  0.50750095 0.5057384  0.5056918\n",
            " 0.50708187 0.50934714 0.50183386 0.5036512  0.5003244  0.5017269\n",
            " 0.50094104 0.5034343  0.50137866 0.5424325  0.50766486 0.515735\n",
            " 0.50464666 0.5113672  0.5059474  0.5029228  0.50661767 0.5220796\n",
            " 0.5088066  0.5047192 ]\n",
            "\n",
            "Matched draws\n",
            "Count: 0, Index: (array([], dtype=int64),)\n",
            "\n",
            "\n",
            "Top 23 Possibility\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "First 23 Numbers\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "2 To 3 Digits Numbers\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "\n",
            "\n",
            "All matched\n",
            "Empty DataFrame\n",
            "Columns: [DrawNo, DrawDate, PrizeType, LuckyNo]\n",
            "Index: []\n",
            "CPU times: user 3h 29min 14s, sys: 5 s, total: 3h 29min 19s\n",
            "Wall time: 53min 17s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSzWpN1kJHma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "61bb6629-8fef-42eb-d00e-5c18f3a2cf40"
      },
      "source": [
        "%time gen_train_test_set(pd.datetime(2020,2,1), feature_matrix_selection, file_prefix='test')\n",
        "%time model(pd.datetime(2020,2,1), feature_matrix_selection, file_prefix='test')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on 969893 observations.\n",
            "Testing on 10000 observations.\n",
            "\n",
            "Data shape before balancing: (969893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes in previous data:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n",
            "\n",
            "NO balancing\n",
            "Data shape after balancing: (969893, 63)\n",
            "\n",
            "Counts of strikes VS non-strikes in new data:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "\n",
            "Filtering features\n",
            "\n",
            "Data shape before feature selection: (969893, 63)\n",
            "\n",
            "Counts of strikes vs non-strikes before feature selection:\n",
            "0    937508\n",
            "1     32385\n",
            "Name: Label, dtype: int64\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VE6Xbz-IyvLj",
        "colab": {}
      },
      "source": [
        "# weight=1.0\n",
        "# decrement = 0.000\n",
        "# to_stop=False\n",
        "\n",
        "# dt = pd.datetime(2020,1,1)\n",
        "# %time gen_train_test_set(dt, feature_matrix_selection, file_prefix='test')\n",
        "# while not to_stop:\n",
        "#   to_stop = model(dt, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#   decrement = decrement + 0.005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qktZbi7OGqP3",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# start_mt = pd.datetime(2019,7,1)\n",
        "# how_many_mt = 6 \n",
        "# for i in range(how_many_mt):\n",
        "#   month_to_predict = start_mt + relativedelta(months=i)\n",
        "#   print(f\"\\n{month_to_predict}\\n-------------------\\n\")\n",
        "\n",
        "#   weight=1.0\n",
        "#   decrement = 0.000\n",
        "#   to_stop=False\n",
        "\n",
        "#   gen_train_test_set(month_to_predict, feature_matrix_selection, file_prefix='test')\n",
        "#   while not to_stop:\n",
        "#     to_stop = model(month_to_predict, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#     decrement = decrement + 0.001\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N8tcqn4yIl21",
        "colab": {}
      },
      "source": [
        "# weight=1.0\n",
        "# decrement = 0.000\n",
        "# to_stop=False\n",
        "\n",
        "# dt = pd.datetime(2020,2,1)\n",
        "# %time gen_train_test_set(dt, feature_matrix_selection, file_prefix='test')\n",
        "# while not to_stop:\n",
        "#   to_stop = model(dt, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#   decrement = decrement + 0.005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1SC8YG8P_ljs",
        "colab": {}
      },
      "source": [
        "# weight=1.0\n",
        "# decrement = 0.000\n",
        "# to_stop=False\n",
        "\n",
        "# dt = pd.datetime(2020,3,1)\n",
        "# %time gen_train_test_set(dt, feature_matrix_selection, file_prefix='test')\n",
        "# while not to_stop:\n",
        "#   to_stop = model(dt, feature_matrix_selection, file_prefix='test', class_weight=(weight-decrement))\n",
        "#   decrement = decrement + 0.00"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yASd8nZFR7Qb",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}